<!doctype html><html lang="en"><head><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>一个使用 asyncio 协程的网络爬虫（三） - 归墟星火集</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a href="/about/">关于</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">一个使用 asyncio 协程的网络爬虫（三）</h1><span class="post-date">2017-03-06</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/">软件开发</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/Python/">Python</a> <a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a> <a href="/tags/%E5%9B%9E%E8%B0%83/">回调</a> <a href="/tags/%E5%BC%82%E6%AD%A5/">异步</a> <a href="/tags/%E5%8D%8F%E7%A8%8B/">协程</a> <a href="/tags/asyncio/">asyncio</a></div><div class="post-content"><blockquote><p>本文作者：</p><p>A. Jesse Jiryu Davis 是纽约 MongoDB 的工程师。他编写了异步 MongoDB Python 驱动程序 Motor，也是 MongoDB C 驱动程序的开发领袖和 PyMongo 团队成员。 他也为 asyncio 和 Tornado 做了贡献，在 <a target="_blank" rel="noopener" href="http://emptysqua.re/">http://emptysqua.re</a> 上写作。</p><p>Guido van Rossum 是主流编程语言 Python 的创造者，Python 社区称他为 BDFL （仁慈的终生大独裁者 (Benevolent Dictator For Life)）——这是一个来自 Monty Python 短剧的称号。他的主页是 <a target="_blank" rel="noopener" href="http://www.python.org/%7Eguido/">http://www.python.org/~guido&#x2F;</a> 。</p></blockquote><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201703/04/160319vrfrrsl2x2lrlpzp.jpg"></p><h3 id="使用协程"><a href="#使用协程" class="headerlink" title="使用协程"></a>使用协程</h3><p>我们将从描述爬虫如何工作开始。现在是时候用 asynio 去实现它了。</p><p>我们的爬虫从获取第一个网页开始，解析出链接并把它们加到队列中。此后它开始傲游整个网站，并发地获取网页。但是由于客户端和服务端的负载限制，我们希望有一个最大数目的运行的 worker，不能再多。任何时候一个 worker 完成一个网页的获取，它应该立即从队列中取出下一个链接。我们会遇到没有那么多事干的时候，所以一些 worker 必须能够暂停。一旦又有 worker 获取一个有很多链接的网页，队列会突增，暂停的 worker 立马被唤醒干活。最后，当任务完成后我们的程序必须马上退出。</p><p>假如你的 worker 是线程，怎样去描述你的爬虫算法？我们可以使用 Python 标准库中的<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/queue.html">同步队列</a>。每次有新的一项加入，队列增加它的 “tasks” 计数器。线程 worker 完成一个任务后调用 <code>task_done</code>。主线程阻塞在 <code>Queue.join</code>，直到“tasks”计数器与 <code>task_done</code> 调用次数相匹配，然后退出。</p><p>协程通过 asyncio 队列，使用和线程一样的模式来实现！首先我们<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/asyncio-sync.html">导入它</a>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">    from asyncio import JoinableQueue as Queue</span><br><span class="line">except ImportError:</span><br><span class="line">    # In Python 3.5, asyncio.JoinableQueue is</span><br><span class="line">    # merged into Queue.</span><br><span class="line">    from asyncio import Queue</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们把 worker 的共享状态收集在一个 crawler 类中，主要的逻辑写在 <code>crawl</code> 方法中。我们在一个协程中启动 <code>crawl</code>,运行 asyncio 的事件循环直到 <code>crawl</code> 完成：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">crawler = crawling.Crawler(&#x27;http://xkcd.com&#x27;,</span><br><span class="line">                           max_redirect=10)</span><br><span class="line"></span><br><span class="line">loop.run_until_complete(crawler.crawl())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>crawler 用一个根 URL 和最大重定向数 <code>max_redirect</code> 来初始化，它把 <code>(URL, max_redirect)</code> 序对放入队列中。（为什么要这样做，请看下文）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Crawler:</span><br><span class="line">    def __init__(self, root_url, max_redirect):</span><br><span class="line">        self.max_tasks = 10</span><br><span class="line">        self.max_redirect = max_redirect</span><br><span class="line">        self.q = Queue()</span><br><span class="line">        self.seen_urls = set()</span><br><span class="line"></span><br><span class="line">        # aiohttp&#x27;s ClientSession does connection pooling and</span><br><span class="line">        # HTTP keep-alives for us.</span><br><span class="line">        self.session = aiohttp.ClientSession(loop=loop)</span><br><span class="line"></span><br><span class="line">        # Put (URL, max_redirect) in the queue.</span><br><span class="line">        self.q.put((root_url, self.max_redirect))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在队列中未完成的任务数是 1。回到我们的主程序，启动事件循环和 <code>crawl</code> 方法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loop.run_until_complete(crawler.crawl())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>crawl</code> 协程把 worker 们赶起来干活。它像一个主线程：阻塞在 <code>join</code> 上直到所有任务完成，同时 worker 们在后台运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@asyncio.coroutine</span><br><span class="line">def crawl(self):</span><br><span class="line">    &quot;&quot;&quot;Run the crawler until all work is done.&quot;&quot;&quot;</span><br><span class="line">    workers = [asyncio.Task(self.work())</span><br><span class="line">               for _ in range(self.max_tasks)]</span><br><span class="line"></span><br><span class="line">    # When all work is done, exit.</span><br><span class="line">    yield from self.q.join()</span><br><span class="line">    for w in workers:</span><br><span class="line">        w.cancel()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果 worker 是线程，可能我们不会一次把它们全部创建出来。为了避免创建线程的昂贵代价，通常一个线程池会按需增长。但是协程很廉价，我们可以直接把他们全部创建出来。</p><p>怎么关闭这个 <code>crawler</code> 很有趣。当 <code>join</code> 完成，worker 存活但是被暂停：他们等待更多的 URL，所以主协程要在退出之前清除它们。否则 Python 解释器关闭并调用所有对象的析构函数时，活着的 worker 会哭喊到：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR:asyncio:Task was destroyed but it is pending!</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>cancel</code> 又是如何工作的呢？生成器还有一个我们还没介绍的特点。你可以从外部抛一个异常给它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; gen = gen_fn()</span><br><span class="line">&gt;&gt;&gt; gen.send(None)  # Start the generator as usual.</span><br><span class="line">1</span><br><span class="line">&gt;&gt;&gt; gen.throw(Exception(&#x27;error&#x27;))</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;input&gt;&quot;, line 3, in &lt;module&gt;</span><br><span class="line">  File &quot;&lt;input&gt;&quot;, line 2, in gen_fn</span><br><span class="line">Exception: error</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>生成器被 <code>throw</code> 恢复，但是它现在抛出一个异常。如过生成器的调用堆栈中没有捕获异常的代码，这个异常被传递到顶层。所以注销一个协程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Method of Task class.</span><br><span class="line">def cancel(self):</span><br><span class="line">    self.coro.throw(CancelledError)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>任何时候生成器暂停，在某些 <code>yield from</code> 语句它恢复并且抛出一个异常。我们在 task 的 <code>step</code> 方法中处理注销。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Method of Task class.</span><br><span class="line">def step(self, future):</span><br><span class="line">    try:</span><br><span class="line">        next_future = self.coro.send(future.result)</span><br><span class="line">    except CancelledError:</span><br><span class="line">        self.cancelled = True</span><br><span class="line">        return</span><br><span class="line">    except StopIteration:</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    next_future.add_done_callback(self.step)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在 task 知道它被注销了，所以当它被销毁时，它不再抱怨。</p><p>一旦 <code>crawl</code> 注销了 worker，它就退出。同时事件循环看见这个协程结束了（我们后面会见到的），也就退出。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loop.run_until_complete(crawler.crawl())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>crawl</code> 方法包含了所有主协程需要做的事。而 worker 则完成从队列中获取 URL、获取网页、解析它们得到新的链接。每个 worker 独立地运行 <code>work</code> 协程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@asyncio.coroutine</span><br><span class="line">def work(self):</span><br><span class="line">    while True:</span><br><span class="line">        url, max_redirect = yield from self.q.get()</span><br><span class="line"></span><br><span class="line">        # Download page and add new links to self.q.</span><br><span class="line">        yield from self.fetch(url, max_redirect)</span><br><span class="line">        self.q.task_done()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Python 看见这段代码包含 <code>yield from</code> 语句，就把它编译成生成器函数。所以在 <code>crawl</code> 方法中，我们调用了 10 次 <code>self.work</code>，但并没有真正执行，它仅仅创建了 10 个指向这段代码的生成器对象并把它们包装成 Task 对象。task 接收每个生成器所 yield 的 future，通过调用 <code>send</code> 方法，当 future 解决时，用 future 的结果做为 <code>send</code> 的参数，来驱动它。由于生成器有自己的栈帧，它们可以独立运行，带有独立的局部变量和指令指针。</p><p>worker 使用队列来协调其小伙伴。它这样等待新的 URL：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">url, max_redirect = yield from self.q.get()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>队列的 <code>get</code> 方法自身也是一个协程，它一直暂停到有新的 URL 进入队列，然后恢复并返回该条目。</p><p>碰巧，这也是当主协程注销 worker 时，最后 crawl 停止，worker 协程暂停的地方。从协程的角度，<code>yield from</code> 抛出<code>CancelledError</code> 结束了它在循环中的最后旅程。</p><p>worker 获取一个网页，解析链接，把新的链接放入队列中，接着调用<code>task_done</code>减小计数器。最终一个worker遇到一个没有新链接的网页，并且队列里也没有任务，这次<code>task_done</code>的调用使计数器减为0，而<code>crawl</code>正阻塞在<code>join</code>方法上，现在它就可以结束了。</p><p>我们承诺过要解释为什么队列中要使用序对，像这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># URL to fetch, and the number of redirects left.</span><br><span class="line">(&#x27;http://xkcd.com/353&#x27;, 10)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>新的 URL 的重定向次数是10。获取一个特别的 URL 会重定向一个新的位置。我们减小重定向次数，并把新的 URL 放入队列中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># URL with a trailing slash. Nine redirects left.</span><br><span class="line">(&#x27;http://xkcd.com/353/&#x27;, 9)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们使用的 <code>aiohttp</code> 默认会跟踪重定向并返回最终结果。但是，我们告诉它不要这样做，爬虫自己来处理重定向，以便它可以合并那些目的相同的重定向路径：如果我们已经在 <code>self.seen_urls</code> 看到一个 URL，说明它已经从其他的地方走过这条路了。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201703/04/160327uc6a40g8xq4apx4x.png" alt="Figure 5.4 - Redirects"></p><p>crawler 获取“foo”并发现它重定向到了“baz”，所以它会加“baz”到队列和 <code>seen_urls</code> 中。如果它获取的下一个页面“bar” 也重定向到“baz”，fetcher 不会再次将 “baz”加入到队列中。如果该响应是一个页面，而不是一个重定向，<code>fetch</code> 会解析它的链接，并把新链接放到队列中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">@asyncio.coroutine</span><br><span class="line">def fetch(self, url, max_redirect):</span><br><span class="line">    # Handle redirects ourselves.</span><br><span class="line">    response = yield from self.session.get(</span><br><span class="line">        url, allow_redirects=False)</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        if is_redirect(response):</span><br><span class="line">            if max_redirect &gt; 0:</span><br><span class="line">                next_url = response.headers[&#x27;location&#x27;]</span><br><span class="line">                if next_url in self.seen_urls:</span><br><span class="line">                    # We have been down this path before.</span><br><span class="line">                    return</span><br><span class="line"></span><br><span class="line">                # Remember we have seen this URL.</span><br><span class="line">                self.seen_urls.add(next_url)</span><br><span class="line"></span><br><span class="line">                # Follow the redirect. One less redirect remains.</span><br><span class="line">                self.q.put_nowait((next_url, max_redirect - 1))</span><br><span class="line">         else:</span><br><span class="line">             links = yield from self.parse_links(response)</span><br><span class="line">             # Python set-logic:</span><br><span class="line">             for link in links.difference(self.seen_urls):</span><br><span class="line">                self.q.put_nowait((link, self.max_redirect))</span><br><span class="line">            self.seen_urls.update(links)</span><br><span class="line">    finally:</span><br><span class="line">        # Return connection to pool.</span><br><span class="line">        yield from response.release()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果这是多进程代码，就有可能遇到讨厌的竞争条件。比如，一个 worker 检查一个链接是否在 <code>seen_urls</code> 中，如果没有它就把这个链接加到队列中并把它放到 <code>seen_urls</code> 中。如果它在这两步操作之间被中断，而另一个 worker 解析到相同的链接，发现它并没有出现在 <code>seen_urls</code> 中就把它加入队列中。这（至少）导致同样的链接在队列中出现两次，做了重复的工作和错误的统计。</p><p>然而，一个协程只在 <code>yield from</code> 时才会被中断。这是协程比多线程少遇到竞争条件的关键。多线程必须获得锁来明确的进入一个临界区，否则它就是可中断的。而 Python 的协程默认是不会被中断的，只有它明确 yield 时才主动放弃控制权。</p><p>我们不再需要在用回调方式时用的 fetcher 类了。这个类只是不高效回调的一个变通方法：在等待 I&#x2F;O 时，它需要一个存储状态的地方，因为局部变量并不能在函数调用间保留。倒是 <code>fetch</code> 协程可以像普通函数一样用局部变量保存它的状态，所以我们不再需要一个类。</p><p>当 <code>fetch</code> 完成对服务器响应的处理，它返回到它的调用者 <code>work</code>。<code>work</code> 方法对队列调用 <code>task_done</code>，接着从队列中取出一个要获取的 URL。</p><p>当 <code>fetch</code> 把新的链接放入队列中，它增加未完成的任务计数器，并停留在主协程，主协程在等待 <code>q.join</code>，处于暂停状态。而当没有新的链接并且这是队列中最后一个 URL 时，当 <code>work 调用</code>task_done<code>，任务计数器变为 0，主协程从</code>join&#96; 中退出。</p><p>与 worker 和主协程一起工作的队列代码像这样（实际的 <code>asyncio.Queue</code> 实现在 Future 所展示的地方使用 <code>asyncio.Event</code> 。不同之处在于 Event 是可以重置的，而 Future 不能从已解决返回变成待决。）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Queue:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self._join_future = Future()</span><br><span class="line">        self._unfinished_tasks = 0</span><br><span class="line">        # ... other initialization ...</span><br><span class="line"></span><br><span class="line">    def put_nowait(self, item):</span><br><span class="line">        self._unfinished_tasks += 1</span><br><span class="line">        # ... store the item ...</span><br><span class="line"></span><br><span class="line">    def task_done(self):</span><br><span class="line">        self._unfinished_tasks -= 1</span><br><span class="line">        if self._unfinished_tasks == 0:</span><br><span class="line">            self._join_future.set_result(None)</span><br><span class="line"></span><br><span class="line">    @asyncio.coroutine</span><br><span class="line">    def join(self):</span><br><span class="line">        if self._unfinished_tasks &gt; 0:</span><br><span class="line">            yield from self._join_future</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>主协程 <code>crawl</code> yield from <code>join</code>。所以当最后一个 worker 把计数器减为 0，它告诉 <code>crawl</code> 恢复运行并结束。</p><p>旅程快要结束了。我们的程序从 <code>crawl</code> 调用开始：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loop.run_until_complete(self.crawler.crawl())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>程序如何结束？因为 <code>crawl</code> 是一个生成器函数，调用它返回一个生成器。为了驱动它，asyncio 把它包装成一个 task：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class EventLoop:</span><br><span class="line">    def run_until_complete(self, coro):</span><br><span class="line">        &quot;&quot;&quot;Run until the coroutine is done.&quot;&quot;&quot;</span><br><span class="line">        task = Task(coro)</span><br><span class="line">        task.add_done_callback(stop_callback)</span><br><span class="line">        try:</span><br><span class="line">            self.run_forever()</span><br><span class="line">        except StopError:</span><br><span class="line">            pass</span><br><span class="line"></span><br><span class="line">class StopError(BaseException):</span><br><span class="line">    &quot;&quot;&quot;Raised to stop the event loop.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def stop_callback(future):</span><br><span class="line">    raise StopError</span><br></pre></td></tr></table></figure><p>当这个任务完成，它抛出 <code>StopError</code>，事件循环把这个异常当作正常退出的信号。</p><p>但是，task 的 <code>add_done_callbock</code> 和 <code>result</code> 方法又是什么呢？你可能认为 task 就像一个 future，不错，你的直觉是对的。我们必须承认一个向你隐藏的细节，task 是 future。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class Task(Future):</span><br><span class="line">    &quot;&quot;&quot;A coroutine wrapped in a Future.&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><p>通常，一个 future 被别人调用 <code>set_result</code> 解决。但是 task，当协程结束时，它自己解决自己。记得我们解释过当 Python 生成器返回时，它抛出一个特殊的 <code>StopIteration</code> 异常：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Method of class Task.</span><br><span class="line">def step(self, future):</span><br><span class="line">    try:</span><br><span class="line">        next_future = self.coro.send(future.result)</span><br><span class="line">    except CancelledError:</span><br><span class="line">        self.cancelled = True</span><br><span class="line">        return</span><br><span class="line">    except StopIteration as exc:</span><br><span class="line"></span><br><span class="line">        # Task resolves itself with coro&#x27;s return</span><br><span class="line">        # value.</span><br><span class="line">        self.set_result(exc.value)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    next_future.add_done_callback(self.step)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>所以当事件循环调用 <code>task.add_done_callback(stop_callback)</code>，它就准备被这个 task 停止。在看一次<code>run_until_complete</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Method of event loop.</span><br><span class="line">def run_until_complete(self, coro):</span><br><span class="line">    task = Task(coro)</span><br><span class="line">    task.add_done_callback(stop_callback)</span><br><span class="line">    try:</span><br><span class="line">        self.run_forever()</span><br><span class="line">    except StopError:</span><br><span class="line">        pass</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当 task 捕获 <code>StopIteration</code> 并解决自己，这个回调从循环中抛出 <code>StopError</code>。循环结束，调用栈回到<code>run_until_complete</code>。我们的程序结束。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>现代的程序越来越多是 I&#x2F;O 密集型而不是 CPU 密集型。对于这样的程序，Python 的线程在两个方面不合适：全局解释器锁阻止真正的并行计算，并且抢占切换也导致他们更容易出现竞争。异步通常是正确的选择。但是随着基于回调的异步代码增加，它会变得非常混乱。协程是一个更整洁的替代者。它们自然地重构成子过程，有健全的异常处理和栈追溯。</p><p>如果我们换个角度看 <code>yield from</code> 语句，一个协程看起来像一个传统的做阻塞 I&#x2F;O 的线程。甚至我们可以采用经典的多线程模式编程，不需要重新发明。因此，与回调相比，协程更适合有经验的多线程的编码者。</p><p>但是当我们睁开眼睛关注 <code>yield from</code> 语句，我们能看到协程放弃控制权、允许其它人运行的标志点。不像多线程，协程展示出我们的代码哪里可以被中断哪里不能。在 Glyph Lefkowitz 富有启发性的文章“<a target="_blank" rel="noopener" href="https://glyph.twistedmatrix.com/2014/02/unyielding.html">Unyielding</a>”：“线程让局部推理变得困难，然而局部推理可能是软件开发中最重要的事”。然而，明确的 yield，让“通过过程本身而不是整个系统理解它的行为（和因此、正确性）”成为可能。</p><p>这章写于 Python 和异步的复兴时期。你刚学到的基于生成器的的协程，在 2014 年发布在 Python 3.4 的 asyncio 模块中。2015 年 9 月，Python 3.5 发布，协程成为语言的一部分。这个原生的协程通过“async def”来声明, 使用“await”而不是“yield from”委托一个协程或者等待 Future。</p><p>除了这些优点，核心的思想不变。Python 新的原生协程与生成器只是在语法上不同，工作原理非常相似。事实上，在 Python 解释器中它们共用同一个实现方法。Task、Future 和事件循环在 asynico 中扮演着同样的角色。</p><p>你已经知道 asyncio 协程是如何工作的了，现在你可以忘记大部分的细节。这些机制隐藏在一个整洁的接口下。但是你对这基本原理的理解能让你在现代异步环境下正确而高效的编写代码。</p><p>（题图素材来自：<a target="_blank" rel="noopener" href="http://ruth-tay.deviantart.com/art/Bearded-Vulture-553800995">ruth-tay.deviantart.com</a>）</p><hr><p>via: <a target="_blank" rel="noopener" href="http://aosabook.org/en/500L/pages/a-web-crawler-with-asyncio-coroutines.html">http://aosabook.org/en/500L/pages/a-web-crawler-with-asyncio-coroutines.html</a></p><p>作者：A. Jesse Jiryu Davis , Guido van Rossum 译者：<a target="_blank" rel="noopener" href="https://github.com/qingyunha">qingyunha</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创翻译，<a target="_blank" rel="noopener" href="http://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p></div></div></body></html>