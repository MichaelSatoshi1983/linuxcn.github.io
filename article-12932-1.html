<!doctype html><html lang="en"><head><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>用 PyTorch 实现基于字符的循环神经网络 - 归墟星火集</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a href="/about/">关于</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">用 PyTorch 实现基于字符的循环神经网络</h1><span class="post-date">2020-12-19</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/">软件开发</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a> <a href="/tags/RNN/">RNN</a></div><div class="post-content"><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202012/19/102319oe36em6d63bolg0i.jpg"></p><p>在过去的几周里，我花了很多时间用 PyTorch 实现了一个 <a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">char-rnn</a> 的版本。我以前从未训练过神经网络，所以这可能是一个有趣的开始。</p><p>这个想法（来自 <a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">循环神经网络的不合理效应</a>）可以让你在文本上训练一个基于字符的<ruby>循环神经网络 <rt>recurrent neural network</rt></ruby>（RNN），并得到一些出乎意料好的结果。</p><p>不过，虽然没有得到我想要的结果，但是我还是想分享一些示例代码和结果，希望对其他开始尝试使用 PyTorch 和 RNN 的人有帮助。</p><p>这是 Jupyter 笔记本格式的代码：<a target="_blank" rel="noopener" href="https://gist.github.com/jvns/b6dda36b2fdcc02b833ed5b0c7a09112">char-rnn in PyTorch.ipynb</a>。你可以点击这个网页最上面那个按钮 “Open in Colab”，就可以在 Google 的 Colab 服务中打开，并使用免费的 GPU 进行训练。所有的东西加起来大概有 75 行代码，我将在这篇博文中尽可能地详细解释。</p><h3 id="第一步：准备数据"><a href="#第一步：准备数据" class="headerlink" title="第一步：准备数据"></a>第一步：准备数据</h3><p>首先，我们要下载数据。我使用的是<ruby>古登堡项目 <rt>Project Gutenberg</rt></ruby>中的这个数据：<a target="_blank" rel="noopener" href="https://www.gutenberg.org/cache/epub/27200/pg27200.txt">Hans Christian Anderson’s fairy tales</a>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!wget -O fairy-tales.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个是准备数据的代码。我使用 <code>fastai</code> 库中的 <code>Vocab</code> 类进行数据处理，它能将一堆字母转换成“词表”，然后用这个“词表”把字母变成数字。</p><p>之后我们就得到了一个大的数字数组（<code>training_set</code>），我们可以用于训练我们的模型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from fastai.text import *</span><br><span class="line">text = unidecode.unidecode(open(&#x27;fairy-tales.txt&#x27;).read())</span><br><span class="line">v = Vocab.create((x for x in text), max_vocab=400, min_freq=1)</span><br><span class="line">training_set = torch.Tensor(v.numericalize([x for x in text])).type(torch.LongTensor).cuda()</span><br><span class="line">num_letters = len(v.itos)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="第二步：定义模型"><a href="#第二步：定义模型" class="headerlink" title="第二步：定义模型"></a>第二步：定义模型</h3><p>这个是 PyTorch 中 <code>LSTM</code> 类的封装。除了封装 <code>LSTM</code> 类以外，它还做了三件事：</p><ol><li>对输入向量进行 one-hot 编码，使得它们具有正确的维度。</li><li>在 <code>LSTM</code> 层后一层添加一个线性变换，因为 <code>LSTM</code> 输出的是一个长度为 <code>hidden_size</code> 的向量，我们需要的是一个长度为 <code>input_size</code> 的向量这样才能把它变成一个字符。</li><li>把 <code>LSTM</code> 隐藏层的输出向量（实际上有 2 个向量）保存成实例变量，然后在每轮运行结束后执行 <code>.detach()</code> 函数。（我很难解释清 <code>.detach()</code> 的作用，但我的理解是，它在某种程度上“结束”了模型的求导计算）（LCTT 译注：<code>detach()</code> 函数是将该张量的 <code>requires_grad</code> 参数设置为 <code>False</code>，即反向传播到该张量就结束。）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class MyLSTM(nn.Module):</span><br><span class="line">    def __init__(self, input_size, hidden_size):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)</span><br><span class="line">        self.h2o = nn.Linear(hidden_size, input_size)</span><br><span class="line">        self.input_size=input_size</span><br><span class="line">        self.hidden = None</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        input = torch.nn.functional.one_hot(input, num_classes=self.input_size).type(torch.FloatTensor).cuda().unsqueeze(0)</span><br><span class="line">        if self.hidden is None:</span><br><span class="line">            l_output, self.hidden = self.lstm(input)</span><br><span class="line">        else:</span><br><span class="line">            l_output, self.hidden = self.lstm(input, self.hidden)</span><br><span class="line">        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())</span><br><span class="line"></span><br><span class="line">        return self.h2o(l_output)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个代码还做了一些比较神奇但是不太明显的功能。如果你的输入是一个向量（比如 <code>[1,2,3,4,5,6]</code>），对应六个字母，那么我的理解是 <code>nn.LSTM</code> 会在内部使用<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Backpropagation_through_time">沿时间反向传播</a>更新隐藏向量 6 次。</p><h3 id="第三步：编写训练代码"><a href="#第三步：编写训练代码" class="headerlink" title="第三步：编写训练代码"></a>第三步：编写训练代码</h3><p>模型不会自己训练的！</p><p>我最开始的时候尝试用 <code>fastai</code> 库中的一个辅助类（也是 PyTorch 中的封装）。我有点疑惑因为我不知道它在做什么，所以最后我自己编写了模型训练代码。</p><p>下面这些代码（<code>epoch()</code> 方法）就是有关于一轮训练过程的基本信息。基本上就是重复做下面这几件事情：</p><ol><li>往 RNN 模型中传入一个字符串，比如 <code>and they ought not to teas</code>。（要以数字向量的形式传入）</li><li>得到下一个字母的预测结果</li><li>计算 RNN 模型预测结果和真实的下一个字母之间的损失函数（<code>e</code>，因为 <code>tease</code> 这个单词是以 <code>e</code> 结尾的）</li><li>计算梯度（用 <code>loss.backward()</code> 函数）</li><li>沿着梯度下降的方向修改模型中参数的权重（用 <code>self.optimizer.step()</code> 函数）</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Trainer():</span><br><span class="line">  def __init__(self):</span><br><span class="line">      self.rnn = MyLSTM(input_size, hidden_size).cuda()</span><br><span class="line">      self.optimizer = torch.optim.Adam(self.rnn.parameters(), amsgrad=True, lr=lr)</span><br><span class="line">  def epoch(self):</span><br><span class="line">      i = 0</span><br><span class="line">      while i &lt; len(training_set) - 40:</span><br><span class="line">        seq_len = random.randint(10, 40)</span><br><span class="line">        input, target = training_set[i:i+seq_len],training_set[i+1:i+1+seq_len]</span><br><span class="line">        i += seq_len</span><br><span class="line">        # forward pass</span><br><span class="line">        output = self.rnn(input)</span><br><span class="line">        loss = F.cross_entropy(output.squeeze()[-1:], target[-1:])</span><br><span class="line">        # compute gradients and take optimizer step</span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="使用-nn-LSTM-沿着时间反向传播，不要自己写代码"><a href="#使用-nn-LSTM-沿着时间反向传播，不要自己写代码" class="headerlink" title="使用 nn.LSTM 沿着时间反向传播，不要自己写代码"></a>使用 nn.LSTM 沿着时间反向传播，不要自己写代码</h3><p>开始的时候我自己写代码每次传一个字母到 LSTM 层中，之后定期计算导数，就像下面这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for i in range(20):</span><br><span class="line">    input, target = next(iter)</span><br><span class="line">    output, hidden = self.lstm(input, hidden)</span><br><span class="line">loss = F.cross_entropy(output, target)</span><br><span class="line">hidden = hidden.detach()</span><br><span class="line">self.optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">self.optimizer.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这段代码每次传入 20 个字母，每次一个，并且在最后训练了一次。这个步骤就被称为<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Backpropagation_through_time">沿时间反向传播</a>，Karpathy 在他的博客中就是用这种方法。</p><p>这个方法有些用处，我编写的损失函数开始能够下降一段时间，但之后就会出现峰值。我不知道为什么会出现这种现象，但之后我改为一次传入 20 个字符到 LSTM 之后（按 <code>seq_len</code> 维度），再进行反向传播，情况就变好了。</p><h3 id="第四步：训练模型！"><a href="#第四步：训练模型！" class="headerlink" title="第四步：训练模型！"></a>第四步：训练模型！</h3><p>我在同样的数据上重复执行了这个训练代码大概 300 次，直到模型开始输出一些看起来像英文的文本。差不多花了一个多小时吧。</p><p>这种情况下我也不关注模型是不是过拟合了，但是如果你在真实场景中训练模型，应该要在验证集上验证你的模型。</p><h3 id="第五步：生成输出！"><a href="#第五步：生成输出！" class="headerlink" title="第五步：生成输出！"></a>第五步：生成输出！</h3><p>最后一件要做的事就是用这个模型生成一些输出。我写了一个辅助方法从这个训练好的模型中生成文本（<code>make_preds</code> 和 <code>next_pred</code>）。这里主要是把向量的维度对齐，重要的一点是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output = rnn(input)</span><br><span class="line">prediction_vector = F.softmax(output/temperature)</span><br><span class="line">letter = v.textify(torch.multinomial(prediction_vector, 1).flatten(), sep=&#x27;&#x27;).replace(&#x27;_&#x27;, &#x27; &#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>基本上做的事情就是这些：</p><ol><li>RNN 层为字母表中的每一个字母或者符号输出一个数值向量（<code>output</code>）。</li><li>这个 <code>output</code> 向量<strong>并不是</strong>一个概率向量，所以需要 <code>F.softmax(output/temperature)</code> 操作，将其转换为概率值（也就是所有数值加起来和为 1）。<code>temperature</code> 某种程度上控制了对更高概率的权重，在限制范围内，如果设置 <code>temperature=0.0000001</code>，它将始终选择概率最高的字母。</li><li><code>torch.multinomial(prediction_vector)</code> 用于获取概率向量，并使用这些概率在向量中选择一个索引（如 <code>12</code>）。</li><li><code>v.textify</code> 把 <code>12</code> 转换为字母。</li></ol><p>如果我们想要处理的文本长度为 300，那么只需要重复这个过程 300 次就可以了。</p><h3 id="结果！"><a href="#结果！" class="headerlink" title="结果！"></a>结果！</h3><p>我把预测函数中的参数设置为 <code>temperature = 1</code> 得到了下面的这些由模型生成的结果。看起来有点像英语，这个结果已经很不错了，因为这个模型要从头开始“学习”英语，并且是在字符序列的级别上进行学习的。</p><p>虽然这些话没有什么<em>含义</em>，但我们也不知道到底想要得到什么输出。</p><blockquote><p>“An who was you colotal said that have to have been a little crimantable and beamed home the beetle. “I shall be in the head of the green for the sound of the wood. The pastor. “I child hand through the emperor’s sorthes, where the mother was a great deal down the conscious, which are all the gleam of the wood they saw the last great of the emperor’s forments, the house of a large gone there was nothing of the wonded the sound of which she saw in the converse of the beetle. “I shall know happy to him. This stories herself and the sound of the young mons feathery in the green safe.”</p><p>“That was the pastor. The some and hand on the water sound of the beauty be and home to have been consider and tree and the face. The some to the froghesses and stringing to the sea, and the yellow was too intention, he was not a warm to the pastor. The pastor which are the faten to go and the world from the bell, why really the laborer’s back of most handsome that she was a caperven and the confectioned and thoughts were seated to have great made</p></blockquote><p>下面这些结果是当 <code>temperature=0.1</code> 时生成的，它选择字符的方式更接近于“每次都选择出现概率最高的字符”。这就使得输出结果有很多是重复的。</p><blockquote><p>ole the sound of the beauty of the beetle. “She was a great emperor of the sea, and the sun was so warm to the confectioned the beetle. “I shall be so many for the beetle. “I shall be so many for the beetle. “I shall be so standen for the world, and the sun was so warm to the sea, and the sun was so warm to the sea, and the sound of the world from the bell, where the beetle was the sea, and the sound of the world from the bell, where the beetle was the sea, and the sound of the wood flowers and the sound of the wood, and the sound of the world from the bell, where the world from the wood, and the sound of the</p></blockquote><p>这段输出对这几个单词 <code>beetles</code>、<code>confectioners</code>、<code>sun</code> 和 <code>sea</code> 有着奇怪的执念。</p><h3 id="总结！"><a href="#总结！" class="headerlink" title="总结！"></a>总结！</h3><p>至此，我的结果远不及 Karpathy 的好，可能有一下几个原因：</p><ol><li>没有足够多的训练数据。</li><li>训练了一个小时之后我就没有耐心去查看 Colab 笔记本上的信息。</li><li>Karpathy 使用了两层LSTM，包含了更多的参数，而我只使用了一层。</li><li>完全是另一回事。</li></ol><p>但我得到了一些大致说得过去的结果！还不错！</p><hr><p>via: <a target="_blank" rel="noopener" href="https://jvns.ca/blog/2020/11/30/implement-char-rnn-in-pytorch/">https://jvns.ca/blog/2020/11/30/implement-char-rnn-in-pytorch/</a></p><p>作者：<a target="_blank" rel="noopener" href="https://jvns.ca/">Julia Evans</a> 选题：<a target="_blank" rel="noopener" href="https://github.com/lujun9972">lujun9972</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/zxp93">zhangxiangping</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p></div></div></body></html>