<!doctype html><html lang="en"><head><meta name="description" content="一个LinuxCN的镜像站"><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>使用深度学习检测疟疾 - 归墟星火集 又一个 LinuxCN 站点</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 又一个 LinuxCN 站点 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">使用深度学习检测疟疾</h1><span class="post-date">2019-05-24</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/">软件开发</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/AI/">AI</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> <a href="/tags/%E7%96%9F%E7%96%BE/">疟疾</a></div><div class="post-content"><blockquote><p>人工智能结合开源硬件工具能够提升严重传染病疟疾的诊断。</p></blockquote><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020031gvej75e5rmu5qkz5.png"></p><p>人工智能（AI）和开源工具、技术和框架是促进社会进步的强有力的结合。“健康就是财富”可能有点陈词滥调，但它却是非常准确的！在本篇文章，我们将测试 AI 是如何与低成本、有效、精确的开源深度学习方法结合起来一起用来检测致死的传染病疟疾。</p><p>我既不是一个医生，也不是一个医疗保健研究者，我也绝不像他们那样合格，我只是对将 AI 应用到医疗保健研究感兴趣。在这片文章中我的想法是展示 AI 和开源解决方案如何帮助疟疾检测和减少人工劳动的方法。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020032sbtcaoncgn495557.png" alt="Python and TensorFlow" title="Python and TensorFlow"></p><p><em>Python 和 TensorFlow: 一个构建开源深度学习方法的很棒的结合</em></p><p>感谢 Python 的强大和像 TensorFlow 这样的深度学习框架，我们能够构建健壮的、大规模的、有效的深度学习方法。因为这些工具是自由和开源的，我们能够构建非常经济且易于被任何人采纳和使用的解决方案。让我们开始吧！</p><h3 id="项目动机"><a href="#项目动机" class="headerlink" title="项目动机"></a>项目动机</h3><p>疟疾是由<em>疟原虫</em>造成的致死的、有传染性的、蚊子传播的疾病，主要通过受感染的雌性按蚊叮咬传播。共有五种寄生虫能够引起疟疾，但是大多数病例是这两种类型造成的：恶性疟原虫和间日疟原虫。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020032i8it8lf4q2at5wzn.png" alt="疟疾热图" title="Malaria heat map"></p><p>这个地图显示了疟疾在全球传播分布形势，尤其在热带地区，但疾病的性质和致命性是该项目的主要动机。</p><p>如果一只受感染雌性蚊子叮咬了你，蚊子携带的寄生虫进入你的血液，并且开始破坏携带氧气的红细胞（RBC）。通常，疟疾的最初症状类似于流感病毒，在蚊子叮咬后，他们通常在几天或几周内发作。然而，这些致死的寄生虫可以在你的身体里生存长达一年并且不会造成任何症状，延迟治疗可能造成并发症甚至死亡。因此，早期的检查能够挽救生命。</p><p>世界健康组织（WHO）的<a target="_blank" rel="noopener" href="https://www.who.int/features/factfiles/malaria/en/">疟疾实情</a>表明，世界近乎一半的人口面临疟疾的风险，有超过 2 亿的疟疾病例，每年由于疟疾造成的死亡将近 40 万。这是使疟疾检测和诊断快速、简单和有效的一个动机。</p><h3 id="检测疟疾的方法"><a href="#检测疟疾的方法" class="headerlink" title="检测疟疾的方法"></a>检测疟疾的方法</h3><p>有几种方法能够用来检测和诊断疟疾。该文中的项目就是基于 Rajaraman, et al. 的论文：“<a target="_blank" rel="noopener" href="https://peerj.com/articles/4568/">预先训练的卷积神经网络作为特征提取器，用于改善薄血涂片图像中的疟疾寄生虫检测</a>”介绍的一些方法，包含聚合酶链反应（PCR）和快速诊断测试（RDT）。这两种测试通常用于无法提供高质量显微镜服务的地方。</p><p>标准的疟疾诊断通常是基于血液涂片工作流程的，根据 Carlos Ariza 的文章“<a target="_blank" rel="noopener" href="https://blog.insightdatascience.com/https-blog-insightdatascience-com-malaria-hero-a47d3d5fc4bb">Malaria Hero：一个更快诊断疟原虫的网络应用</a>”，我从中了解到 Adrian Rosebrock 的“<a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2018/12/03/deep-learning-and-medical-image-analysis-with-keras/">使用 Keras 的深度学习和医学图像分析</a>”。我感激这些优秀的资源的作者，让我在疟原虫预防、诊断和治疗方面有了更多的想法。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020033dmm6sm1pp6wskwwm.png" alt="疟原虫检测的血涂片工作流程" title="Blood smear workflow for Malaria detection"></p><p><em>一个疟原虫检测的血涂片工作流程</em></p><p>根据 WHO 方案，诊断通常包括对放大 100 倍的血涂片的集中检测。受过训练的人们手工计算在 5000 个细胞中有多少红细胞中包含疟原虫。正如上述解释中引用的 Rajaraman， et al. 的论文：</p><blockquote><p>厚血涂片有助于检测寄生虫的存在，而薄血涂片有助于识别引起感染的寄生虫种类（疾病控制和预防中心, 2012）。诊断准确性在很大程度上取决于诊断人的专业知识，并且可能受到观察者间差异和疾病流行&#x2F;资源受限区域大规模诊断所造成的不利影响（Mitiku, Mengistu 和 Gelaw, 2003）。可替代的技术是使用聚合酶链反应（PCR）和快速诊断测试（RDT）；然而，PCR 分析受限于它的性能（Hommelsheim, et al., 2014），RDT 在疾病流行的地区成本效益低（Hawkes, Katsuva 和 Masumbuko, 2009）。</p></blockquote><p>因此，疟疾检测可能受益于使用机器学习的自动化。</p><h3 id="疟疾检测的深度学习"><a href="#疟疾检测的深度学习" class="headerlink" title="疟疾检测的深度学习"></a>疟疾检测的深度学习</h3><p>人工诊断血涂片是一个繁重的手工过程，需要专业知识来分类和计数被寄生虫感染的和未感染的细胞。这个过程可能不能很好的规模化，尤其在那些专业人士不足的地区。在利用最先进的图像处理和分析技术提取人工选取特征和构建基于机器学习的分类模型方面取得了一些进展。然而，这些模型不能大规模推广，因为没有更多的数据用来训练，并且人工选取特征需要花费很长时间。</p><p>深度学习模型，或者更具体地讲，卷积神经网络（CNN），已经被证明在各种计算机视觉任务中非常有效。（如果你想更多的了解关于 CNN 的背景知识，我推荐你阅读<a target="_blank" rel="noopener" href="http://cs231n.github.io/convolutional-networks/">视觉识别的 CS2331n 卷积神经网络</a>。）简单地讲，CNN 模型的关键层包含卷积和池化层，正如下图所示。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020034xybcrznfbfrnlnbk.png" alt="A typical CNN architecture" title="A typical CNN architecture"></p><p><em>一个典型的 CNN 架构</em></p><p>卷积层从数据中学习空间层级模式，它是平移不变的，因此它们能够学习图像的不同方面。例如，第一个卷积层将学习小的和局部图案，例如边缘和角落，第二个卷积层将基于第一层的特征学习更大的图案，等等。这允许 CNN 自动化提取特征并且学习对于新数据点通用的有效的特征。池化层有助于下采样和减少尺寸。</p><p>因此，CNN 有助于自动化和规模化的特征工程。同样，在模型末尾加上密集层允许我们执行像图像分类这样的任务。使用像 CNN 这样的深度学习模型自动的疟疾检测可能非常有效、便宜和具有规模性，尤其是迁移学习和预训练模型效果非常好，甚至在少量数据的约束下。</p><p>Rajaraman, et al. 的论文在一个数据集上利用六个预训练模型在检测疟疾对比无感染样本获取到令人吃惊的 95.9% 的准确率。我们的重点是从头开始尝试一些简单的 CNN 模型和用一个预训练的训练模型使用迁移学习来查看我们能够从相同的数据集中得到什么。我们将使用开源工具和框架，包括 Python 和 TensorFlow，来构建我们的模型。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们分析的数据来自 Lister Hill 国家生物医学交流中心（LHNCBC）的研究人员，该中心是国家医学图书馆（NLM）的一部分，他们细心收集和标记了公开可用的健康和受感染的血涂片图像的<a target="_blank" rel="noopener" href="https://ceb.nlm.nih.gov/repositories/malaria-datasets/">数据集</a>。这些研究者已经开发了一个运行在 Android 智能手机的<a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/pubmed/29360430">疟疾检测手机应用</a>，连接到一个传统的光学显微镜。它们使用吉姆萨染液将 150 个受恶性疟原虫感染的和 50 个健康病人的薄血涂片染色，这些薄血涂片是在孟加拉的吉大港医学院附属医院收集和照相的。使用智能手机的内置相机获取每个显微镜视窗内的图像。这些图片由在泰国曼谷的马希多-牛津热带医学研究所的一个专家使用幻灯片阅读器标记的。</p><p>让我们简要地查看一下数据集的结构。首先，我将安装一些基础的依赖（基于使用的操作系统）。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020034txfwzh2g7943l999.png" alt="Installing dependencies" title="Installing dependencies"></p><p>我使用的是云上的带有一个 GPU 的基于 Debian 的操作系统，这样我能更快的运行我的模型。为了查看目录结构，我们必须使用 <code>sudo apt install tree</code> 安装 <code>tree</code> 及其依赖（如果我们没有安装的话）。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020035atuk6kb6z75xbvkk.png" alt="Installing the tree dependency" title="Installing the tree dependency"></p><p>我们有两个文件夹包含血细胞的图像，包括受感染的和健康的。我们通过输入可以获取关于图像总数更多的细节：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line"></span><br><span class="line">base_dir = os.path.join(&#x27;./cell_images&#x27;)</span><br><span class="line">infected_dir = os.path.join(base_dir,&#x27;Parasitized&#x27;)</span><br><span class="line">healthy_dir = os.path.join(base_dir,&#x27;Uninfected&#x27;)</span><br><span class="line"></span><br><span class="line">infected_files = glob.glob(infected_dir+&#x27;/*.png&#x27;)</span><br><span class="line">healthy_files = glob.glob(healthy_dir+&#x27;/*.png&#x27;)</span><br><span class="line">len(infected_files), len(healthy_files)</span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">(13779, 13779)</span><br></pre></td></tr></table></figure><p>看起来我们有一个平衡的数据集，包含 13,779 张疟疾的和 13,779 张非疟疾的（健康的）血细胞图像。让我们根据这些构建数据帧，我们将用这些数据帧来构建我们的数据集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">np.random.seed(42)</span><br><span class="line"></span><br><span class="line">files_df = pd.DataFrame(&#123;</span><br><span class="line">    &#x27;filename&#x27;: infected_files + healthy_files,</span><br><span class="line">    &#x27;label&#x27;: [&#x27;malaria&#x27;] * len(infected_files) + [&#x27;healthy&#x27;] * len(healthy_files)</span><br><span class="line">&#125;).sample(frac=1, random_state=42).reset_index(drop=True)</span><br><span class="line"></span><br><span class="line">files_df.head()</span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020035m15h5kk31qq3zq3k.png" alt="Datasets" title="Datasets"></p><h3 id="构建和了解图像数据集"><a href="#构建和了解图像数据集" class="headerlink" title="构建和了解图像数据集"></a>构建和了解图像数据集</h3><p>为了构建深度学习模型，我们需要训练数据，但是我们还需要使用不可见的数据测试模型的性能。相应的，我们将使用 60:10:30 的比例来划分用于训练、验证和测试的数据集。我们将在训练期间应用训练和验证数据集，并用测试数据集来检查模型的性能。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from collections import Counter</span><br><span class="line"></span><br><span class="line">train_files, test_files, train_labels, test_labels = train_test_split(files_df[&#x27;filename&#x27;].values,</span><br><span class="line">                                                                      files_df[&#x27;label&#x27;].values, </span><br><span class="line">                                                                      test_size=0.3, random_state=42)</span><br><span class="line">train_files, val_files, train_labels, val_labels = train_test_split(train_files,</span><br><span class="line">                                                                    train_labels, </span><br><span class="line">                                                                    test_size=0.1, random_state=42)</span><br><span class="line"></span><br><span class="line">print(train_files.shape, val_files.shape, test_files.shape)</span><br><span class="line">print(&#x27;Train:&#x27;, Counter(train_labels), &#x27;\nVal:&#x27;, Counter(val_labels), &#x27;\nTest:&#x27;, Counter(test_labels))</span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">(17361,) (1929,) (8268,)</span><br><span class="line">Train: Counter(&#123;&#x27;healthy&#x27;: 8734, &#x27;malaria&#x27;: 8627&#125;) </span><br><span class="line">Val: Counter(&#123;&#x27;healthy&#x27;: 970, &#x27;malaria&#x27;: 959&#125;) </span><br><span class="line">Test: Counter(&#123;&#x27;malaria&#x27;: 4193, &#x27;healthy&#x27;: 4075&#125;)</span><br></pre></td></tr></table></figure><p>这些图片尺寸并不相同，因为血涂片和细胞图像是基于人、测试方法、图片方向不同而不同的。让我们总结我们的训练数据集的统计信息来决定最佳的图像尺寸（牢记，我们根本不会碰测试数据集）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">from concurrent import futures</span><br><span class="line">import threading</span><br><span class="line"></span><br><span class="line">def get_img_shape_parallel(idx, img, total_imgs):</span><br><span class="line">    if idx % 5000 == 0 or idx == (total_imgs - 1):</span><br><span class="line">        print(&#x27;&#123;&#125;: working on img num: &#123;&#125;&#x27;.format(threading.current_thread().name,</span><br><span class="line">                                                  idx))</span><br><span class="line">    return cv2.imread(img).shape</span><br><span class="line">  </span><br><span class="line">ex = futures.ThreadPoolExecutor(max_workers=None)</span><br><span class="line">data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]</span><br><span class="line">print(&#x27;Starting Img shape computation:&#x27;)</span><br><span class="line">train_img_dims_map = ex.map(get_img_shape_parallel, </span><br><span class="line">                            [record[0] for record in data_inp],</span><br><span class="line">                            [record[1] for record in data_inp],</span><br><span class="line">                            [record[2] for record in data_inp])</span><br><span class="line">train_img_dims = list(train_img_dims_map)</span><br><span class="line">print(&#x27;Min Dimensions:&#x27;, np.min(train_img_dims, axis=0)) </span><br><span class="line">print(&#x27;Avg Dimensions:&#x27;, np.mean(train_img_dims, axis=0))</span><br><span class="line">print(&#x27;Median Dimensions:&#x27;, np.median(train_img_dims, axis=0))</span><br><span class="line">print(&#x27;Max Dimensions:&#x27;, np.max(train_img_dims, axis=0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Starting Img shape computation:</span><br><span class="line">ThreadPoolExecutor-0_0: working on img num: 0</span><br><span class="line">ThreadPoolExecutor-0_17: working on img num: 5000</span><br><span class="line">ThreadPoolExecutor-0_15: working on img num: 10000</span><br><span class="line">ThreadPoolExecutor-0_1: working on img num: 15000</span><br><span class="line">ThreadPoolExecutor-0_7: working on img num: 17360</span><br><span class="line">Min Dimensions: [46 46  3]</span><br><span class="line">Avg Dimensions: [132.77311215 132.45757733   3.]</span><br><span class="line">Median Dimensions: [130. 130.   3.]</span><br><span class="line">Max Dimensions: [385 394   3]</span><br></pre></td></tr></table></figure><p>我们应用并行处理来加速图像读取，并且基于汇总统计结果，我们将每幅图片的尺寸重新调整到 125x125 像素。让我们载入我们所有的图像并重新调整它们为这些固定尺寸。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">IMG_DIMS = (125, 125)</span><br><span class="line"></span><br><span class="line">def get_img_data_parallel(idx, img, total_imgs):</span><br><span class="line">    if idx % 5000 == 0 or idx == (total_imgs - 1):</span><br><span class="line">        print(&#x27;&#123;&#125;: working on img num: &#123;&#125;&#x27;.format(threading.current_thread().name,</span><br><span class="line">                                                  idx))</span><br><span class="line">    img = cv2.imread(img)</span><br><span class="line">    img = cv2.resize(img, dsize=IMG_DIMS, </span><br><span class="line">                     interpolation=cv2.INTER_CUBIC)</span><br><span class="line">    img = np.array(img, dtype=np.float32)</span><br><span class="line">    return img</span><br><span class="line"></span><br><span class="line">ex = futures.ThreadPoolExecutor(max_workers=None)</span><br><span class="line">train_data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]</span><br><span class="line">val_data_inp = [(idx, img, len(val_files)) for idx, img in enumerate(val_files)]</span><br><span class="line">test_data_inp = [(idx, img, len(test_files)) for idx, img in enumerate(test_files)]</span><br><span class="line"></span><br><span class="line">print(&#x27;Loading Train Images:&#x27;)</span><br><span class="line">train_data_map = ex.map(get_img_data_parallel, </span><br><span class="line">                        [record[0] for record in train_data_inp],</span><br><span class="line">                        [record[1] for record in train_data_inp],</span><br><span class="line">                        [record[2] for record in train_data_inp])</span><br><span class="line">train_data = np.array(list(train_data_map))</span><br><span class="line"></span><br><span class="line">print(&#x27;\nLoading Validation Images:&#x27;)</span><br><span class="line">val_data_map = ex.map(get_img_data_parallel, </span><br><span class="line">                        [record[0] for record in val_data_inp],</span><br><span class="line">                        [record[1] for record in val_data_inp],</span><br><span class="line">                        [record[2] for record in val_data_inp])</span><br><span class="line">val_data = np.array(list(val_data_map))</span><br><span class="line"></span><br><span class="line">print(&#x27;\nLoading Test Images:&#x27;)</span><br><span class="line">test_data_map = ex.map(get_img_data_parallel, </span><br><span class="line">                        [record[0] for record in test_data_inp],</span><br><span class="line">                        [record[1] for record in test_data_inp],</span><br><span class="line">                        [record[2] for record in test_data_inp])</span><br><span class="line">test_data = np.array(list(test_data_map))</span><br><span class="line"></span><br><span class="line">train_data.shape, val_data.shape, test_data.shape  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Loading Train Images:</span><br><span class="line">ThreadPoolExecutor-1_0: working on img num: 0</span><br><span class="line">ThreadPoolExecutor-1_12: working on img num: 5000</span><br><span class="line">ThreadPoolExecutor-1_6: working on img num: 10000</span><br><span class="line">ThreadPoolExecutor-1_10: working on img num: 15000</span><br><span class="line">ThreadPoolExecutor-1_3: working on img num: 17360</span><br><span class="line"></span><br><span class="line">Loading Validation Images:</span><br><span class="line">ThreadPoolExecutor-1_13: working on img num: 0</span><br><span class="line">ThreadPoolExecutor-1_18: working on img num: 1928</span><br><span class="line"></span><br><span class="line">Loading Test Images:</span><br><span class="line">ThreadPoolExecutor-1_5: working on img num: 0</span><br><span class="line">ThreadPoolExecutor-1_19: working on img num: 5000</span><br><span class="line">ThreadPoolExecutor-1_8: working on img num: 8267</span><br><span class="line">((17361, 125, 125, 3), (1929, 125, 125, 3), (8268, 125, 125, 3))</span><br></pre></td></tr></table></figure><p>我们再次应用并行处理来加速有关图像载入和重新调整大小的计算。最终，我们获得了所需尺寸的图片张量，正如前面的输出所示。我们现在查看一些血细胞图像样本，以对我们的数据有个印象。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(1 , figsize = (8 , 8))</span><br><span class="line">n = 0 </span><br><span class="line">for i in range(16):</span><br><span class="line">    n += 1 </span><br><span class="line">    r = np.random.randint(0 , train_data.shape[0] , 1)</span><br><span class="line">    plt.subplot(4 , 4 , n)</span><br><span class="line">    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)</span><br><span class="line">    plt.imshow(train_data[r[0]]/255.)</span><br><span class="line">    plt.title(&#x27;&#123;&#125;&#x27;.format(train_labels[r[0]]))</span><br><span class="line">    plt.xticks([]) , plt.yticks([])</span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020035vmyssdy5d1p7nadz.png" alt="Malaria cell samples" title="Malaria cell samples"></p><p>基于这些样本图像，我们看到一些疟疾和健康细胞图像的细微不同。我们将使我们的深度学习模型试图在模型训练中学习这些模式。</p><p>开始我们的模型训练前，我们必须建立一些基础的配置设置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = 64</span><br><span class="line">NUM_CLASSES = 2</span><br><span class="line">EPOCHS = 25</span><br><span class="line">INPUT_SHAPE = (125, 125, 3)</span><br><span class="line"></span><br><span class="line">train_imgs_scaled = train_data / 255.</span><br><span class="line">val_imgs_scaled = val_data / 255.</span><br><span class="line"></span><br><span class="line"># encode text category labels</span><br><span class="line">from sklearn.preprocessing import LabelEncoder</span><br><span class="line"></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">le.fit(train_labels)</span><br><span class="line">train_labels_enc = le.transform(train_labels)</span><br><span class="line">val_labels_enc = le.transform(val_labels)</span><br><span class="line"></span><br><span class="line">print(train_labels[:6], train_labels_enc[:6])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">[&#x27;malaria&#x27; &#x27;malaria&#x27; &#x27;malaria&#x27; &#x27;healthy&#x27; &#x27;healthy&#x27; &#x27;malaria&#x27;] [1 1 1 0 0 1]</span><br></pre></td></tr></table></figure><p>我们修复我们的图像尺寸、批量大小，和纪元，并编码我们的分类的类标签。TensorFlow 2.0 于 2019 年三月发布，这个练习是尝试它的完美理由。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># Load the TensorBoard notebook extension (optional)</span><br><span class="line">%load_ext tensorboard.notebook</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(42)</span><br><span class="line">tf.__version__</span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">&#x27;2.0.0-alpha0&#x27;</span><br></pre></td></tr></table></figure><h3 id="深度学习训练"><a href="#深度学习训练" class="headerlink" title="深度学习训练"></a>深度学习训练</h3><p>在模型训练阶段，我们将构建三个深度训练模型，使用我们的训练集训练，使用验证数据比较它们的性能。然后，我们保存这些模型并在之后的模型评估阶段使用它们。</p><h4 id="模型-1：从头开始的-CNN"><a href="#模型-1：从头开始的-CNN" class="headerlink" title="模型 1：从头开始的 CNN"></a>模型 1：从头开始的 CNN</h4><p>我们的第一个疟疾检测模型将从头开始构建和训练一个基础的 CNN。首先，让我们定义我们的模型架构，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">inp = tf.keras.layers.Input(shape=INPUT_SHAPE)</span><br><span class="line"></span><br><span class="line">conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), </span><br><span class="line">                               activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(inp)</span><br><span class="line">pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)</span><br><span class="line">conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), </span><br><span class="line">                               activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(pool1)</span><br><span class="line">pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)</span><br><span class="line">conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), </span><br><span class="line">                               activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(pool2)</span><br><span class="line">pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)</span><br><span class="line"></span><br><span class="line">flat = tf.keras.layers.Flatten()(pool3)</span><br><span class="line"></span><br><span class="line">hidden1 = tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;)(flat)</span><br><span class="line">drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)</span><br><span class="line">hidden2 = tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;)(drop1)</span><br><span class="line">drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)</span><br><span class="line"></span><br><span class="line">out = tf.keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;)(drop2)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Model(inputs=inp, outputs=out)</span><br><span class="line">model.compile(optimizer=&#x27;adam&#x27;,</span><br><span class="line">                loss=&#x27;binary_crossentropy&#x27;,</span><br><span class="line">                metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Model: &quot;model&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 125, 125, 3)]     0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d (Conv2D)              (None, 125, 125, 32)      896       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 62, 62, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)            (None, 62, 62, 64)        18496     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 512)               262656    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 512)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (None, 1)                 513       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 15,102,529</span><br><span class="line">Trainable params: 15,102,529</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>基于这些代码的架构，我们的 CNN 模型有三个卷积和一个池化层，其后是两个致密层，以及用于正则化的失活。让我们训练我们的模型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">logdir = os.path.join(&#x27;/home/dipanzan_sarkar/projects/tensorboard_logs&#x27;, </span><br><span class="line">                      datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)</span><br><span class="line">reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.5,</span><br><span class="line">                              patience=2, min_lr=0.000001)</span><br><span class="line">callbacks = [reduce_lr, tensorboard_callback]</span><br><span class="line"></span><br><span class="line">history = model.fit(x=train_imgs_scaled, y=train_labels_enc, </span><br><span class="line">                    batch_size=BATCH_SIZE,</span><br><span class="line">                    epochs=EPOCHS, </span><br><span class="line">                    validation_data=(val_imgs_scaled, val_labels_enc), </span><br><span class="line">                    callbacks=callbacks,</span><br><span class="line">                    verbose=1)</span><br><span class="line">                    </span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Train on 17361 samples, validate on 1929 samples</span><br><span class="line">Epoch 1/25</span><br><span class="line">17361/17361 [====] - 32s 2ms/sample - loss: 0.4373 - accuracy: 0.7814 - val_loss: 0.1834 - val_accuracy: 0.9393</span><br><span class="line">Epoch 2/25</span><br><span class="line">17361/17361 [====] - 30s 2ms/sample - loss: 0.1725 - accuracy: 0.9434 - val_loss: 0.1567 - val_accuracy: 0.9513</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 24/25</span><br><span class="line">17361/17361 [====] - 30s 2ms/sample - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3693 - val_accuracy: 0.9565</span><br><span class="line">Epoch 25/25</span><br><span class="line">17361/17361 [====] - 30s 2ms/sample - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3699 - val_accuracy: 0.9559</span><br></pre></td></tr></table></figure><p>我们获得了 95.6% 的验证精确率，这很好，尽管我们的模型看起来有些过拟合（通过查看我们的训练精确度，是 99.9%）。通过绘制训练和验证的精度和损失曲线，我们可以清楚地看到这一点。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))</span><br><span class="line">t = f.suptitle(&#x27;Basic CNN Performance&#x27;, fontsize=12)</span><br><span class="line">f.subplots_adjust(top=0.85, wspace=0.3)</span><br><span class="line"></span><br><span class="line">max_epoch = len(history.history[&#x27;accuracy&#x27;])+1</span><br><span class="line">epoch_list = list(range(1,max_epoch))</span><br><span class="line">ax1.plot(epoch_list, history.history[&#x27;accuracy&#x27;], label=&#x27;Train Accuracy&#x27;)</span><br><span class="line">ax1.plot(epoch_list, history.history[&#x27;val_accuracy&#x27;], label=&#x27;Validation Accuracy&#x27;)</span><br><span class="line">ax1.set_xticks(np.arange(1, max_epoch, 5))</span><br><span class="line">ax1.set_ylabel(&#x27;Accuracy Value&#x27;)</span><br><span class="line">ax1.set_xlabel(&#x27;Epoch&#x27;)</span><br><span class="line">ax1.set_title(&#x27;Accuracy&#x27;)</span><br><span class="line">l1 = ax1.legend(loc=&quot;best&quot;)</span><br><span class="line"></span><br><span class="line">ax2.plot(epoch_list, history.history[&#x27;loss&#x27;], label=&#x27;Train Loss&#x27;)</span><br><span class="line">ax2.plot(epoch_list, history.history[&#x27;val_loss&#x27;], label=&#x27;Validation Loss&#x27;)</span><br><span class="line">ax2.set_xticks(np.arange(1, max_epoch, 5))</span><br><span class="line">ax2.set_ylabel(&#x27;Loss Value&#x27;)</span><br><span class="line">ax2.set_xlabel(&#x27;Epoch&#x27;)</span><br><span class="line">ax2.set_title(&#x27;Loss&#x27;)</span><br><span class="line">l2 = ax2.legend(loc=&quot;best&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020036c2li4aa0drahlh44.png" alt="Learning curves for basic CNN" title="Learning curves for basic CNN"></p><p><em>基础 CNN 学习曲线</em></p><p>我们可以看在在第五个纪元，情况并没有改善很多。让我们保存这个模型用于将来的评估。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(&#x27;basic_cnn.h5&#x27;)</span><br></pre></td></tr></table></figure><h4 id="深度迁移学习"><a href="#深度迁移学习" class="headerlink" title="深度迁移学习"></a>深度迁移学习</h4><p>就像人类有与生俱来在不同任务间传输知识的能力一样，迁移学习允许我们利用从以前任务学到的知识用到新的相关的任务，即使在机器学习或深度学习的情况下也是如此。如果想深入探究迁移学习，你应该看我的文章“<a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">一个易于理解与现实应用一起学习深度学习中的迁移学习的指导实践</a>”和我的书《<a target="_blank" rel="noopener" href="https://github.com/dipanjanS/hands-on-transfer-learning-with-python">Python 迁移学习实践</a>》。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020036i4u47qb3rx4444nq.png" alt="深度迁移学习的想法" title="Ideas for deep transfer learning"></p><p>在这篇实践中我们想要探索的想法是：</p><blockquote><p>在我们的问题背景下，我们能够利用一个预训练深度学习模型（在大数据集上训练的，像 ImageNet）通过应用和迁移知识来解决疟疾检测的问题吗？</p></blockquote><p>我们将应用两个最流行的深度迁移学习策略。</p><ul><li>预训练模型作为特征提取器</li><li>微调的预训练模型</li></ul><p>我们将使用预训练的 VGG-19 深度训练模型（由剑桥大学的视觉几何组（VGG）开发）进行我们的实验。像 VGG-19 这样的预训练模型是在一个大的数据集（<a target="_blank" rel="noopener" href="http://image-net.org/index">Imagenet</a>）上使用了很多不同的图像分类训练的。因此，这个模型应该已经学习到了健壮的特征层级结构，相对于你的 CNN 模型学到的特征，是空间不变的、转动不变的、平移不变的。因此，这个模型，已经从百万幅图片中学习到了一个好的特征显示，对于像疟疾检测这样的计算机视觉问题，可以作为一个好的合适新图像的特征提取器。在我们的问题中发挥迁移学习的能力之前，让我们先讨论 VGG-19 模型。</p><h5 id="理解-VGG-19-模型"><a href="#理解-VGG-19-模型" class="headerlink" title="理解 VGG-19 模型"></a>理解 VGG-19 模型</h5><p>VGG-19 模型是一个构建在 ImageNet 数据库之上的 19 层（卷积和全连接的）的深度学习网络，ImageNet 数据库为了图像识别和分类的目的而开发。该模型是由 Karen Simonyan 和 Andrew Zisserman 构建的，在他们的论文“<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">大规模图像识别的非常深的卷积网络</a>”中进行了描述。VGG-19 的架构模型是：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020037i9eizi6dia3z2zdj.png" alt="VGG-19 模型架构" title="VGG-19 Model Architecture"></p><p>你可以看到我们总共有 16 个使用 3x3 卷积过滤器的卷积层，与最大的池化层来下采样，和由 4096 个单元组成的两个全连接的隐藏层，每个隐藏层之后跟随一个由 1000 个单元组成的致密层，每个单元代表 ImageNet 数据库中的一个分类。我们不需要最后三层，因为我们将使用我们自己的全连接致密层来预测疟疾。我们更关心前五个块，因此我们可以利用 VGG 模型作为一个有效的特征提取器。</p><p>我们将使用模型之一作为一个简单的特征提取器，通过冻结五个卷积块的方式来确保它们的位权在每个纪元后不会更新。对于最后一个模型，我们会对 VGG 模型进行微调，我们会解冻最后两个块（第 4 和第 5）因此当我们训练我们的模型时，它们的位权在每个时期（每批数据）被更新。</p><h4 id="模型-2：预训练的模型作为一个特征提取器"><a href="#模型-2：预训练的模型作为一个特征提取器" class="headerlink" title="模型 2：预训练的模型作为一个特征提取器"></a>模型 2：预训练的模型作为一个特征提取器</h4><p>为了构建这个模型，我们将利用 TensorFlow 载入 VGG-19 模型并冻结卷积块，因此我们能够将它们用作特征提取器。我们在末尾插入我们自己的致密层来执行分类任务。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#x27;imagenet&#x27;, </span><br><span class="line">                                        input_shape=INPUT_SHAPE)</span><br><span class="line">vgg.trainable = False</span><br><span class="line"># Freeze the layers</span><br><span class="line">for layer in vgg.layers:</span><br><span class="line">    layer.trainable = False</span><br><span class="line">    </span><br><span class="line">base_vgg = vgg</span><br><span class="line">base_out = base_vgg.output</span><br><span class="line">pool_out = tf.keras.layers.Flatten()(base_out)</span><br><span class="line">hidden1 = tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;)(pool_out)</span><br><span class="line">drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)</span><br><span class="line">hidden2 = tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;)(drop1)</span><br><span class="line">drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)</span><br><span class="line"></span><br><span class="line">out = tf.keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;)(drop2)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Model(inputs=base_vgg.input, outputs=out)</span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),</span><br><span class="line">                loss=&#x27;binary_crossentropy&#x27;,</span><br><span class="line">                metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Model: &quot;model_1&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_2 (InputLayer)         [(None, 125, 125, 3)]     0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">block1_conv1 (Conv2D)        (None, 125, 125, 64)      1792      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">block1_conv2 (Conv2D)        (None, 125, 125, 64)      36928     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">_________________________________________________________________</span><br><span class="line">block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 4608)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 512)               2359808   </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 512)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_4 (Dense)              (None, 512)               262656    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_3 (Dropout)          (None, 512)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_5 (Dense)              (None, 1)                 513       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 22,647,361</span><br><span class="line">Trainable params: 2,622,977</span><br><span class="line">Non-trainable params: 20,024,384</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>从整个输出可以明显看出，在我们的模型中我们有了很多层，我们将只利用 VGG-19 模型的冻结层作为特征提取器。你可以使用下列代码来验证我们的模型有多少层是实际可训练的，以及我们的网络中总共存在多少层。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Total Layers:&quot;, len(model.layers))</span><br><span class="line">print(&quot;Total trainable layers:&quot;, </span><br><span class="line">      sum([1 for l in model.layers if l.trainable]))</span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Total Layers: 28</span><br><span class="line">Total trainable layers: 6</span><br></pre></td></tr></table></figure><p>我们将使用和我们之前的模型相似的配置和回调来训练我们的模型。参考<a target="_blank" rel="noopener" href="https://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/tree/master/os_malaria_detection/">我的 GitHub 仓库</a>以获取训练模型的完整代码。我们观察下列图表，以显示模型精确度和损失曲线。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020038jh4u1ux6lqlqblh3.png" alt="Learning curves for frozen pre-trained CNN" title="Learning curves for frozen pre-trained CNN"></p><p><em>冻结的预训练的 CNN 的学习曲线</em></p><p>这表明我们的模型没有像我们的基础 CNN 模型那样过拟合，但是性能有点不如我们的基础的 CNN 模型。让我们保存这个模型，以备将来的评估。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(&#x27;vgg_frozen.h5&#x27;)</span><br></pre></td></tr></table></figure><h4 id="模型-3：使用图像增强来微调预训练的模型"><a href="#模型-3：使用图像增强来微调预训练的模型" class="headerlink" title="模型 3：使用图像增强来微调预训练的模型"></a>模型 3：使用图像增强来微调预训练的模型</h4><p>在我们的最后一个模型中，我们将在预定义好的 VGG-19 模型的最后两个块中微调层的位权。我们同样引入了图像增强的概念。图像增强背后的想法和其名字一样。我们从训练数据集中载入现有图像，并且应用转换操作，例如旋转、裁剪、转换、放大缩小等等，来产生新的、改变过的版本。由于这些随机转换，我们每次获取到的图像不一样。我们将应用 tf.keras 中的一个名为 ImageDataGenerator 的优秀工具来帮助构建图像增强器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,</span><br><span class="line">                                                                zoom_range=0.05, </span><br><span class="line">                                                                rotation_range=25,</span><br><span class="line">                                                                width_shift_range=0.05, </span><br><span class="line">                                                                height_shift_range=0.05, </span><br><span class="line">                                                                shear_range=0.05, horizontal_flip=True, </span><br><span class="line">                                                                fill_mode=&#x27;nearest&#x27;)</span><br><span class="line"></span><br><span class="line">val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)</span><br><span class="line"></span><br><span class="line"># build image augmentation generators</span><br><span class="line">train_generator = train_datagen.flow(train_data, train_labels_enc, batch_size=BATCH_SIZE, shuffle=True)</span><br><span class="line">val_generator = val_datagen.flow(val_data, val_labels_enc, batch_size=BATCH_SIZE, shuffle=False)</span><br></pre></td></tr></table></figure><p>我们不会对我们的验证数据集应用任何转换（除非是调整大小，因为这是必须的），因为我们将使用它评估每个纪元的模型性能。对于在传输学习环境中的图像增强的详细解释，请随时查看我上面引用的<a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">文章</a>。让我们从一批图像增强转换中查看一些样本结果。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img_id = 0</span><br><span class="line">sample_generator = train_datagen.flow(train_data[img_id:img_id+1], train_labels[img_id:img_id+1],</span><br><span class="line">                                      batch_size=1)</span><br><span class="line">sample = [next(sample_generator) for i in range(0,5)]</span><br><span class="line">fig, ax = plt.subplots(1,5, figsize=(16, 6))</span><br><span class="line">print(&#x27;Labels:&#x27;, [item[1][0] for item in sample])</span><br><span class="line">l = [ax[i].imshow(sample[i][0][0]) for i in range(0,5)]</span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020039y00h0eg6wxfgx6gn.png" alt="Sample augmented images" title="Sample augmented images"></p><p>你可以清晰的看到与之前的输出的我们图像的轻微变化。我们现在构建我们的学习模型，确保 VGG-19 模型的最后两块是可以训练的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#x27;imagenet&#x27;, </span><br><span class="line">                                        input_shape=INPUT_SHAPE)</span><br><span class="line"># Freeze the layers</span><br><span class="line">vgg.trainable = True</span><br><span class="line"></span><br><span class="line">set_trainable = False</span><br><span class="line">for layer in vgg.layers:</span><br><span class="line">    if layer.name in [&#x27;block5_conv1&#x27;, &#x27;block4_conv1&#x27;]:</span><br><span class="line">        set_trainable = True</span><br><span class="line">    if set_trainable:</span><br><span class="line">        layer.trainable = True</span><br><span class="line">    else:</span><br><span class="line">        layer.trainable = False</span><br><span class="line">    </span><br><span class="line">base_vgg = vgg</span><br><span class="line">base_out = base_vgg.output</span><br><span class="line">pool_out = tf.keras.layers.Flatten()(base_out)</span><br><span class="line">hidden1 = tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;)(pool_out)</span><br><span class="line">drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)</span><br><span class="line">hidden2 = tf.keras.layers.Dense(512, activation=&#x27;relu&#x27;)(drop1)</span><br><span class="line">drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)</span><br><span class="line"></span><br><span class="line">out = tf.keras.layers.Dense(1, activation=&#x27;sigmoid&#x27;)(drop2)</span><br><span class="line"></span><br><span class="line">model = tf.keras.Model(inputs=base_vgg.input, outputs=out)</span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),</span><br><span class="line">                loss=&#x27;binary_crossentropy&#x27;,</span><br><span class="line">                metrics=[&#x27;accuracy&#x27;])</span><br><span class="line"></span><br><span class="line">print(&quot;Total Layers:&quot;, len(model.layers))</span><br><span class="line">print(&quot;Total trainable layers:&quot;, sum([1 for l in model.layers if l.trainable]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Total Layers: 28</span><br><span class="line">Total trainable layers: 16</span><br></pre></td></tr></table></figure><p>在我们的模型中我们降低了学习率，因为我们不想在微调的时候对预训练的层做大的位权更新。模型的训练过程可能有轻微的不同，因为我们使用了数据生成器，因此我们将应用 <code>fit_generator(...)</code> 函数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)</span><br><span class="line">reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.5,</span><br><span class="line">                              patience=2, min_lr=0.000001)</span><br><span class="line"></span><br><span class="line">callbacks = [reduce_lr, tensorboard_callback]</span><br><span class="line">train_steps_per_epoch = train_generator.n // train_generator.batch_size</span><br><span class="line">val_steps_per_epoch = val_generator.n // val_generator.batch_size</span><br><span class="line">history = model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch, epochs=EPOCHS,</span><br><span class="line">                              validation_data=val_generator, validation_steps=val_steps_per_epoch, </span><br><span class="line">                              verbose=1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">Epoch 1/25</span><br><span class="line">271/271 [====] - 133s 489ms/step - loss: 0.2267 - accuracy: 0.9117 - val_loss: 0.1414 - val_accuracy: 0.9531</span><br><span class="line">Epoch 2/25</span><br><span class="line">271/271 [====] - 129s 475ms/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1292 - val_accuracy: 0.9589</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 24/25</span><br><span class="line">271/271 [====] - 128s 473ms/step - loss: 0.0815 - accuracy: 0.9727 - val_loss: 0.1466 - val_accuracy: 0.9682</span><br><span class="line">Epoch 25/25</span><br><span class="line">271/271 [====] - 128s 473ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.1127 - val_accuracy: 0.9641</span><br></pre></td></tr></table></figure><p>这看起来是我们的最好的模型。它给了我们近乎 96.5% 的验证精确率，基于训练精度，它看起来不像我们的第一个模型那样过拟合。这可以通过下列的学习曲线验证。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020039t5t818u85u41pree.png" alt="Learning curves for fine-tuned pre-trained CNN" title="Learning curves for fine-tuned pre-trained CNN"></p><p><em>微调过的预训练 CNN 的学习曲线</em></p><p>让我们保存这个模型，因此我们能够在测试集上使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(&#x27;vgg_finetuned.h5&#x27;)</span><br></pre></td></tr></table></figure><p>这就完成了我们的模型训练阶段。现在我们准备好了在测试集上测试我们模型的性能。</p><h3 id="深度学习模型性能评估"><a href="#深度学习模型性能评估" class="headerlink" title="深度学习模型性能评估"></a>深度学习模型性能评估</h3><p>我们将通过在我们的测试集上做预测来评估我们在训练阶段构建的三个模型，因为仅仅验证是不够的！我们同样构建了一个检测工具模块叫做 <code>model_evaluation_utils</code>，我们可以使用相关分类指标用来评估使用我们深度学习模型的性能。第一步是扩展我们的数据集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_imgs_scaled = test_data / 255.</span><br><span class="line">test_imgs_scaled.shape, test_labels.shape</span><br><span class="line"></span><br><span class="line"># Output</span><br><span class="line">((8268, 125, 125, 3), (8268,))</span><br></pre></td></tr></table></figure><p>下一步包括载入我们保存的深度学习模型，在测试集上预测。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Load Saved Deep Learning Models</span><br><span class="line">basic_cnn = tf.keras.models.load_model(&#x27;./basic_cnn.h5&#x27;)</span><br><span class="line">vgg_frz = tf.keras.models.load_model(&#x27;./vgg_frozen.h5&#x27;)</span><br><span class="line">vgg_ft = tf.keras.models.load_model(&#x27;./vgg_finetuned.h5&#x27;)</span><br><span class="line"></span><br><span class="line"># Make Predictions on Test Data</span><br><span class="line">basic_cnn_preds = basic_cnn.predict(test_imgs_scaled, batch_size=512)</span><br><span class="line">vgg_frz_preds = vgg_frz.predict(test_imgs_scaled, batch_size=512)</span><br><span class="line">vgg_ft_preds = vgg_ft.predict(test_imgs_scaled, batch_size=512)</span><br><span class="line"></span><br><span class="line">basic_cnn_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 </span><br><span class="line">                                                  for pred in basic_cnn_preds.ravel()])</span><br><span class="line">vgg_frz_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 </span><br><span class="line">                                                  for pred in vgg_frz_preds.ravel()])</span><br><span class="line">vgg_ft_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 </span><br><span class="line">                                                  for pred in vgg_ft_preds.ravel()])</span><br></pre></td></tr></table></figure><p>下一步是应用我们的 <code>model_evaluation_utils</code> 模块根据相应分类指标来检查每个模块的性能。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import model_evaluation_utils as meu</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">basic_cnn_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=basic_cnn_pred_labels)</span><br><span class="line">vgg_frz_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_frz_pred_labels)</span><br><span class="line">vgg_ft_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_ft_pred_labels)</span><br><span class="line"></span><br><span class="line">pd.DataFrame([basic_cnn_metrics, vgg_frz_metrics, vgg_ft_metrics], </span><br><span class="line">             index=[&#x27;Basic CNN&#x27;, &#x27;VGG-19 Frozen&#x27;, &#x27;VGG-19 Fine-tuned&#x27;])</span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201905/24/020039pkgzie39gevrdfgy.png" alt="Model accuracy" title="Model accuracy"></p><p>看起来我们的第三个模型在我们的测试集上执行的最好，给出了一个模型精确性为 96% 的 F1 得分，这非常好，与我们之前提到的研究论文和文章中的更复杂的模型相当。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>疟疾检测不是一个简单的过程，全球的合格人员的不足在病例诊断和治疗当中是一个严重的问题。我们研究了一个关于疟疾的有趣的真实世界的医学影像案例。利用 AI 的、易于构建的、开源的技术在检测疟疾方面可以为我们提供最先进的精确性，因此使 AI 具有社会效益。</p><p>我鼓励你查看这篇文章中提到的文章和研究论文，没有它们，我就不能形成概念并写出来。如果你对运行和采纳这些技术感兴趣，本篇文章所有的代码都可以在<a target="_blank" rel="noopener" href="https://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/tree/master/os_malaria_detection/">我的 GitHub 仓库</a>获得。记得从<a target="_blank" rel="noopener" href="https://ceb.nlm.nih.gov/repositories/malaria-datasets/">官方网站</a>下载数据。</p><p>让我们希望在健康医疗方面更多的采纳开源的 AI 能力，使它在世界范围内变得更便宜、更易用。</p><hr><p>via: <a target="_blank" rel="noopener" href="https://opensource.com/article/19/4/detecting-malaria-deep-learning">https://opensource.com/article/19/4/detecting-malaria-deep-learning</a></p><p>作者：<a target="_blank" rel="noopener" href="https://opensource.com/users/djsarkar">Dipanjan (DJ) Sarkar</a> 选题：<a target="_blank" rel="noopener" href="https://github.com/lujun9972">lujun9972</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/warmfrog">warmfrog</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p><script defer src="https://pv.undefined.today/tracker.min.js" data-website-id="LinuxCNMirror-tracker"></script></div></div></body></html>