<!doctype html><html lang="en"><head><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>如何在 Linux 中找到并删除重复文件 - 归墟星火集</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a href="/about/">关于</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">如何在 Linux 中找到并删除重复文件</h1><span class="post-date">2018-10-16</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/%E5%88%A0%E9%99%A4/">删除</a> <a href="/tags/%E9%87%8D%E5%A4%8D/">重复</a></div><div class="post-content"><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201810/16/170704e7dhu41fqsrqkzf1.png"></p><p>在编辑或修改配置文件或旧文件前，我经常会把它们备份到硬盘的某个地方，因此我如果意外地改错了这些文件，我可以从备份中恢复它们。但问题是如果我忘记清理备份文件，一段时间之后，我的磁盘会被这些大量重复文件填满 —— 我觉得要么是懒得清理这些旧文件，要么是担心可能会删掉重要文件。如果你们像我一样，在类 Unix 操作系统中，大量多版本的相同文件放在不同的备份目录，你可以使用下面的工具找到并删除重复文件。</p><p><strong>提醒一句：</strong></p><p>在删除重复文件的时请尽量小心。如果你不小心，也许会导致<a target="_blank" rel="noopener" href="https://www.ostechnix.com/prevent-files-folders-accidental-deletion-modification-linux/">意外丢失数据</a>。我建议你在使用这些工具的时候要特别注意。</p><h3 id="在-Linux-中找到并删除重复文件"><a href="#在-Linux-中找到并删除重复文件" class="headerlink" title="在 Linux 中找到并删除重复文件"></a>在 Linux 中找到并删除重复文件</h3><p>出于本指南的目的，我将讨论下面的三个工具：</p><ol><li>Rdfind</li><li>Fdupes</li><li>FSlint</li></ol><p>这三个工具是自由开源的，且运行在大多数类 Unix 系统中。</p><h4 id="1-Rdfind"><a href="#1-Rdfind" class="headerlink" title="1. Rdfind"></a>1. Rdfind</h4><p><strong>Rdfind</strong> 意即 <strong>r</strong>edundant <strong>d</strong>ata <strong>find</strong>（冗余数据查找），是一个通过访问目录和子目录来找出重复文件的自由开源的工具。它是基于文件内容而不是文件名来比较。Rdfind 使用<strong>排序</strong>算法来区分原始文件和重复文件。如果你有两个或者更多的相同文件，Rdfind 会很智能的找到原始文件并认定剩下的文件为重复文件。一旦找到副本文件，它会向你报告。你可以决定是删除还是使用<a target="_blank" rel="noopener" href="https://www.ostechnix.com/explaining-soft-link-and-hard-link-in-linux-with-examples/">硬链接或者符号（软）链接</a>代替它们。</p><p><strong>安装 Rdfind</strong></p><p>Rdfind 存在于 <a target="_blank" rel="noopener" href="https://aur.archlinux.org/packages/rdfind/">AUR</a> 中。因此，在基于 Arch 的系统中，你可以像下面一样使用任一如 <a target="_blank" rel="noopener" href="https://www.ostechnix.com/yay-found-yet-another-reliable-aur-helper/">Yay</a> AUR 程序助手安装它。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yay -S rdfind</span><br></pre></td></tr></table></figure><p>在 Debian、Ubuntu、Linux Mint 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install rdfind</span><br></pre></td></tr></table></figure><p>在 Fedora 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dnf install rdfind</span><br></pre></td></tr></table></figure><p>在 RHEL、CentOS 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install epel-release</span><br><span class="line">$ sudo yum install rdfind</span><br></pre></td></tr></table></figure><p><strong>用法</strong></p><p>一旦安装完成，仅带上目录路径运行 Rdfind 命令就可以扫描重复文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind ~/Downloads</span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201810/16/170705e3dr7l37atsabsci.png"></p><p>正如你看到上面的截屏，Rdfind 命令将扫描 <code>~/Downloads</code> 目录，并将结果存储到当前工作目录下一个名为 <code>results.txt</code> 的文件中。你可以在 <code>results.txt</code> 文件中看到可能是重复文件的名字。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ cat results.txt</span><br><span class="line"># Automatically generated</span><br><span class="line"># duptype id depth size device inode priority name</span><br><span class="line">DUPTYPE_FIRST_OCCURRENCE 1469 8 9 2050 15864884 1 /home/sk/Downloads/tor-browser_en-US/Browser/TorBrowser/Tor/PluggableTransports/fte/tests/dfas/test5.regex</span><br><span class="line">DUPTYPE_WITHIN_SAME_TREE -1469 8 9 2050 15864886 1 /home/sk/Downloads/tor-browser_en-US/Browser/TorBrowser/Tor/PluggableTransports/fte/tests/dfas/test6.regex</span><br><span class="line">[...]</span><br><span class="line">DUPTYPE_FIRST_OCCURRENCE 13 0 403635 2050 15740257 1 /home/sk/Downloads/Hyperledger(1).pdf</span><br><span class="line">DUPTYPE_WITHIN_SAME_TREE -13 0 403635 2050 15741071 1 /home/sk/Downloads/Hyperledger.pdf</span><br><span class="line"># end of file</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>通过检查 <code>results.txt</code> 文件，你可以很容易的找到那些重复文件。如果愿意你可以手动的删除它们。</p><p>此外，你可在不修改其他事情情况下使用 <code>-dryrun</code> 选项找出所有重复文件，并在终端上输出汇总信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind -dryrun true ~/Downloads</span><br></pre></td></tr></table></figure><p>一旦找到重复文件，你可以使用硬链接或符号链接代替他们。</p><p>使用硬链接代替所有重复文件，运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind -makehardlinks true ~/Downloads</span><br></pre></td></tr></table></figure><p>使用符号链接&#x2F;软链接代替所有重复文件，运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind -makesymlinks true ~/Downloads</span><br></pre></td></tr></table></figure><p>目录中有一些空文件，也许你想忽略他们，你可以像下面一样使用 <code>-ignoreempty</code> 选项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind -ignoreempty true ~/Downloads</span><br></pre></td></tr></table></figure><p>如果你不再想要这些旧文件，删除重复文件，而不是使用硬链接或软链接代替它们。</p><p>删除重复文件，就运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind -deleteduplicates true ~/Downloads</span><br></pre></td></tr></table></figure><p>如果你不想忽略空文件，并且和所哟重复文件一起删除。运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind -deleteduplicates true -ignoreempty false ~/Downloads</span><br></pre></td></tr></table></figure><p>更多细节，参照帮助部分：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rdfind --help</span><br></pre></td></tr></table></figure><p>手册页：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ man rdfind</span><br></pre></td></tr></table></figure><h4 id="2-Fdupes"><a href="#2-Fdupes" class="headerlink" title="2. Fdupes"></a>2. Fdupes</h4><p><strong>Fdupes</strong> 是另一个在指定目录以及子目录中识别和移除重复文件的命令行工具。这是一个使用 C 语言编写的自由开源工具。Fdupes 通过对比文件大小、部分 MD5 签名、全部 MD5 签名，最后执行逐个字节对比校验来识别重复文件。</p><p>与 Rdfind 工具类似，Fdupes 附带非常少的选项来执行操作，如：</p><ul><li>在目录和子目录中递归的搜索重复文件</li><li>从计算中排除空文件和隐藏文件</li><li>显示重复文件大小</li><li>出现重复文件时立即删除</li><li>使用不同的拥有者&#x2F;组或权限位来排除重复文件</li><li>更多</li></ul><p><strong>安装 Fdupes</strong></p><p>Fdupes 存在于大多数 Linux 发行版的默认仓库中。</p><p>在 Arch Linux 和它的变种如 Antergos、Manjaro Linux 上，如下使用 Pacman 安装它。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pacman -S fdupes</span><br></pre></td></tr></table></figure><p>在 Debian、Ubuntu、Linux Mint 上:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install fdupes</span><br></pre></td></tr></table></figure><p>在 Fedora 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dnf install fdupes</span><br></pre></td></tr></table></figure><p>在 RHEL、CentOS 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install epel-release</span><br><span class="line">$ sudo yum install fdupes</span><br></pre></td></tr></table></figure><p><strong>用法</strong></p><p>Fdupes 用法非常简单。仅运行下面的命令就可以在目录中找到重复文件，如：<code>~/Downloads</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes ~/Downloads</span><br></pre></td></tr></table></figure><p>我系统中的样例输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/home/sk/Downloads/Hyperledger.pdf</span><br><span class="line">/home/sk/Downloads/Hyperledger(1).pdf</span><br></pre></td></tr></table></figure><p>你可以看到，在 <code>/home/sk/Downloads/</code> 目录下有一个重复文件。它仅显示了父级目录中的重复文件。如何显示子目录中的重复文件？像下面一样，使用 <code>-r</code> 选项。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -r ~/Downloads</span><br></pre></td></tr></table></figure><p>现在你将看到 <code>/home/sk/Downloads/</code> 目录以及子目录中的重复文件。</p><p>Fdupes 也可用来从多个目录中迅速查找重复文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes ~/Downloads ~/Documents/ostechnix</span><br></pre></td></tr></table></figure><p>你甚至可以搜索多个目录，递归搜索其中一个目录，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes ~/Downloads -r ~/Documents/ostechnix</span><br></pre></td></tr></table></figure><p>上面的命令将搜索 <code>~/Downloads</code> 目录，<code>~/Documents/ostechnix</code> 目录和它的子目录中的重复文件。</p><p>有时，你可能想要知道一个目录中重复文件的大小。你可以使用 <code>-S</code> 选项，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -S ~/Downloads</span><br><span class="line">403635 bytes each:</span><br><span class="line">/home/sk/Downloads/Hyperledger.pdf</span><br><span class="line">/home/sk/Downloads/Hyperledger(1).pdf</span><br></pre></td></tr></table></figure><p>类似的，为了显示父目录和子目录中重复文件的大小，使用 <code>-Sr</code> 选项。</p><p>我们可以在计算时分别使用 <code>-n</code> 和 <code>-A</code> 选项排除空白文件以及排除隐藏文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -n ~/Downloads</span><br><span class="line">$ fdupes -A ~/Downloads</span><br></pre></td></tr></table></figure><p>在搜索指定目录的重复文件时，第一个命令将排除零长度文件，后面的命令将排除隐藏文件。</p><p>汇总重复文件信息，使用 <code>-m</code> 选项。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -m ~/Downloads</span><br><span class="line">1 duplicate files (in 1 sets), occupying 403.6 kilobytes</span><br></pre></td></tr></table></figure><p>删除所有重复文件，使用 <code>-d</code> 选项。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -d ~/Downloads</span><br></pre></td></tr></table></figure><p>样例输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[1] /home/sk/Downloads/Hyperledger Fabric Installation.pdf</span><br><span class="line">[2] /home/sk/Downloads/Hyperledger Fabric Installation(1).pdf</span><br><span class="line"></span><br><span class="line">Set 1 of 1, preserve files [1 - 2, all]:</span><br></pre></td></tr></table></figure><p>这个命令将提示你保留还是删除所有其他重复文件。输入任一号码保留相应的文件，并删除剩下的文件。当使用这个选项的时候需要更加注意。如果不小心，你可能会删除原文件。</p><p>如果你想要每次保留每个重复文件集合的第一个文件，且无提示的删除其他文件，使用 <code>-dN</code> 选项（不推荐）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -dN ~/Downloads</span><br></pre></td></tr></table></figure><p>当遇到重复文件时删除它们，使用 <code>-I</code> 标志。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes -I ~/Downloads</span><br></pre></td></tr></table></figure><p>关于 Fdupes 的更多细节，查看帮助部分和 man 页面。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fdupes --help</span><br><span class="line">$ man fdupes</span><br></pre></td></tr></table></figure><h4 id="3-FSlint"><a href="#3-FSlint" class="headerlink" title="3. FSlint"></a>3. FSlint</h4><p><strong>FSlint</strong> 是另外一个查找重复文件的工具，有时我用它去掉 Linux 系统中不需要的重复文件并释放磁盘空间。不像另外两个工具，FSlint 有 GUI 和 CLI 两种模式。因此对于新手来说它更友好。FSlint 不仅仅找出重复文件，也找出坏符号链接、坏名字文件、临时文件、坏的用户 ID、空目录和非精简的二进制文件等等。</p><p><strong>安装 FSlint</strong></p><p>FSlint 存在于 <a target="_blank" rel="noopener" href="https://aur.archlinux.org/packages/fslint/">AUR</a>，因此你可以使用任一 AUR 助手安装它。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yay -S fslint</span><br></pre></td></tr></table></figure><p>在 Debian、Ubuntu、Linux Mint 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install fslint</span><br></pre></td></tr></table></figure><p>在 Fedora 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dnf install fslint</span><br></pre></td></tr></table></figure><p>在 RHEL，CentOS 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install epel-release</span><br><span class="line">$ sudo yum install fslint</span><br></pre></td></tr></table></figure><p>一旦安装完成，从菜单或者应用程序启动器启动它。</p><p>FSlint GUI 展示如下：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201810/16/170706vy0opgozoi3ycypg.png"></p><p>如你所见，FSlint 界面友好、一目了然。在 “Search path” 栏，添加你要扫描的目录路径，点击左下角 “Find” 按钮查找重复文件。验证递归选项可以在目录和子目录中递归的搜索重复文件。FSlint 将快速的扫描给定的目录并列出重复文件。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201810/16/170707dfqw80zq8o8e819h.png"></p><p>从列表中选择那些要清理的重复文件，也可以选择 “Save”、“Delete”、“Merge” 和 “Symlink” 操作他们。</p><p>在 “Advanced search parameters” 栏，你可以在搜索重复文件的时候指定排除的路径。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201810/16/170709p5105l91u185dlce.png"></p><p><strong>FSlint 命令行选项</strong></p><p>FSlint 提供下面的 CLI 工具集在你的文件系统中查找重复文件。</p><ul><li><code>findup</code> — 查找重复文件</li><li><code>findnl</code> — 查找名称规范（有问题的文件名）</li><li><code>findu8</code> — 查找非法的 utf8 编码的文件名</li><li><code>findbl</code> — 查找坏链接（有问题的符号链接）</li><li><code>findsn</code> — 查找同名文件（可能有冲突的文件名）</li><li><code>finded</code> — 查找空目录</li><li><code>findid</code> — 查找死用户的文件</li><li><code>findns</code> — 查找非精简的可执行文件</li><li><code>findrs</code> — 查找文件名中多余的空白</li><li><code>findtf</code> — 查找临时文件</li><li><code>findul</code> — 查找可能未使用的库</li><li><code>zipdir</code> — 回收 ext2 目录项下浪费的空间</li></ul><p>所有这些工具位于 <code>/usr/share/fslint/fslint/fslint</code> 下面。</p><p>例如，在给定的目录中查找重复文件，运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/share/fslint/fslint/findup ~/Downloads/</span><br></pre></td></tr></table></figure><p>类似的，找出空目录命令是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/share/fslint/fslint/finded ~/Downloads/</span><br></pre></td></tr></table></figure><p>获取每个工具更多细节，例如：<code>findup</code>，运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/share/fslint/fslint/findup --help</span><br></pre></td></tr></table></figure><p>关于 FSlint 的更多细节，参照帮助部分和 man 页。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/share/fslint/fslint/fslint --help</span><br><span class="line">$ man fslint</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>现在你知道在 Linux 中，使用三个工具来查找和删除不需要的重复文件。这三个工具中，我经常使用 Rdfind。这并不意味着其他的两个工具效率低下，因为到目前为止我更喜欢 Rdfind。好了，到你了。你的最喜欢哪一个工具呢？为什么？在下面的评论区留言让我们知道吧。</p><p>就到这里吧。希望这篇文章对你有帮助。更多的好东西就要来了，敬请期待。</p><p>谢谢！</p><hr><p>via: <a target="_blank" rel="noopener" href="https://www.ostechnix.com/how-to-find-and-delete-duplicate-files-in-linux/">https://www.ostechnix.com/how-to-find-and-delete-duplicate-files-in-linux/</a></p><p>作者：<a target="_blank" rel="noopener" href="https://www.ostechnix.com/author/sk/">SK</a> 选题：<a target="_blank" rel="noopener" href="https://github.com/lujun9972">lujun9972</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/pygmalion666">pygmalion666</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p></div></div></body></html>