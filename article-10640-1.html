<!doctype html><html lang="en"><head><meta name="description" content="一个LinuxCN的镜像站"><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>x86 和 ARM 的 Python 爬虫速度对比 - 归墟星火集 又一个 LinuxCN 站点</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 又一个 LinuxCN 站点 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">x86 和 ARM 的 Python 爬虫速度对比</h1><span class="post-date">2019-03-21</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/">软件开发</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/Python/">Python</a> <a href="/tags/ARM/">ARM</a> <a href="/tags/CPU/">CPU</a> <a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post-content"><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201903/21/212936ctlppldn0ipqp8wt.jpg"></p><p>假如说，如果你的老板给你的任务是一次又一次地访问竞争对手的网站，把对方商品的价格记录下来，而且要纯手工操作，恐怕你会想要把整个办公室都烧掉。</p><p>之所以现在网络爬虫的影响力如此巨大，就是因为网络爬虫可以被用于追踪客户的情绪和趋向、搜寻空缺的职位、监控房地产的交易，甚至是获取 UFC 的比赛结果。除此以外，还有很多意想不到的用途。</p><p>对于有这方面爱好的人来说，爬虫无疑是一个很好的工具。因此，我使用了 <a target="_blank" rel="noopener" href="https://scrapy.org/">Scrapy</a> 这个基于 Python 编写的开源网络爬虫框架。</p><p>鉴于我不太了解这个工具是否会对我的计算机造成伤害，我并没有将它搭建在我的主力机器上，而是搭建在了一台树莓派上面。</p><p>令人感到意外的是，Scrapy 在树莓派上面的性能并不差，或许这是 ARM 架构服务器的又一个成功例子？</p><p>我尝试 Google 了一下，但并没有得到令我满意的结果，仅仅找到了一篇相关的《<a target="_blank" rel="noopener" href="https://www.info2007.net/blog/2018/review-scaleway-arm-based-cloud-server.html">Drupal 建站对比</a>》。这篇文章的结论是，ARM 架构服务器性能比昂贵的 x86 架构服务器要更好。</p><p>从另一个角度来看，这种 web 服务可以看作是一个“被爬虫”服务，但和 Scrapy 对比起来，前者是基于 LAMP 技术栈，而后者则依赖于 Python，这就导致两者之间没有太多的可比性。</p><p>那我们该怎样做呢？只能在一些 VPS 上搭建服务来对比一下了。</p><h3 id="什么是-ARM-架构处理器？"><a href="#什么是-ARM-架构处理器？" class="headerlink" title="什么是 ARM 架构处理器？"></a>什么是 ARM 架构处理器？</h3><p>ARM 是目前世界上最流行的 CPU 架构。</p><p>但 ARM 架构处理器在很多人眼中的地位只是作为一个省钱又省电的选择，而不是跑在生产环境中的处理器的首选。</p><p>然而，诞生于英国剑桥的 ARM CPU，最初是用于极其昂贵的 <a target="_blank" rel="noopener" href="https://blog.dxmtechsupport.com.au/playing-badass-acorn-archimedes-games-on-a-raspberry-pi/">Acorn Archimedes</a> 计算机上的，这是当时世界上最强大的桌面计算机，甚至在很长一段时间内，它的运算速度甚至比最快的 386 还要快好几倍。</p><p>Acorn 公司和 Commodore、Atari 的理念类似，他们认为一家伟大的计算机公司就应该制造出伟大的计算机，让人感觉有点目光短浅。而比尔盖茨的想法则有所不同，他力图在更多不同种类和价格的 x86 机器上使用他的 DOS 系统。</p><p>拥有大量用户基数的平台会成为第三方开发者开发软件的平台，而软件资源丰富又会让你的计算机更受用户欢迎。</p><p>即使是苹果公司也几乎被打败。在 x86 芯片上投入大量的财力，最终，这些芯片被用于生产环境计算任务。</p><p>但 ARM 架构也并没有消失。基于 ARM 架构的芯片不仅运算速度快，同时也非常节能。因此诸如机顶盒、PDA、数码相机、MP3 播放器这些电子产品多数都会采用 ARM 架构的芯片，甚至在很多需要用电池或不配备大散热风扇的电子产品上，都可以见到 ARM 芯片的身影。</p><p>而 ARM 则脱离 Acorn 成为了一种特殊的商业模式，他们不生产实物芯片，仅仅是向芯片生产厂商出售相关的知识产权。</p><p>因此，这或多或少是 ARM 芯片被应用于如此之多的手机和平板电脑上的原因。当 Linux 被移植到这种架构的芯片上时，开源技术的大门就已经向它打开了，这才让我们今天得以在这些芯片上运行 web 爬虫程序。</p><h4 id="服务器端的-ARM"><a href="#服务器端的-ARM" class="headerlink" title="服务器端的 ARM"></a>服务器端的 ARM</h4><p>诸如<a target="_blank" rel="noopener" href="https://www.computerworld.com/article/3178544/microsoft-windows/microsoft-and-arm-look-to-topple-intel-in-servers.html">微软</a>和 <a target="_blank" rel="noopener" href="https://www.datacenterknowledge.com/design/cloudflare-bets-arm-servers-it-expands-its-data-center-network">Cloudflare</a> 这些大厂都在基础设施建设上花了重金，所以对于我们这些预算不高的用户来说，可以选择的余地并不多。</p><p>实际上，如果你的信用卡只够付每月数美元的 VPS 费用，一直以来只能考虑 <a target="_blank" rel="noopener" href="https://www.scaleway.com/">Scaleway</a> 这个高性价比的厂商。</p><p>但自从数个月前公有云巨头 <a target="_blank" rel="noopener" href="https://aws.amazon.com/">AWS</a> 推出了他们自研的 ARM 处理器 <a target="_blank" rel="noopener" href="https://www.theregister.co.uk/2018/11/27/amazon_aws_graviton_specs/">AWS Graviton</a> 之后，选择似乎就丰富了一些。</p><p>我决定在其中选择一款 VPS 厂商，将它提供的 ARM 处理器和 x86 处理器作出对比。</p><h3 id="深入了解"><a href="#深入了解" class="headerlink" title="深入了解"></a>深入了解</h3><p>所以我们要对比的是什么指标呢？</p><h4 id="Scaleway"><a href="#Scaleway" class="headerlink" title="Scaleway"></a>Scaleway</h4><p>Scaleway 自身的定位是“专为开发者设计”。我觉得这个定位很准确，对于开发和原型设计来说，Scaleway 提供的产品确实可以作为一个很好的沙盒环境。</p><p>Scaleway 提供了一个简洁的仪表盘页面，让用户可以快速地从主页进入 bash shell 界面。对于很多小企业、自由职业者或者技术顾问，如果想要运行 web 爬虫，这个产品毫无疑问是一个物美价廉的选择。</p><p>ARM 方面我们选择 <a target="_blank" rel="noopener" href="https://www.scaleway.com/virtual-cloud-servers/#anchor_arm">ARM64-2GB</a> 这一款服务器，每月只需要 3 欧元。它带有 4 个 Cavium ThunderX 核心，这是在 2014 年推出的第一款服务器级的 ARMv8 处理器。但现在看来它已经显得有点落后了，并逐渐被更新的 ThunderX2 取代。</p><p>x86 方面我们选择 <a target="_blank" rel="noopener" href="https://www.scaleway.com/virtual-cloud-servers/#anchor_starter">1-S</a>，每月的费用是 4 欧元。它拥有 2 个英特尔 Atom C3995 核心。英特尔的 Atom 系列处理器的特点是低功耗、单线程，最初是用在笔记本电脑上的，后来也被服务器所采用。</p><p>两者在处理器以外的条件都大致相同，都使用 2 GB 的内存、50 GB 的 SSD 存储以及 200 Mbit&#x2F;s 的带宽。磁盘驱动器可能会有所不同，但由于我们运行的是 web 爬虫，基本都是在内存中完成操作，因此这方面的差异可以忽略不计。</p><p>为了避免我不能熟练使用包管理器的尴尬局面，两方的操作系统我都会选择使用 Debian 9。</p><h4 id="Amazon-Web-Services（AWS）"><a href="#Amazon-Web-Services（AWS）" class="headerlink" title="Amazon Web Services（AWS）"></a>Amazon Web Services（AWS）</h4><p>当你还在注册 AWS 账号的时候，使用 Scaleway 的用户可能已经把提交信用卡信息、启动 VPS 实例、添加 sudo 用户、安装依赖包这一系列流程都完成了。AWS 的操作相对来说比较繁琐，甚至需要详细阅读手册才能知道你正在做什么。</p><p>当然这也是合理的，对于一些需求复杂或者特殊的企业用户，确实需要通过详细的配置来定制合适的使用方案。</p><p>我们所采用的 AWS Graviton 处理器是 AWS EC2（<ruby>弹性计算云 <rt>Elastic Compute Cloud</rt></ruby>）的一部分，我会以按需实例的方式来运行，这也是最贵但最简捷的方式。AWS 同时也提供<a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/spot/pricing/">竞价实例</a>，这样可以用较低的价格运行实例，但实例的运行时间并不固定。如果实例需要长时间持续运行，还可以选择<a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/pricing/reserved-instances/">预留实例</a>。</p><p>看，AWS 就是这么复杂……</p><p>我们分别选择 <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/a1/">a1.medium</a> 和 <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/t2/">t2.small</a> 两种型号的实例进行对比，两者都带有 2GB 内存。这个时候问题来了，这里提到的 vCPU 又是什么？两种型号的不同之处就在于此。</p><p>对于 a1.medium 型号的实例，vCPU 是 AWS Graviton 芯片提供的单个计算核心。这个芯片由被亚马逊在 2015 收购的以色列厂商 Annapurna Labs 研发，是 AWS 独有的单线程 64 位 ARMv8 内核。它的按需价格为每小时 0.0255 美元。</p><p>而 t2.small 型号实例使用英特尔至强系列芯片，但我不确定具体是其中的哪一款。它每个核心有两个线程，但我们并不能用到整个核心，甚至整个线程。</p><p>我们能用到的只是“20% 的基准性能，可以使用 CPU 积分突破这个基准”。这可能有一定的原因，但我没有弄懂。它的按需价格是每小时 0.023 美元。</p><p>在镜像库中没有 Debian 发行版的镜像，因此我选择了 Ubuntu 18.04。</p><h3 id="瘪四与大头蛋爬取-Moz-排行榜前-500-的网站"><a href="#瘪四与大头蛋爬取-Moz-排行榜前-500-的网站" class="headerlink" title="瘪四与大头蛋爬取 Moz 排行榜前 500 的网站"></a>瘪四与大头蛋爬取 Moz 排行榜前 500 的网站</h3><p>要测试这些 VPS 的 CPU 性能，就该使用爬虫了。一个方法是对几个网站在尽可能短的时间里发出尽可能多的请求，但这种操作不太礼貌，我的做法是只向大量网站发出少数几个请求。</p><p>为此，我编写了 <code>beavis.py</code>（瘪四）这个爬虫程序（致敬我最喜欢的物理学家和制片人 Mike Judge）。这个程序会将 Moz 上排行前 500 的网站都爬取 3 层的深度，并计算 “wood” 和 “ass” 这两个单词在 HTML 文件中出现的次数。（LCTT 译注：beavis（瘪四）和 butt-head（大头蛋） 都是 Mike Judge 的动画片《瘪四与大头蛋》中的角色）</p><p>但我实际爬取的网站可能不足 500 个，因为我需要遵循网站的 <code>robot.txt</code> 协定，另外还有些网站需要提交 javascript 请求，也不一定会计算在内。但这已经是一个足以让 CPU 保持繁忙的爬虫任务了。</p><p>Python 的<a target="_blank" rel="noopener" href="https://wiki.python.org/moin/GlobalInterpreterLock">全局解释器锁</a>机制会让我的程序只能用到一个 CPU 线程。为了测试多线程的性能，我需要启动多个独立的爬虫程序进程。</p><p>因此我还编写了 <code>butthead.py</code>，尽管大头蛋很粗鲁，它也总是比瘪四要略胜一筹。</p><p>我将整个爬虫任务拆分为多个部分，这可能会对爬取到的链接数量有一点轻微的影响。但无论如何，每次爬取都会有所不同，我们要关注的是爬取了多少个页面，以及耗时多长。</p><h3 id="在-ARM-服务器上安装-Scrapy"><a href="#在-ARM-服务器上安装-Scrapy" class="headerlink" title="在 ARM 服务器上安装 Scrapy"></a>在 ARM 服务器上安装 Scrapy</h3><p>安装 Scrapy 的过程与芯片的不同架构没有太大的关系，都是安装 <code>pip</code> 和相关的依赖包之后，再使用 <code>pip</code> 来安装 Scrapy。</p><p>据我观察，在使用 ARM 的机器上使用 <code>pip</code> 安装 Scrapy 确实耗时要长一点，我估计是由于需要从源码编译为二进制文件。</p><p>在 Scrapy 安装结束后，就可以通过 shell 来查看它的工作状态了。</p><p>在 Scaleway 的 ARM 机器上，Scrapy 安装完成后会无法正常运行，这似乎和 <code>service_identity</code> 模块有关。这个现象也会在树莓派上出现，但在 AWS Graviton 上不会出现。</p><p>对于这个问题，可以用这个命令来解决：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install service_identity --force --upgrade</span><br></pre></td></tr></table></figure><p>接下来就可以开始对比了。</p><h3 id="单线程爬虫"><a href="#单线程爬虫" class="headerlink" title="单线程爬虫"></a>单线程爬虫</h3><p>Scrapy 的官方文档建议<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/broad-crawls.html">将爬虫程序的 CPU 使用率控制在 80% 到 90% 之间</a>，在真实操作中并不容易，尤其是对于我自己写的代码。根据我的观察，实际的 CPU 使用率变动情况是一开始非常繁忙，随后稍微下降，接着又再次升高。</p><p>在爬取任务的最后，也就是大部分目标网站都已经被爬取了的这个阶段，会持续数分钟的时间。这让人有点失望，因为在这个阶段当中，任务的运行时长只和网站的大小有比较直接的关系，并不能以之衡量 CPU 的性能。</p><p>所以这并不是一次严谨的基准测试，只是我通过自己写的爬虫程序来观察实际的现象。</p><p>下面我们来看看最终的结果。首先是 Scaleway 的机器：</p><table><thead><tr><th>机器种类</th><th>耗时</th><th>爬取页面数</th><th>每小时爬取页面数</th><th>每百万页面费用（欧元）</th></tr></thead><tbody><tr><td>Scaleway ARM64-2GB</td><td>108m 59.27s</td><td>38,205</td><td>21,032.623</td><td>0.28527</td></tr><tr><td>Scaleway 1-S</td><td>97m 44.067s</td><td>39,476</td><td>24,324.648</td><td>0.33011</td></tr></tbody></table><p>我使用了 <a target="_blank" rel="noopener" href="https://linux.die.net/man/1/top">top</a> 工具来查看爬虫程序运行期间的 CPU 使用率。在任务刚开始的时候，两者的 CPU 使用率都达到了 100%，但 ThunderX 大部分时间都达到了 CPU 的极限，无法看出来 Atom 的性能会比 ThunderX 超出多少。</p><p>通过 <code>top</code> 工具，我还观察了它们的内存使用情况。随着爬取任务的进行，ARM 机器的内存使用率最终达到了 14.7%，而 x86 则最终是 15%。</p><p>从运行日志还可以看出来，当 CPU 使用率到达极限时，会有大量的超时页面产生，最终导致页面丢失。这也是合理出现的现象，因为 CPU 过于繁忙会无法完整地记录所有爬取到的页面。</p><p>如果仅仅是为了对比爬虫的速度，页面丢失并不是什么大问题。但在实际中，业务成果和爬虫数据的质量是息息相关的，因此必须为 CPU 留出一些用量，以防出现这种现象。</p><p>再来看看 AWS 这边：</p><table><thead><tr><th>机器种类</th><th>耗时</th><th>爬取页面数</th><th>每小时爬取页面数</th><th>每百万页面费用（美元）</th></tr></thead><tbody><tr><td>a1.medium</td><td>100m 39.900s</td><td>41,294</td><td>24,612.725</td><td>1.03605</td></tr><tr><td>t2.small</td><td>78m 53.171s</td><td>41,200</td><td>31,336.286</td><td>0.73397</td></tr></tbody></table><p>为了方便比较，对于在 AWS 上跑的爬虫，我记录的指标和 Scaleway 上一致，但似乎没有达到预期的效果。这里我没有使用 <code>top</code>，而是使用了 AWS 提供的控制台来监控 CPU 的使用情况，从监控结果来看，我的爬虫程序并没有完全用到这两款服务器所提供的所有性能。</p><p>a1.medium 型号的机器尤为如此，在任务开始阶段，它的 CPU 使用率达到了峰值 45%，但随后一直在 20% 到 30% 之间。</p><p>让我有点感到意外的是，这个程序在 ARM 处理器上的运行速度相当慢，但却远未达到 Graviton CPU 能力的极限，而在 Intel Atom 处理器上则可以在某些时候达到 CPU 能力的极限。它们运行的代码是完全相同的，处理器的不同架构可能导致了对代码的不同处理方式。</p><p>个中原因无论是由于处理器本身的特性，还是二进制文件的编译，又或者是两者皆有，对我来说都是一个黑盒般的存在。我认为，既然在 AWS 机器上没有达到 CPU 处理能力的极限，那么只有在 Scaleway 机器上跑出来的性能数据是可以作为参考的。</p><p>t2.small 型号的机器性能让人费解。CPU 利用率大概 20%，最高才达到 35%，是因为手册中说的“20% 的基准性能，可以使用 CPU 积分突破这个基准”吗？但在控制台中可以看到 CPU 积分并没有被消耗。</p><p>为了确认这一点，我安装了 <a target="_blank" rel="noopener" href="https://linux.die.net/man/1/stress">stress</a> 这个软件，然后运行了一段时间，这个时候发现居然可以把 CPU 使用率提高到 100% 了。</p><p>显然，我需要调整一下它们的配置文件。我将 <code>CONCURRENT_REQUESTS</code> 参数设置为 5000，将 <code>REACTOR_THREADPOOL_MAXSIZE</code> 参数设置为 120，将爬虫任务的负载调得更大。</p><table><thead><tr><th>机器种类</th><th>耗时</th><th>爬取页面数</th><th>每小时爬取页面数</th><th>每万页面费用（美元）</th></tr></thead><tbody><tr><td>a1.medium</td><td>46m 13.619s</td><td>40,283</td><td>52,285.047</td><td>0.48771</td></tr><tr><td>t2.small</td><td>41m7.619s</td><td>36,241</td><td>52,871.857</td><td>0.43501</td></tr><tr><td>t2.small（无 CPU 积分）</td><td>73m 8.133s</td><td>34,298</td><td>28,137.8891</td><td>0.81740</td></tr></tbody></table><p>a1.medium 型号机器的 CPU 使用率在爬虫任务开始后 5 分钟飙升到了 100%，随后下降到 80% 并持续了 20 分钟，然后再次攀升到 96%，直到任务接近结束时再次下降。这大概就是我想要的效果了。</p><p>而 t2.small 型号机器在爬虫任务的前期就达到了 50%，并一直保持在这个水平直到任务接近结束。如果每个核心都有两个线程，那么 50% 的 CPU 使用率确实是单个线程可以达到的极限了。</p><p>现在我们看到它们的性能都差不多了。但至强处理器的线程持续跑满了 CPU，Graviton 处理器则只是有一段时间如此。可以认为 Graviton 略胜一筹。</p><p>然而，如果 CPU 积分耗尽了呢？这种情况下的对比可能更为公平。为了测试这种情况，我使用 <code>stress</code> 把所有的 CPU 积分用完，然后再次启动了爬虫任务。</p><p>在没有 CPU 积分的情况下，CPU 使用率在 27% 就到达极限不再上升了，同时又出现了丢失页面的现象。这么看来，它的性能比负载较低的时候更差。</p><h3 id="多线程爬虫"><a href="#多线程爬虫" class="headerlink" title="多线程爬虫"></a>多线程爬虫</h3><p>将爬虫任务分散到不同的进程中，可以有效利用机器所提供的多个核心。</p><p>一开始，我将爬虫任务分布在 10 个不同的进程中并同时启动，结果发现比每个核心仅使用 1 个进程的时候还要慢。</p><p>经过尝试，我得到了一个比较好的方案。把爬虫任务分布在 10 个进程中，但每个核心只启动 1 个进程，在每个进程接近结束的时候，再从剩余的进程中选出 1 个进程启动起来。</p><p>如果还需要优化，还可以让运行时间越长的爬虫进程在启动顺序中排得越靠前，我也在尝试实现这个方法。</p><p>想要预估某个域名的页面量，一定程度上可以参考这个域名主页的链接数量。我用另一个程序来对这个数量进行了统计，然后按照降序排序。经过这样的预处理之后，只会额外增加 1 分钟左右的时间。</p><p>结果，爬虫运行的总耗时超过了两个小时！毕竟把链接最多的域名都堆在同一个进程中也存在一定的弊端。</p><p>针对这个问题，也可以通过调整各个进程爬取的域名数量来进行优化，又或者在排序之后再作一定的修改。不过这种优化可能有点复杂了。</p><p>因此，我还是用回了最初的方法，它的效果还是相当不错的：</p><table><thead><tr><th>机器种类</th><th>耗时</th><th>爬取页面数</th><th>每小时爬取页面数</th><th>每万页面费用（欧元）</th></tr></thead><tbody><tr><td>Scaleway ARM64-2GB</td><td>62m 10.078s</td><td>36,158</td><td>34,897.0719</td><td>0.17193</td></tr><tr><td>Scaleway 1-S</td><td>60m 56.902s</td><td>36,725</td><td>36,153.5529</td><td>0.22128</td></tr></tbody></table><p>毕竟，使用多个核心能够大大加快爬虫的速度。</p><p>我认为，如果让一个经验丰富的程序员来优化的话，一定能够更好地利用所有的计算核心。但对于开箱即用的 Scrapy 来说，想要提高性能，使用更快的线程似乎比使用更多核心要简单得多。</p><p>从数量来看，Atom 处理器在更短的时间内爬取到了更多的页面。但如果从性价比角度来看，ThunderX 又是稍稍领先的。不过总的来说差距不大。</p><h3 id="爬取结果分析"><a href="#爬取结果分析" class="headerlink" title="爬取结果分析"></a>爬取结果分析</h3><p>在爬取了 38205 个页面之后，我们可以统计到在这些页面中 “ass” 出现了 24170435 次，而 “wood” 出现了 54368 次。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201903/21/212936zvarv0r370korpe2.png"></p><p>“wood” 的出现次数不少，但和 “ass” 比起来简直微不足道。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>从上面的数据来看，对于性能而言，CPU 的架构并没有它们的问世时间重要，2018 年生产的 AWS Graviton 是单线程情况下性能最佳的。</p><p>你当然可以说按核心来比，Xeon 仍然赢了。但是，你不但需要计算美元的变化，甚至还要计算线程数。</p><p>另外在性能方面 2017 年生产的 Atom 轻松击败了 2014 年生产的 ThunderX，而 ThunderX 则在性价比方面占优。当然，如果你使用 AWS 的机器的话，还是使用 Graviton 吧。</p><p>总之，ARM 架构的硬件是可以用来运行爬虫程序的，而且在性能和费用方面也相当有竞争力。</p><p>而这种差异是否足以让你将整个技术架构迁移到 ARM 上？这就是另一回事了。当然，如果你已经是 AWS 用户，并且你的代码有很强的可移植性，那么不妨尝试一下 a1 型号的实例。</p><p>希望 ARM 设备在不久的将来能够在公有云上大放异彩。</p><h3 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h3><p>这是我第一次使用 Python 和 Scrapy 来做一个项目，所以我的代码写得可能不是很好，例如代码中使用全局变量就有点力不从心。</p><p>不过我仍然会在下面开源我的代码。</p><p>要运行这些代码，需要预先安装 Scrapy，并且需要 <a target="_blank" rel="noopener" href="https://moz.com/top500">Moz 上排名前 500 的网站</a>的 csv 文件。如果要运行 <code>butthead.py</code>，还需要安装 <a target="_blank" rel="noopener" href="https://pypi.org/project/psutil/">psutil</a> 这个库。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line"></span><br><span class="line">ass = 0</span><br><span class="line">wood = 0</span><br><span class="line">totalpages = 0</span><br><span class="line"></span><br><span class="line">def getdomains():</span><br><span class="line"></span><br><span class="line">  moz500file = open(&#x27;top500.domains.05.18.csv&#x27;)</span><br><span class="line"></span><br><span class="line">  domains = []</span><br><span class="line">  moz500csv = moz500file.readlines()</span><br><span class="line"></span><br><span class="line">  del moz500csv[0]</span><br><span class="line"></span><br><span class="line">  for csvline in moz500csv:</span><br><span class="line">    leftquote = csvline.find(&#x27;&quot;&#x27;)    </span><br><span class="line">    rightquote = leftquote + csvline[leftquote + 1:].find(&#x27;&quot;&#x27;)</span><br><span class="line">    domains.append(csvline[leftquote + 1:rightquote])</span><br><span class="line"></span><br><span class="line">  return domains</span><br><span class="line"></span><br><span class="line">def getstartpages(domains):</span><br><span class="line">  </span><br><span class="line">  startpages = []</span><br><span class="line">  </span><br><span class="line">  for domain in domains:</span><br><span class="line">    startpages.append(&#x27;http://&#x27; + domain)</span><br><span class="line">  </span><br><span class="line">  return startpages</span><br><span class="line">  </span><br><span class="line">class AssWoodItem(scrapy.Item):</span><br><span class="line">  ass = scrapy.Field()</span><br><span class="line">  wood = scrapy.Field()</span><br><span class="line">  url = scrapy.Field()</span><br><span class="line">  </span><br><span class="line">class AssWoodPipeline(object):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    self.asswoodstats = []</span><br><span class="line"></span><br><span class="line">  def process_item(self, item, spider):</span><br><span class="line">    self.asswoodstats.append((item.get(&#x27;url&#x27;), item.get(&#x27;ass&#x27;), item.get(&#x27;wood&#x27;)))</span><br><span class="line">    </span><br><span class="line">  def close_spider(self, spider):</span><br><span class="line">    asstally, woodtally = 0, 0</span><br><span class="line">    </span><br><span class="line">    for asswoodcount in self.asswoodstats:</span><br><span class="line">      asstally += asswoodcount[1]</span><br><span class="line">      woodtally += asswoodcount[2]</span><br><span class="line">      </span><br><span class="line">    global ass, wood, totalpages</span><br><span class="line">    ass = asstally</span><br><span class="line">    wood = woodtally</span><br><span class="line">    totalpages = len(self.asswoodstats)</span><br><span class="line"></span><br><span class="line">class BeavisSpider(CrawlSpider):</span><br><span class="line">  name = &quot;Beavis&quot;</span><br><span class="line">  allowed_domains = getdomains()</span><br><span class="line">  start_urls = getstartpages(allowed_domains)</span><br><span class="line">  #start_urls = [ &#x27;http://medium.com&#x27; ]</span><br><span class="line">  custom_settings = &#123;</span><br><span class="line">    &#x27;DEPTH_LIMIT&#x27;: 3,</span><br><span class="line">    &#x27;DOWNLOAD_DELAY&#x27;: 3,</span><br><span class="line">    &#x27;CONCURRENT_REQUESTS&#x27;: 1500,</span><br><span class="line">    &#x27;REACTOR_THREADPOOL_MAXSIZE&#x27;: 60,</span><br><span class="line">    &#x27;ITEM_PIPELINES&#x27;: &#123; &#x27;__main__.AssWoodPipeline&#x27;: 10 &#125;,</span><br><span class="line">    &#x27;LOG_LEVEL&#x27;: &#x27;INFO&#x27;,</span><br><span class="line">    &#x27;RETRY_ENABLED&#x27;: False,</span><br><span class="line">    &#x27;DOWNLOAD_TIMEOUT&#x27;: 30,</span><br><span class="line">    &#x27;COOKIES_ENABLED&#x27;: False,</span><br><span class="line">    &#x27;AJAXCRAWL_ENABLED&#x27;: True</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  rules = ( Rule(LinkExtractor(), callback=&#x27;parse_asswood&#x27;), )</span><br><span class="line">  </span><br><span class="line">  def parse_asswood(self, response):</span><br><span class="line">    if isinstance(response, scrapy.http.TextResponse):</span><br><span class="line">      item = AssWoodItem()</span><br><span class="line">      item[&#x27;ass&#x27;] = response.text.casefold().count(&#x27;ass&#x27;)</span><br><span class="line">      item[&#x27;wood&#x27;] = response.text.casefold().count(&#x27;wood&#x27;)</span><br><span class="line">      item[&#x27;url&#x27;] = response.url</span><br><span class="line">      yield item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">  process = CrawlerProcess(&#123;</span><br><span class="line">      &#x27;USER_AGENT&#x27;: &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  process.crawl(BeavisSpider)</span><br><span class="line">  process.start()</span><br><span class="line"></span><br><span class="line">  print(&#x27;Uhh, that was, like, &#x27; + str(totalpages) + &#x27; pages crawled.&#x27;)</span><br><span class="line">  print(&#x27;Uh huhuhuhuh. It said ass &#x27; + str(ass) + &#x27; times.&#x27;)</span><br><span class="line">  print(&#x27;Uh huhuhuhuh. It said wood &#x27; + str(wood) + &#x27; times.&#x27;)</span><br></pre></td></tr></table></figure><p><em>beavis.py</em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">import scrapy, time, psutil</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule, Spider</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line">from multiprocessing import Process, Queue, cpu_count</span><br><span class="line"></span><br><span class="line">ass = 0</span><br><span class="line">wood = 0</span><br><span class="line">totalpages = 0</span><br><span class="line">linkcounttuples =[]</span><br><span class="line"></span><br><span class="line">def getdomains():</span><br><span class="line"></span><br><span class="line">  moz500file = open(&#x27;top500.domains.05.18.csv&#x27;)</span><br><span class="line"></span><br><span class="line">  domains = []</span><br><span class="line">  moz500csv = moz500file.readlines()</span><br><span class="line"></span><br><span class="line">  del moz500csv[0]</span><br><span class="line"></span><br><span class="line">  for csvline in moz500csv:</span><br><span class="line">    leftquote = csvline.find(&#x27;&quot;&#x27;)    </span><br><span class="line">    rightquote = leftquote + csvline[leftquote + 1:].find(&#x27;&quot;&#x27;)</span><br><span class="line">    domains.append(csvline[leftquote + 1:rightquote])</span><br><span class="line"></span><br><span class="line">  return domains</span><br><span class="line"></span><br><span class="line">def getstartpages(domains):</span><br><span class="line">  </span><br><span class="line">  startpages = []</span><br><span class="line">  </span><br><span class="line">  for domain in domains:</span><br><span class="line">    startpages.append(&#x27;http://&#x27; + domain)</span><br><span class="line">  </span><br><span class="line">  return startpages</span><br><span class="line">  </span><br><span class="line">class AssWoodItem(scrapy.Item):</span><br><span class="line">  ass = scrapy.Field()</span><br><span class="line">  wood = scrapy.Field()</span><br><span class="line">  url = scrapy.Field()</span><br><span class="line">  </span><br><span class="line">class AssWoodPipeline(object):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    self.asswoodstats = []</span><br><span class="line"></span><br><span class="line">  def process_item(self, item, spider):</span><br><span class="line">    self.asswoodstats.append((item.get(&#x27;url&#x27;), item.get(&#x27;ass&#x27;), item.get(&#x27;wood&#x27;)))</span><br><span class="line">    </span><br><span class="line">  def close_spider(self, spider):</span><br><span class="line">    asstally, woodtally = 0, 0</span><br><span class="line">    </span><br><span class="line">    for asswoodcount in self.asswoodstats:</span><br><span class="line">      asstally += asswoodcount[1]</span><br><span class="line">      woodtally += asswoodcount[2]</span><br><span class="line">      </span><br><span class="line">    global ass, wood, totalpages</span><br><span class="line">    ass = asstally</span><br><span class="line">    wood = woodtally</span><br><span class="line">    totalpages = len(self.asswoodstats)</span><br><span class="line">          </span><br><span class="line"></span><br><span class="line">class ButtheadSpider(CrawlSpider):</span><br><span class="line">  name = &quot;Butthead&quot;</span><br><span class="line">  custom_settings = &#123;</span><br><span class="line">    &#x27;DEPTH_LIMIT&#x27;: 3,</span><br><span class="line">    &#x27;DOWNLOAD_DELAY&#x27;: 3,</span><br><span class="line">    &#x27;CONCURRENT_REQUESTS&#x27;: 250,</span><br><span class="line">    &#x27;REACTOR_THREADPOOL_MAXSIZE&#x27;: 30,</span><br><span class="line">    &#x27;ITEM_PIPELINES&#x27;: &#123; &#x27;__main__.AssWoodPipeline&#x27;: 10 &#125;,</span><br><span class="line">    &#x27;LOG_LEVEL&#x27;: &#x27;INFO&#x27;,</span><br><span class="line">    &#x27;RETRY_ENABLED&#x27;: False,</span><br><span class="line">    &#x27;DOWNLOAD_TIMEOUT&#x27;: 30,</span><br><span class="line">    &#x27;COOKIES_ENABLED&#x27;: False,</span><br><span class="line">    &#x27;AJAXCRAWL_ENABLED&#x27;: True</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  rules = ( Rule(LinkExtractor(), callback=&#x27;parse_asswood&#x27;), )</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  def parse_asswood(self, response):</span><br><span class="line">    if isinstance(response, scrapy.http.TextResponse):</span><br><span class="line">      item = AssWoodItem()</span><br><span class="line">      item[&#x27;ass&#x27;] = response.text.casefold().count(&#x27;ass&#x27;)</span><br><span class="line">      item[&#x27;wood&#x27;] = response.text.casefold().count(&#x27;wood&#x27;)</span><br><span class="line">      item[&#x27;url&#x27;] = response.url</span><br><span class="line">      yield item</span><br><span class="line"></span><br><span class="line">def startButthead(domainslist, urlslist, asswoodqueue):</span><br><span class="line">  crawlprocess = CrawlerProcess(&#123;</span><br><span class="line">      &#x27;USER_AGENT&#x27;: &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  crawlprocess.crawl(ButtheadSpider, allowed_domains = domainslist, start_urls = urlslist)</span><br><span class="line">  crawlprocess.start()</span><br><span class="line">  asswoodqueue.put( (ass, wood, totalpages) )</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">  asswoodqueue = Queue()</span><br><span class="line">  domains=getdomains()</span><br><span class="line">  startpages=getstartpages(domains)</span><br><span class="line">  processlist =[]</span><br><span class="line">  cores = cpu_count()</span><br><span class="line">  </span><br><span class="line">  for i in range(10):</span><br><span class="line">    domainsublist = domains[i * 50:(i + 1) * 50]</span><br><span class="line">    pagesublist = startpages[i * 50:(i + 1) * 50]</span><br><span class="line">    p = Process(target = startButthead, args = (domainsublist, pagesublist, asswoodqueue))</span><br><span class="line">    processlist.append(p)</span><br><span class="line">  </span><br><span class="line">  for i in range(cores):</span><br><span class="line">    processlist[i].start()</span><br><span class="line">    </span><br><span class="line">  time.sleep(180)</span><br><span class="line">  </span><br><span class="line">  i = cores</span><br><span class="line">  </span><br><span class="line">  while i != 10:</span><br><span class="line">    time.sleep(60)</span><br><span class="line">    if psutil.cpu_percent() &lt; 66.7:</span><br><span class="line">      processlist[i].start()</span><br><span class="line">      i += 1</span><br><span class="line">  </span><br><span class="line">  for i in range(10):</span><br><span class="line">    processlist[i].join()</span><br><span class="line">  </span><br><span class="line">  for i in range(10):</span><br><span class="line">    asswoodtuple = asswoodqueue.get()</span><br><span class="line">    ass += asswoodtuple[0]</span><br><span class="line">    wood += asswoodtuple[1]</span><br><span class="line">    totalpages += asswoodtuple[2]</span><br><span class="line"></span><br><span class="line">  print(&#x27;Uhh, that was, like, &#x27; + str(totalpages) + &#x27; pages crawled.&#x27;)</span><br><span class="line">  print(&#x27;Uh huhuhuhuh. It said ass &#x27; + str(ass) + &#x27; times.&#x27;)</span><br><span class="line">  print(&#x27;Uh huhuhuhuh. It said wood &#x27; + str(wood) + &#x27; times.&#x27;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p><em>butthead.py</em></p><hr><p>via: <a target="_blank" rel="noopener" href="https://blog.dxmtechsupport.com.au/speed-test-x86-vs-arm-for-web-crawling-in-python/">https://blog.dxmtechsupport.com.au/speed-test-x86-vs-arm-for-web-crawling-in-python/</a></p><p>作者：<a target="_blank" rel="noopener" href="https://blog.dxmtechsupport.com.au/author/james-mawson/">James Mawson</a> 选题：<a target="_blank" rel="noopener" href="https://github.com/lujun9972">lujun9972</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/HankChow">HankChow</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p><script defer src="https://pv.undefined.today/tracker.min.js" data-website-id="LinuxCNMirror-tracker"></script></div></div></body></html>