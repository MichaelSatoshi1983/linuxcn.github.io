<!doctype html><html lang="en"><head><meta name="description" content="一个LinuxCN的镜像站"><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>大数据探索：在树莓派上通过 Apache Spark on YARN 搭建 Hadoop 集群 - 归墟星火集 又一个 LinuxCN 站点</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 又一个 LinuxCN 站点 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">大数据探索：在树莓派上通过 Apache Spark on YARN 搭建 Hadoop 集群</h1><span class="post-date">2017-05-07</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/">树莓派</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/">树莓派</a> <a href="/tags/Hadoop/">Hadoop</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post-content"><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201705/07/023744ysdd41g0e4g45d1k.jpg"></p><p>有些时候我们想从 DQYDJ 网站的数据中分析点有用的东西出来，在过去，我们要<a target="_blank" rel="noopener" href="https://dqydj.com/how-to-import-fixed-width-data-into-a-spreadsheet-via-r-playing-with-ipums-cps-data/">用 R 语言提取固定宽度的数据</a>，然后通过数学建模来分析<a target="_blank" rel="noopener" href="http://dqydj.com/negative-income-tax-cost-calculator-united-states/">美国的最低收入补贴</a>，当然也包括其他优秀的方法。</p><p>今天我将向你展示对大数据的一点探索，不过有点变化，使用的是全世界最流行的微型电脑————<a target="_blank" rel="noopener" href="https://www.raspberrypi.org/">树莓派</a>，如果手头没有，那就看下一篇吧（可能是已经处理好的数据），对于其他用户，请继续阅读吧，今天我们要建立一个树莓派 Hadoop集群！</p><h3 id="I-为什么要建立一个树莓派的-Hadoop-集群？"><a href="#I-为什么要建立一个树莓派的-Hadoop-集群？" class="headerlink" title="I. 为什么要建立一个树莓派的 Hadoop 集群？"></a>I. 为什么要建立一个树莓派的 Hadoop 集群？</h3><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201705/07/023809pk52e4yff97f2ke4.png"></p><p><em>由三个树莓派节点组成的 Hadoop 集群</em></p><p>我们对 DQYDJ 的数据做了<a target="_blank" rel="noopener" href="https://dqydj.com/finance-calculators-investment-calculators-and-visualizations/">大量的处理工作</a>，但这些还不能称得上是大数据。</p><p>和许许多多有争议的话题一样，数据的大小之别被解释成这样一个笑话：</p><blockquote><p>如果能被内存所存储，那么它就不是大数据。 ————佚名</p></blockquote><p>似乎这儿有两种解决问题的方法：</p><ol><li>我们可以找到一个足够大的数据集合，任何家用电脑的物理或虚拟内存都存不下。</li><li>我们可以买一些不用特别定制，我们现有数据就能淹没它的电脑：<br>—— 上手树莓派 2B</li></ol><p>这个由设计师和工程师制作出来的精致小玩意儿拥有 1GB 的内存， MicroSD 卡充当它的硬盘，此外，每一台的价格都低于 50 美元，这意味着你可以花不到 250 美元的价格搭建一个 Hadoop 集群。</p><p>或许天下没有比这更便宜的入场券来带你进入大数据的大门。</p><h3 id="II-制作一个树莓派集群"><a href="#II-制作一个树莓派集群" class="headerlink" title="II. 制作一个树莓派集群"></a>II. 制作一个树莓派集群</h3><p>我最喜欢制作的原材料。</p><p>这里我将给出我原来为了制作树莓派集群购买原材料的链接，如果以后要在亚马逊购买的话你可先这些链接收藏起来，也是对本站的一点支持。(谢谢)</p><ul><li><a target="_blank" rel="noopener" href="http://amzn.to/2bEFTVh">树莓派 2B 3 块</a></li><li><a target="_blank" rel="noopener" href="http://amzn.to/2bTo1br">4 层亚克力支架</a></li><li><a target="_blank" rel="noopener" href="http://amzn.to/2bEGO8g">6 口 USB 转接器</a>，我选了白色 RAVPower 50W 10A 6 口 USB 转接器</li><li><a target="_blank" rel="noopener" href="http://amzn.to/2cguV9I">MicroSD 卡</a>，这个五件套 32GB 卡非常棒</li><li><a target="_blank" rel="noopener" href="http://amzn.to/2bX2mwm">短的 MicroUSB 数据线</a>，用于给树莓派供电</li><li><a target="_blank" rel="noopener" href="http://amzn.to/2bDACQJ">短网线</a></li><li>双面胶，我有一些 3M 的，很好用</li></ul><h4 id="开始制作"><a href="#开始制作" class="headerlink" title="开始制作"></a>开始制作</h4><ol><li>首先，装好三个树莓派，每一个用螺丝钉固定在亚克力面板上。（看下图）</li><li>接下来，安装以太网交换机，用双面胶贴在其中一个在亚克力面板上。</li><li>用双面胶贴将 USB 转接器贴在一个在亚克力面板使之成为最顶层。</li><li>接着就是一层一层都拼好——这里我选择将树莓派放在交换机和USB转接器的底下（可以看看完整安装好的两张截图）</li></ol><p>想办法把线路放在需要的地方——如果你和我一样购买力 USB 线和网线，我可以将它们卷起来放在亚克力板子的每一层</p><p>现在不要急着上电，需要将系统烧录到 SD 卡上才能继续。</p><h4 id="烧录-Raspbian"><a href="#烧录-Raspbian" class="headerlink" title="烧录 Raspbian"></a>烧录 Raspbian</h4><p>按照<a target="_blank" rel="noopener" href="https://www.raspberrypi.org/downloads/raspbian/">这个教程</a>将 Raspbian 烧录到三张 SD 卡上，我使用的是 Win7 下的 <a target="_blank" rel="noopener" href="https://sourceforge.net/projects/win32diskimager/">Win32DiskImager</a>。</p><p>将其中一张烧录好的 SD 卡插在你想作为主节点的树莓派上，连接 USB 线并启动它。</p><h4 id="启动主节点"><a href="#启动主节点" class="headerlink" title="启动主节点"></a>启动主节点</h4><p>这里有<a target="_blank" rel="noopener" href="http://www.becausewecangeek.com/building-a-raspberry-pi-hadoop-cluster-part-1/">一篇非常棒的“Because We Can Geek”的教程</a>，讲如何安装 Hadoop 2.7.1，此处就不再熬述。</p><p>在启动过程中有一些要注意的地方，我将带着你一起设置直到最后一步，记住我现在使用的 IP 段为 192.168.1.50 – 192.168.1.52，主节点是 .50，从节点是 .51 和 .52，你的网络可能会有所不同，如果你想设置静态 IP 的话可以在评论区看看或讨论。</p><p>一旦你完成了这些步骤，接下来要做的就是启用交换文件，Spark on YARN 将分割出一块非常接近内存大小的交换文件，当你内存快用完时便会使用这个交换分区。</p><p>（如果你以前没有做过有关交换分区的操作的话，可以看看<a target="_blank" rel="noopener" href="https://www.digitalocean.com/community/tutorials/how-to-add-swap-on-ubuntu-14-04">这篇教程</a>，让 <code>swappiness</code> 保持较低水准，因为 MicroSD 卡的性能扛不住）</p><p>现在我准备介绍有关我的和“Because We Can Geek”关于启动设置一些微妙的区别。</p><p>对于初学者，确保你给你的树莓派起了一个正式的名字——在 <code>/etc/hostname</code> 设置，我的主节点设置为 ‘RaspberryPiHadoopMaster’ ，从节点设置为 ‘RaspberryPiHadoopSlave#’</p><p>主节点的 <code>/etc/hosts</code> 配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#/etc/hosts</span><br><span class="line">127.0.0.1       localhost</span><br><span class="line">::1             localhost ip6-localhost ip6-loopback</span><br><span class="line">ff02::1         ip6-allnodes</span><br><span class="line">ff02::2         ip6-allrouters</span><br><span class="line"></span><br><span class="line">192.168.1.50    RaspberryPiHadoopMaster</span><br><span class="line">192.168.1.51    RaspberryPiHadoopSlave1</span><br><span class="line">192.168.1.52    RaspberryPiHadoopSlave2</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果你想让 Hadoop、YARN 和 Spark 运行正常的话，你也需要修改这些配置文件（不妨现在就编辑）。</p><p>这是 <code>hdfs-site.xml</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://RaspberryPiHadoopMaster:54310&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/hdfs/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这是 <code>yarn-site.xml</code> （注意内存方面的改变）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;128&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;  </span><br><span class="line">&lt;value&gt;RaspberryPiHadoopMaster:8025&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br><span class="line">&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;  </span><br><span class="line">&lt;value&gt;RaspberryPiHadoopMaster:8030&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br><span class="line">&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;  </span><br><span class="line">&lt;value&gt;RaspberryPiHadoopMaster:8040&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>slaves</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RaspberryPiHadoopMaster</span><br><span class="line">RaspberryPiHadoopSlave1</span><br><span class="line">RaspberryPiHadoopSlave2</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>core-site.xml</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://RaspberryPiHadoopMaster:54310&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/hdfs/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="设置两个从节点："><a href="#设置两个从节点：" class="headerlink" title="设置两个从节点："></a>设置两个从节点：</h4><p>接下来<a target="_blank" rel="noopener" href="http://www.becausewecangeek.com/building-a-raspberry-pi-hadoop-cluster-part-2/">按照 “Because We Can Geek”上的教程</a>，你需要对上面的文件作出小小的改动。 在 <code>yarn-site.xml</code> 中主节点没有改变，所以从节点中不必含有这个 <code>slaves</code> 文件。</p><h3 id="III-在我们的树莓派集群中测试-YARN"><a href="#III-在我们的树莓派集群中测试-YARN" class="headerlink" title="III. 在我们的树莓派集群中测试 YARN"></a>III. 在我们的树莓派集群中测试 YARN</h3><p>如果所有设备都正常工作，在主节点上你应该执行如下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当设备启动后，以 Hadoop 用户执行，如果你遵循教程，用户应该是 <code>hduser</code>。</p><p>接下来执行 <code>hdfs dfsadmin -report</code> 查看三个节点是否都正确启动，确认你看到一行粗体文字 ‘Live datanodes (3)’：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Configured Capacity: 93855559680 (87.41 GB)</span><br><span class="line">Raspberry Pi Hadoop Cluster picture Straight On</span><br><span class="line">Present Capacity: 65321992192 (60.84 GB)</span><br><span class="line">DFS Remaining: 62206627840 (57.93 GB)</span><br><span class="line">DFS Used: 3115364352 (2.90 GB)</span><br><span class="line">DFS Used%: 4.77%</span><br><span class="line">Under replicated blocks: 0</span><br><span class="line">Blocks with corrupt replicas: 0</span><br><span class="line">Missing blocks: 0</span><br><span class="line">Missing blocks (with replication factor 1): 0</span><br><span class="line">————————————————-</span><br><span class="line">Live datanodes (3):</span><br><span class="line">Name: 192.168.1.51:50010 (RaspberryPiHadoopSlave1)</span><br><span class="line">Hostname: RaspberryPiHadoopSlave1</span><br><span class="line">Decommission Status : Normal</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>你现在可以做一些简单的诸如 ‘Hello, World!’ 的测试，或者直接进行下一步。</p><h3 id="IV-安装-SPARK-ON-YARN"><a href="#IV-安装-SPARK-ON-YARN" class="headerlink" title="IV. 安装 SPARK ON YARN"></a>IV. 安装 SPARK ON YARN</h3><p>YARN 的意思是另一种非常好用的资源调度器（Yet Another Resource Negotiator），已经作为一个易用的资源管理器集成在 Hadoop 基础安装包中。</p><p><a target="_blank" rel="noopener" href="https://spark.apache.org/">Apache Spark</a> 是 Hadoop 生态圈中的另一款软件包，它是一个毁誉参半的执行引擎和<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html">捆绑的 MapReduce</a>。在一般情况下，相对于基于磁盘存储的 MapReduce，Spark 更适合基于内存的存储，某些运行任务能够得到 10-100 倍提升——安装完成集群后你可以试试 Spark 和 MapReduce 有什么不同。</p><p>我个人对 Spark 还是留下非常深刻的印象，因为它提供了两种数据工程师和科学家都比较擅长的语言—— Python 和 R。</p><p>安装 Apache Spark 非常简单，在你家目录下，<code>wget &quot;为 Hadoop 2.7 构建的 Apache Spark”</code>（<a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">来自这个页面</a>），然后运行 <code>tar -xzf “tgz 文件”</code>，最后把解压出来的文件移动至 <code>/opt</code>，并清除刚才下载的文件，以上这些就是安装步骤。</p><p>我又创建了只有两行的文件 <code>spark-env.sh</code>，其中包含 Spark 的配置文件目录。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_IP=192.168.1.50</span><br><span class="line">SPARK_WORKER_MEMORY=512m</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>(在 YARN 跑起来之前我不确定这些是否有必要。)</p><h3 id="V-你好，世界-为-Apache-Spark-寻找有趣的数据集"><a href="#V-你好，世界-为-Apache-Spark-寻找有趣的数据集" class="headerlink" title="V. 你好，世界! 为 Apache Spark 寻找有趣的数据集!"></a>V. 你好，世界! 为 Apache Spark 寻找有趣的数据集!</h3><p>在 Hadoop 世界里面的 ‘Hello, World!’ 就是做单词计数。</p><p>我决定让我们的作品做一些内省式……为什么不统计本站最常用的单词呢？也许统计一些关于本站的大数据会更有用。</p><p>如果你有一个正在运行的 WordPress 博客，可以通过简单的两步来导出和净化。</p><ol><li>我使用 <a target="_blank" rel="noopener" href="https://wordpress.org/support/plugin/export-to-text">Export to Text</a> 插件导出文章的内容到纯文本文件中</li><li>我使用一些<a target="_blank" rel="noopener" href="https://pypi.python.org/pypi/bleach">压缩库</a>编写了一个 Python 脚本来剔除 HTML</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import bleach</span><br><span class="line"></span><br><span class="line"># Change this next line to your &#x27;import&#x27; filename, whatever you would like to strip</span><br><span class="line"># HTML tags from.</span><br><span class="line">ascii_string = open(&#x27;dqydj_with_tags.txt&#x27;, &#x27;r&#x27;).read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">new_string = bleach.clean(ascii_string, tags=[], attributes=&#123;&#125;, styles=[], strip=True)</span><br><span class="line">new_string = new_string.encode(&#x27;utf-8&#x27;).strip()</span><br><span class="line"></span><br><span class="line"># Change this next line to your &#x27;export&#x27; filename</span><br><span class="line">f = open(&#x27;dqydj_stripped.txt&#x27;, &#x27;w&#x27;)</span><br><span class="line">f.write(new_string)</span><br><span class="line">f.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在我们有了一个更小的、适合复制到树莓派所搭建的 HDFS 集群上的文件。</p><p>如果你不能树莓派主节点上完成上面的操作，找个办法将它传输上去（scp、 rsync 等等），然后用下列命令行复制到 HDFS 上。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -copyFromLocal dqydj_stripped.txt /dqydj_stripped.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在准备进行最后一步 - 向 Apache Spark 写入一些代码。</p><h3 id="VI-点亮-Apache-Spark"><a href="#VI-点亮-Apache-Spark" class="headerlink" title="VI. 点亮 Apache Spark"></a>VI. 点亮 Apache Spark</h3><p>Cloudera 有个极棒的程序可以作为我们的超级单词计数程序的基础，<a target="_blank" rel="noopener" href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/spark_develop_run.html">你可以在这里找到</a>。我们接下来为我们的内省式单词计数程序修改它。</p><p>在主节点上<a target="_blank" rel="noopener" href="https://pypi.python.org/pypi/stop-words">安装‘stop-words’</a>这个 python 第三方包，虽然有趣（我在 DQYDJ 上使用了 23,295 次 the 这个单词），你可能不想看到这些语法单词占据着单词计数的前列，另外，在下列代码用你自己的数据集替换所有有关指向 dqydj 文件的地方。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">from stop_words import get_stop_words</span><br><span class="line">from pyspark import SparkContext, SparkConf</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">  # create Spark context with Spark configuration</span><br><span class="line">  conf = SparkConf().setAppName(&quot;Spark Count&quot;)</span><br><span class="line">  sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">  # get threshold</span><br><span class="line">  try:</span><br><span class="line">    threshold = int(sys.argv[2])</span><br><span class="line">  except:</span><br><span class="line">    threshold = 5</span><br><span class="line"></span><br><span class="line">  # read in text file and split each document into words</span><br><span class="line">  tokenized = sc.textFile(sys.argv[1]).flatMap(lambda line: line.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">  # count the occurrence of each word</span><br><span class="line">  wordCounts = tokenized.map(lambda word: (word.lower().strip(), 1)).reduceByKey(lambda v1,v2:v1 +v2)</span><br><span class="line"></span><br><span class="line">  # filter out words with fewer than threshold occurrences</span><br><span class="line">  filtered = wordCounts.filter(lambda pair:pair[1] &gt;= threshold)</span><br><span class="line"></span><br><span class="line">  print &quot;*&quot; * 80</span><br><span class="line">  print &quot;Printing top words used&quot;</span><br><span class="line">  print &quot;-&quot; * 80</span><br><span class="line">  filtered_sorted = sorted(filtered.collect(), key=lambda x: x[1], reverse = True)</span><br><span class="line">  for (word, count) in filtered_sorted: print &quot;%s : %d&quot; % (word.encode(&#x27;utf-8&#x27;).strip(), count)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  # Remove stop words</span><br><span class="line">  print &quot;\n\n&quot;</span><br><span class="line">  print &quot;*&quot; * 80</span><br><span class="line">  print &quot;Printing top non-stop words used&quot;</span><br><span class="line">  print &quot;-&quot; * 80</span><br><span class="line">  # Change this to your language code (see the stop-words documentation)</span><br><span class="line">  stop_words = set(get_stop_words(&#x27;en&#x27;))</span><br><span class="line">  no_stop_words = filter(lambda x: x[0] not in stop_words, filtered_sorted)</span><br><span class="line">  for (word, count) in no_stop_words: print &quot;%s : %d&quot; % (word.encode(&#x27;utf-8&#x27;).strip(), count)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>保存好 wordCount.py，确保上面的路径都是正确无误的。</p><p>现在，准备念出咒语，让运行在 YARN 上的 Spark 跑起来，你可以看到我在 DQYDJ 使用最多的单词是哪一个。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark-2.0.0-bin-hadoop2.7/bin/spark-submit –master yarn –executor-memory 512m –name wordcount –executor-cores 8 wordCount.py /dqydj_stripped.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="VII-我在-DQYDJ-使用最多的单词"><a href="#VII-我在-DQYDJ-使用最多的单词" class="headerlink" title="VII. 我在 DQYDJ 使用最多的单词"></a>VII. 我在 DQYDJ 使用最多的单词</h3><p>可能入列的单词有哪一些呢？“can, will, it’s, one, even, like, people, money, don’t, also“.</p><p>嘿，不错，“money”悄悄挤进了前十。在一个致力于金融、投资和经济的网站上谈论这似乎是件好事，对吧？</p><p>下面是的前 50 个最常用的词汇，请用它们刻画出有关我的文章的水平的结论。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201705/07/023810wrwuwarhr6z6hpph.png"></p><p>我希望你能喜欢这篇关于 Hadoop、YARN 和 Apache Spark 的教程，现在你可以在 Spark 运行和编写其他的应用了。</p><p>你的下一步是任务是开始<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.0.0/api/python/index.html">阅读 pyspark 文档</a>（以及用于其他语言的该库），去学习一些可用的功能。根据你的兴趣和你实际存储的数据，你将会深入学习到更多——有流数据、SQL，甚至机器学习的软件包！</p><p>你怎么看？你要建立一个树莓派 Hadoop 集群吗？想要在其中挖掘一些什么吗？你在上面看到最令你惊奇的单词是什么？为什么 ‘S&amp;P’ 也能上榜？</p><p>（题图：Pixabay，CC0）</p><hr><p>via: <a target="_blank" rel="noopener" href="https://dqydj.com/raspberry-pi-hadoop-cluster-apache-spark-yarn/">https://dqydj.com/raspberry-pi-hadoop-cluster-apache-spark-yarn/</a></p><p>作者：<a target="_blank" rel="noopener" href="https://dqydj.com/about/#contact_us">PK</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/sfantree">popy32</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 组织编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p><script defer src="https://pv.undefined.today/tracker.min.js" data-website-id="LinuxCNMirror-tracker"></script></div></div></body></html>