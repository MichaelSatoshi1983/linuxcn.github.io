<!doctype html><html lang="en"><head><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>利用 Python 爬取网站的新手指南 - 归墟星火集 又一个 LinuxCN 站点</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 又一个 LinuxCN 站点 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">利用 Python 爬取网站的新手指南</h1><span class="post-date">2021-01-24</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/">软件开发</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/%E7%88%AC%E5%8F%96/">爬取</a></div><div class="post-content"><blockquote><p>通过基本的 Python 工具获得爬取完整 HTML 网站的实践经验。</p></blockquote><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093527skakhvc3kalnuxa1.jpg" title="HTML code"></p><p>有很多很棒的书可以帮助你学习 Python ，但是谁真正读了这那些大部头呢？（剧透：反正不是我）。</p><p>许多人觉得教学书籍很有用，但我通常不会从头到尾地阅读一本书来学习。我是通过做一个项目，努力的弄清楚一些内容，然后再读另一本书来学习。因此，暂时丢掉书，让我们一起学习 Python。</p><p>接下来是我的第一个 Python 爬取项目的指南。它对 Python 和 HTML 的假定知识要求很低。这篇文章旨在说明如何使用 Python 的 <a target="_blank" rel="noopener" href="https://requests.readthedocs.io/en/master/">requests</a> 库访问网页内容，并使用 <a target="_blank" rel="noopener" href="https://beautiful-soup-4.readthedocs.io/en/latest/">BeatifulSoup4</a> 库以及 JSON 和 <a target="_blank" rel="noopener" href="https://pandas.pydata.org/">pandas</a> 库解析网页内容。我将简要介绍 <a target="_blank" rel="noopener" href="https://www.selenium.dev/">Selenium</a> 库，但我不会深入研究如何使用该库——这个主题值得有自己的教程。最终，我希望向你展示一些技巧和小窍门，以减少网页爬取过程中遇到的问题。</p><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>我的 <a target="_blank" rel="noopener" href="https://github.com/jpiaskowski/pycas2020_web_scraping">GitHub 存储库</a> 中提供了本指南的所有资源。如果需要安装 Python3 的帮助，请查看 <a target="_blank" rel="noopener" href="https://opensource.com/article/20/4/install-python-linux">Linux</a>、<a target="_blank" rel="noopener" href="https://opensource.com/article/19/8/how-install-python-windows">Windows</a> 和 <a target="_blank" rel="noopener" href="https://opensource.com/article/19/5/python-3-default-mac">Mac</a> 的教程。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python3 -m venv</span><br><span class="line">$ source venv/bin/activate</span><br><span class="line">$ pip install requests bs4 pandas</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果你喜欢使用 JupyterLab ，则可以使用 <a target="_blank" rel="noopener" href="https://github.com/jpiaskowski/pycas2020_web_scraping/blob/master/example/Familydollar_location_scrape-all-states.ipynb">notebook</a> 运行所有代码。<a target="_blank" rel="noopener" href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">安装 JupyterLab</a> 有很多方法，这是其中一种：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># from the same virtual environment as above, run:</span><br><span class="line">$ pip install jupyterlab</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="为网站抓取项目设定目标"><a href="#为网站抓取项目设定目标" class="headerlink" title="为网站抓取项目设定目标"></a>为网站抓取项目设定目标</h3><p>现在我们已经安装了依赖项，但是爬取网页需要做什么？</p><p>让我们退一步，确保使目标清晰。下面是成功完成网页爬取项目需求列表：</p><ul><li>我们收集的信息，是值得我们花大力气去建立一个有效的网页爬取器的。</li><li>我们所下载的信息是可以通过网页爬取器合法和道德地收集的。</li><li>对如何在 HTML 代码中找到目标信息有一定的了解。</li><li>利用恰当的工具：在此情况下，需要使用 BeautifulSoup 库和 requests 库。</li><li>知道（或愿意去学习）如何解析 JSON 对象。</li><li>有足够的 pandas 数据处理技能。</li></ul><p>关于 HTML 的备注：HTML 是运行在互联网上的“猛兽”，但我们最需要了解的是标签的工作方式。标签是一对由尖括号包围关键词（一般成对出现，其内容在两个标签中间）。比如，这是一个假装的标签，称为 <code>pro-tip</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;pro-tip&gt; All you need to know about html is how tags work &lt;/pro-tip&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们可以通过调用标签 <code>pro-tip</code> 来访问其中的信息（<code>All you need to know…</code>）。本教程将进一步介绍如何查找和访问标签。要进一步了解 HTML 基础知识，请查看 <a target="_blank" rel="noopener" href="https://opensource.com/article/20/4/build-websites">本文</a>。</p><h3 id="网站爬取项目中要找的是什么"><a href="#网站爬取项目中要找的是什么" class="headerlink" title="网站爬取项目中要找的是什么"></a>网站爬取项目中要找的是什么</h3><p>有些数据利用网站爬取采集比利用其他方法更合适。以下是我认为合适项目的准则：</p><p>没有可用于数据（处理）的公共 API。通过 API 抓取结构化数据会容易得多，（所以没有 API ）有助于澄清收集数据的合法性和道德性。而有相当数量的结构化数据，并有规律的、可重复的格式，才能证明这种努力的合理性。网页爬取可能会很痛苦。BeautifulSoup（bs4）使操作更容易，但无法避免网站的个别特殊性，需要进行定制。数据的相同格式化不是必须的，但这确实使事情变得更容易。存在的 “边际案例”（偏离规范）越多，爬取就越复杂。</p><p>免责声明：我没有参加过法律培训；以下内容无意作为正式的法律建议。</p><p>关于合法性，访问大量有价值信息可能令人兴奋，但仅仅因为它是可能的，并不意味着应该这样做。</p><p>值得庆幸的是，有一些公共信息可以指导我们的道德规范和网页爬取工具。大多数网站都有与该网站关联的 <a target="_blank" rel="noopener" href="https://www.contentkingapp.com/academy/robotstxt/">robots.txt</a> 文件，指出允许哪些爬取活动，哪些不被允许。它主要用于与搜索引擎（网页抓取工具的终极形态）进行交互。然而，网站上的许多信息都被视为公共信息。因此，有人将 <code>robots.txt</code> 文件视为一组建议，而不是具有法律约束力的文档。 <code>robots.txt</code> 文件并不涉及数据的道德收集和使用等主题。</p><p>在开始爬取项目之前，问自己以下问题：</p><ul><li>我是否在爬取版权材料？</li><li>我的爬取活动会危害个人隐私吗？</li><li>我是否发送了大量可能会使服务器超载或损坏的请求？</li><li>爬取是否会泄露出我不拥有的知识产权？</li><li>是否有规范网站使用的服务条款，我是否遵循了这些条款？</li><li>我的爬取活动会减少原始数据的价值吗？（例如，我是否打算按原样重新打包数据，或者可能从原始来源中抽取网站流量）？</li></ul><p>当我爬取一个网站时，请确保可以对所有这些问题回答 “否”。</p><p>要深入了解这些法律问题，请参阅 2018 年出版的 Krotov 和 Silva 撰写的<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/324907302_Legality_and_Ethics_of_Web_Scraping">《Web 爬取的合法性和道德性》</a> 和 Sellars 的<a target="_blank" rel="noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3221625">《二十年 Web 爬取和计算机欺诈与滥用法案》</a>。</p><h3 id="现在开始爬取网站"><a href="#现在开始爬取网站" class="headerlink" title="现在开始爬取网站"></a>现在开始爬取网站</h3><p>经过上述评估，我想出了一个项目。我的目标是爬取爱达荷州所有 Family Dollar 商店的地址。 这些商店在农村地区规模很大，因此我想了解有多少家这样的商店。</p><p>起点是 <a target="_blank" rel="noopener" href="https://locations.familydollar.com/id/">Family Dollar 的位置页面</a></p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093542e8yzcj1z1jqjuxey.png" alt="爱达荷州 Family Dollar 所在地页面" title="Family Dollar Idaho locations page"></p><p>首先，让我们在 Python 虚拟环境中加载先决条件。此处的代码将被添加到一个 Python 文件（如果你想要个名称，则为 <code>scraper.py</code>）或在 JupyterLab 的单元格中运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import requests # for making standard html requests</span><br><span class="line">from bs4 import BeautifulSoup # magical tool for parsing html data</span><br><span class="line">import json # for parsing data</span><br><span class="line">from pandas import DataFrame as df # premier library for data organization</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来，我们从目标 URL 中请求数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">page = requests.get(&quot;https://locations.familydollar.com/id/&quot;)</span><br><span class="line">soup = BeautifulSoup(page.text, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>BeautifulSoup 将 HTML 或 XML 内容转换为复杂树对象。这是我们将使用的几种常见对象类型。</p><ul><li><code>BeautifulSoup</code> —— 解析的内容</li><li><code>Tag</code> —— 标准 HTML 标记，这是你将遇到的 <code>bs4</code> 元素的主要类型</li><li><code>NavigableString</code> —— 标签内的文本字符串</li><li><code>Comment</code> —— NavigableString 的一种特殊类型</li></ul><p>当我们查看 <code>requests.get()</code> 输出时，还有更多要考虑的问题。我仅使用 <code>page.text()</code> 将请求的页面转换为可读的内容，但是还有其他输出类型：</p><ul><li><code>page.text()</code> 文本（最常见）</li><li><code>page.content()</code> 逐字节输出</li><li><code>page.json()</code> JSON 对象</li><li><code>page.raw()</code> 原始套接字响应（对你没啥用）</li></ul><p>我只在使用拉丁字母的纯英语网站上操作。 <code>requests</code> 中的默认编码设置可以很好地解决这一问题。然而，除了纯英语网站之外，就是更大的互联网世界。为了确保 <code>requests</code> 正确解析内容，你可以设置文本的编码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">page = requests.get(URL)</span><br><span class="line">page.encoding = &#x27;ISO-885901&#x27;</span><br><span class="line">soup = BeautifulSoup(page.text, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>仔细研究 <code>BeautifulSoup</code> 标签，我们看到:</p><ul><li><code>bs4</code> 元素 <code>tag</code> 捕获的是一个 HTML 标记。</li><li>它具有名称和属性，可以像字典一样访问：<code>tag[&#39;someAttribute&#39;]</code>。</li><li>如果标签具有相同名称的多个属性，则仅访问第一个实例。</li><li>可通过 <code>tag.contents</code> 访问子标签。</li><li>所有标签后代都可以通过 <code>tag.contents</code> 访问。</li><li>你始终可以使用以下字符串：<code>re.compile(&quot;your_string&quot;)</code> 访问一个字符串的所有内容，而不是浏览 HTML 树。</li></ul><h3 id="确定如何提取相应内容"><a href="#确定如何提取相应内容" class="headerlink" title="确定如何提取相应内容"></a>确定如何提取相应内容</h3><p>警告：此过程可能令人沮丧。</p><p>网站爬取过程中的提取可能是一个令人生畏的充满了误区的过程。我认为解决此问题的最佳方法是从一个有代表性的示例开始然后进行扩展（此原理对于任何编程任务都是适用的）。查看页面的 HTML 源代码至关重要。有很多方法可以做到这一点。</p><p>你可以在终端中使用 Python 查看页面的整个源代码（不建议使用）。运行此代码需要你自担风险：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(soup.prettify())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>虽然打印出页面的整个源代码可能适用于某些教程中显示的玩具示例，但大多数现代网站的页面上都有大量内容。甚至 404 页面也可能充满了页眉、页脚等代码。</p><p>通常，在你喜欢的浏览器中通过 “查看页面源代码” 来浏览源代码是最容易的（单击右键，然后选择 “查看页面源代码” ）。这是找到目标内容的最可靠方法（稍后我将解释原因）。</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093542m5fcwe6rbq2z1516.png" alt="Family Dollar 页面源代码" title="Family Dollar page source code"></p><p>在这种情况下，我需要在这个巨大的 HTML 海洋中找到我的目标内容 —— 地址、城市、州和邮政编码。通常，对页面源（<code>ctrl+F</code>）的简单搜索就会得到目标位置所在的位置。一旦我实际看到目标内容的示例（至少一个商店的地址），便会找到将该内容与其他内容区分开的属性或标签。</p><p>首先，我需要在爱达荷州 Family Dollar 商店中收集不同城市的网址，并访问这些网站以获取地址信息。这些网址似乎都包含在 <code>href</code> 标记中。太棒了！我将尝试使用 <code>find_all</code> 命令进行搜索：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dollar_tree_list = soup.find_all(&#x27;href&#x27;)</span><br><span class="line">dollar_tree_list</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>搜索 <code>href</code> 不会产生任何结果，该死。这可能是因为 <code>href</code> 嵌套在 <code>itemlist</code> 类中而失败。对于下一次尝试，请搜索 <code>item_list</code>。由于 <code>class</code> 是 Python 中的保留字，因此使用 <code>class_</code> 来作为替代。<code>soup.find_all()</code> 原来是 <code>bs4</code> 函数的瑞士军刀。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dollar_tree_list = soup.find_all(class_ = &#x27;itemlist&#x27;)</span><br><span class="line">for i in dollar_tree_list[:2]:</span><br><span class="line">  print(i)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>有趣的是，我发现搜索一个特定类的方法一般是一种成功的方法。通过找出对象的类型和长度，我们可以了解更多有关对象的信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type(dollar_tree_list)</span><br><span class="line">len(dollar_tree_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以使用 <code>.contents</code> 从 BeautifulSoup “结果集” 中提取内容。这也是创建单个代表性示例的好时机。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">example = dollar_tree_list[2] # a representative example</span><br><span class="line">example_content = example.contents</span><br><span class="line">print(example_content)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>使用 <code>.attr</code> 查找该对象内容中存在的属性。注意：<code>.contents</code> 通常会返回一个项目的精确的列表，因此第一步是使用方括号符号为该项目建立索引。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">example_content = example.contents[0]</span><br><span class="line">example_content.attrs</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在，我可以看到 <code>href</code> 是一个属性，可以像字典项一样提取它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">example_href = example_content[&#x27;href&#x27;]</span><br><span class="line">print(example_href)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="整合网站抓取工具"><a href="#整合网站抓取工具" class="headerlink" title="整合网站抓取工具"></a>整合网站抓取工具</h3><p>所有的这些探索为我们提供了前进的路径。这是厘清上面逻辑的一个清理版本。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">city_hrefs = [] # initialise empty list</span><br><span class="line"></span><br><span class="line">for i in dollar_tree_list:</span><br><span class="line">    cont = i.contents[0]</span><br><span class="line">    href = cont[&#x27;href&#x27;]</span><br><span class="line">    city_hrefs.append(href)</span><br><span class="line"></span><br><span class="line">#  check to be sure all went well</span><br><span class="line">for i in city_hrefs[:2]:</span><br><span class="line">  print(i)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出的内容是一个关于抓取爱达荷州 Family Dollar 商店 URL 的列表。</p><p>也就是说，我仍然没有获得地址信息！现在，需要抓取每个城市的 URL 以获得此信息。因此，我们使用一个具有代表性的示例重新开始该过程。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">page2 = requests.get(city_hrefs[2]) # again establish a representative example</span><br><span class="line">soup2 = BeautifulSoup(page2.text, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093542sicag2cggogamoc4.png" alt="Family Dollar 地图和代码" title="Family Dollar map and code"></p><p>地址信息嵌套在 <code>type=&quot;application/ld+json&quot;</code> 里。经过大量的地理位置抓取之后，我开始认识到这是用于存储地址信息的一般结构。幸运的是，<code>soup.find_all()</code> 开启了利用 <code>type</code> 搜索。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arco = soup2.find_all(type=&quot;application/ld+json&quot;)</span><br><span class="line">print(arco[1])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>地址信息在第二个列表成员中！原来如此！</p><p>使用 <code>.contents</code> 提取（从第二个列表项中）内容（这是过滤后的合适的默认操作）。同样，由于输出的内容是一个列表，因此我为该列表项建立了索引：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arco_contents = arco[1].contents[0]</span><br><span class="line">arco_contents</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>喔，看起来不错。此处提供的格式与 JSON 格式一致（而且，该类型的名称中确实包含 “json”）。 JSON 对象的行为就像是带有嵌套字典的字典。一旦你熟悉利用其去工作，它实际上是一种不错的格式（当然，它比一长串正则表达式命令更容易编程）。尽管从结构上看起来像一个 JSON 对象，但它仍然是 <code>bs4</code> 对象，需要通过编程方式转换为 JSON 对象才能对其进行访问：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arco_json =  json.loads(arco_contents)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type(arco_json)</span><br><span class="line">print(arco_json)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在该内容中，有一个被调用的 <code>address</code> 键，该键要求地址信息在一个比较小的嵌套字典里。可以这样检索：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arco_address = arco_json[&#x27;address&#x27;]</span><br><span class="line">arco_address</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>好吧，请大家注意。现在我可以遍历存储爱达荷州 URL 的列表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">locs_dict = [] # initialise empty list</span><br><span class="line"></span><br><span class="line">for link in city_hrefs:</span><br><span class="line">  locpage = requests.get(link)   # request page info</span><br><span class="line">  locsoup = BeautifulSoup(locpage.text, &#x27;html.parser&#x27;)</span><br><span class="line">      # parse the page&#x27;s content</span><br><span class="line">  locinfo = locsoup.find_all(type=&quot;application/ld+json&quot;)</span><br><span class="line">      # extract specific element</span><br><span class="line">  loccont = locinfo[1].contents[0]  </span><br><span class="line">      # get contents from the bs4 element set</span><br><span class="line">  locjson = json.loads(loccont)  # convert to json</span><br><span class="line">  locaddr = locjson[&#x27;address&#x27;] # get address</span><br><span class="line">  locs_dict.append(locaddr) # add address to list</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="用-Pandas-整理我们的网站抓取结果"><a href="#用-Pandas-整理我们的网站抓取结果" class="headerlink" title="用 Pandas 整理我们的网站抓取结果"></a>用 Pandas 整理我们的网站抓取结果</h3><p>我们在字典中装载了大量数据，但是还有一些额外的无用项，它们会使重用数据变得比需要的更为复杂。要执行最终的数据组织，我们需要将其转换为 Pandas 数据框架，删除不需要的列 <code>@type</code> 和 <code>country</code>，并检查前五行以确保一切正常。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">locs_df = df.from_records(locs_dict)</span><br><span class="line">locs_df.drop([&#x27;@type&#x27;, &#x27;addressCountry&#x27;], axis = 1, inplace = True)</span><br><span class="line">locs_df.head(n = 5)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>确保保存结果！！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.to_csv(locs_df, &quot;family_dollar_ID_locations.csv&quot;, sep = &quot;,&quot;, index = False)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们做到了！所有爱达荷州 Family Dollar 商店都有一个用逗号分隔的列表。多令人兴奋。</p><h3 id="Selenium-和数据抓取的一点说明"><a href="#Selenium-和数据抓取的一点说明" class="headerlink" title="Selenium 和数据抓取的一点说明"></a>Selenium 和数据抓取的一点说明</h3><p><a target="_blank" rel="noopener" href="https://www.selenium.dev/">Selenium</a> 是用于与网页自动交互的常用工具。为了解释为什么有时必须使用它，让我们来看一个使用 Walgreens 网站的示例。 “检查元素” 提供了浏览器显示内容的代码：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093543kizruhh3yqq5yhcy.png" alt="Walgreens 位置页面和代码" title="Walgreens location page and code"></p><p>虽然 “查看页面源代码” 提供了有关 <code>requests</code> 将获得什么内容的代码：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093543wx98gxo3xog299uo.png" alt="Walgreens 源代码" title="Walgreens source code"></p><p>如果这两个不一致，是有一些插件可以修改源代码 —— 因此，应在将页面加载到浏览器后对其进行访问。<code>requests</code> 不能做到这一点，但是 Selenium 可以做到。</p><p>Selenium 需要 Web 驱动程序来检索内容。实际上，它会打开 Web 浏览器，并收集此页面的内容。Selenium 功能强大 —— 它可以通过多种方式与加载的内容进行交互（请阅读文档）。使用 Selenium 获取数据后，继续像以前一样使用 BeautifulSoup：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">url = &quot;https://www.walgreens.com/storelistings/storesbycity.jsp?requestType=locator&amp;state=ID&quot;</span><br><span class="line">driver = webdriver.Firefox(executable_path = &#x27;mypath/geckodriver.exe&#x27;)</span><br><span class="line">driver.get(url)</span><br><span class="line">soup_ID = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;)</span><br><span class="line">store_link_soup = soup_ID.find_all(class_ = &#x27;col-xl-4 col-lg-4 col-md-4&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对于 Family Dollar 这种情形，我不需要 Selenium，但是当呈现的内容与源代码不同时，我确实会保留使用 Selenium。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>总之，使用网站抓取来完成有意义的任务时：</p><ul><li>耐心一点</li><li>查阅手册（它们非常有帮助）</li></ul><p>如果你对答案感到好奇：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/202101/24/093543sbnbl5w8znxsk88b.png" alt="Family Dollar 位置图" title="Family Dollar locations map"></p><p>美国有很多 Family Dollar 商店。</p><p>完整的源代码是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import json</span><br><span class="line">from pandas import DataFrame as df</span><br><span class="line"></span><br><span class="line">page = requests.get(&quot;https://www.familydollar.com/locations/&quot;)</span><br><span class="line">soup = BeautifulSoup(page.text, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br><span class="line"># find all state links</span><br><span class="line">state_list = soup.find_all(class_ = &#x27;itemlist&#x27;)</span><br><span class="line"></span><br><span class="line">state_links = []</span><br><span class="line"></span><br><span class="line">for i in state_list:</span><br><span class="line">    cont = i.contents[0]</span><br><span class="line">    attr = cont.attrs</span><br><span class="line">    hrefs = attr[&#x27;href&#x27;]</span><br><span class="line">    state_links.append(hrefs)</span><br><span class="line"></span><br><span class="line"># find all city links</span><br><span class="line">city_links = []</span><br><span class="line"></span><br><span class="line">for link in state_links:</span><br><span class="line">    page = requests.get(link)</span><br><span class="line">    soup = BeautifulSoup(page.text, &#x27;html.parser&#x27;)</span><br><span class="line">    familydollar_list = soup.find_all(class_ = &#x27;itemlist&#x27;)</span><br><span class="line">    for store in familydollar_list:</span><br><span class="line">        cont = store.contents[0]</span><br><span class="line">        attr = cont.attrs</span><br><span class="line">        city_hrefs = attr[&#x27;href&#x27;]</span><br><span class="line">        city_links.append(city_hrefs)</span><br><span class="line"># to get individual store links</span><br><span class="line">store_links = []</span><br><span class="line"></span><br><span class="line">for link in city_links:</span><br><span class="line">    locpage = requests.get(link)</span><br><span class="line">    locsoup = BeautifulSoup(locpage.text, &#x27;html.parser&#x27;)</span><br><span class="line">    locinfo = locsoup.find_all(type=&quot;application/ld+json&quot;)</span><br><span class="line">    for i in locinfo:</span><br><span class="line">        loccont = i.contents[0]</span><br><span class="line">        locjson = json.loads(loccont)</span><br><span class="line">        try:</span><br><span class="line">            store_url = locjson[&#x27;url&#x27;]</span><br><span class="line">            store_links.append(store_url)</span><br><span class="line">        except:</span><br><span class="line">            pass</span><br><span class="line"></span><br><span class="line"># get address and geolocation information</span><br><span class="line">stores = []</span><br><span class="line"></span><br><span class="line">for store in store_links:</span><br><span class="line">    storepage = requests.get(store)</span><br><span class="line">    storesoup = BeautifulSoup(storepage.text, &#x27;html.parser&#x27;)</span><br><span class="line">    storeinfo = storesoup.find_all(type=&quot;application/ld+json&quot;)</span><br><span class="line">    for i in storeinfo:</span><br><span class="line">        storecont = i.contents[0]</span><br><span class="line">        storejson = json.loads(storecont)</span><br><span class="line">        try:</span><br><span class="line">            store_addr = storejson[&#x27;address&#x27;]</span><br><span class="line">            store_addr.update(storejson[&#x27;geo&#x27;])</span><br><span class="line">            stores.append(store_addr)</span><br><span class="line">        except:</span><br><span class="line">            pass</span><br><span class="line"></span><br><span class="line"># final data parsing</span><br><span class="line">stores_df = df.from_records(stores)</span><br><span class="line">stores_df.drop([&#x27;@type&#x27;, &#x27;addressCountry&#x27;], axis = 1, inplace = True)</span><br><span class="line">stores_df[&#x27;Store&#x27;] = &quot;Family Dollar&quot;</span><br><span class="line"></span><br><span class="line">df.to_csv(stores_df, &quot;family_dollar_locations.csv&quot;, sep = &quot;,&quot;, index = False)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>作者注释：本文改编自 2020 年 2 月 9 日在俄勒冈州波特兰的<a target="_blank" rel="noopener" href="https://2020.pycascades.com/talks/adventures-in-babysitting-webscraping-for-python-and-html-novices/">我在 PyCascades 的演讲</a>。</p><hr><p>via: <a target="_blank" rel="noopener" href="https://opensource.com/article/20/5/web-scraping-python">https://opensource.com/article/20/5/web-scraping-python</a></p><p>作者：<a target="_blank" rel="noopener" href="https://opensource.com/users/julia-piaskowski">Julia Piaskowski</a> 选题：<a target="_blank" rel="noopener" href="https://github.com/lujun9972">lujun9972</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/stevenzdg988">stevenzdg988</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p></div></div></body></html>