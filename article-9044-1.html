<!doctype html><html lang="en"><head><meta name="msvalidate.01" content="D404690CEFCB54C7762AC84935B99171"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?6618da70c90c8744eead2e9371fb5077";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script type="text/javascript">!function(t,e,n,c,s,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/s5f3f0tojf",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta http-equiv="X-UA-Compatible" content="ie=edge"><link rel="stylesheet" href="/styles/base.css"><link rel="stylesheet" href="/styles/theme.css"><link rel="shortcut icon" href="/favicon.png"><title>如何分析博客中最流行的编程语言 - 归墟星火集 又一个 LinuxCN 站点</title><meta name="generator" content="Hexo 7.3.0"></head><body><div class="header-title"><span class="header-light"></span> <span class="header-light"></span> <span class="header-light"></span> <span>归墟星火集 又一个 LinuxCN 站点 linuxcn.undefined.today<span></span></span></div><div class="container"><ul class="nav"><li><a href="/">首页</a></li><li><a target="_blank" rel="noopener" href="https://undefined.today/">Blog</a></li></ul><div class="content"><div class="post-container"><div class="post-header"><span class="ui-tips">标题：</span><h1 class="ui-keyword post-title">如何分析博客中最流行的编程语言</h1><span class="post-date">2017-11-09</span></div><div class="post-header"><span class="ui-tips">分类：</span> <a href="/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91/">软件开发</a></div><div class="post-header"><span class="ui-tips">标签：</span> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/">博客</a></div><div class="post-content"><blockquote><p>摘要：这篇文章我们将对一些各种各样的博客的流行度相对于他们在谷歌上的排名进行一个分析。所有代码可以在 <a target="_blank" rel="noopener" href="https://github.com/Databrawl/blog_analysis">github</a> 上找到。</p></blockquote><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201711/09/180208dcnhwgbjbx1c1sc1.jpg"></p><h3 id="想法来源"><a href="#想法来源" class="headerlink" title="想法来源"></a>想法来源</h3><p>我一直在想，各种各样的博客每天到底都有多少页面浏览量，以及在博客阅读受众中最受欢迎的是什么编程语言。我也很感兴趣的是，它们在谷歌的网站排名是否与它们的受欢迎程度直接相关。</p><p>为了回答这些问题，我决定做一个 Scrapy 项目，它将收集一些数据，然后对所获得的信息执行特定的数据分析和数据可视化。</p><h3 id="第一部分：Scrapy"><a href="#第一部分：Scrapy" class="headerlink" title="第一部分：Scrapy"></a>第一部分：Scrapy</h3><p>我们将使用 <a target="_blank" rel="noopener" href="https://scrapy.org/">Scrapy</a> 为我们的工作，因为它为抓取和对该请求处理后的反馈进行管理提供了干净和健壮的框架。我们还将使用 <a target="_blank" rel="noopener" href="https://github.com/scrapinghub/splash">Splash</a> 来解析需要处理的 Javascript 页面。Splash 使用自己的 Web 服务器充当代理，并处理 Javascript 响应，然后再将其重定向到我们的爬虫进程。</p><blockquote><p>我这里没有描述 Scrapy 的设置，也没有描述 Splash 的集成。你可以在<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/intro/tutorial.html">这里</a>找到 Scrapy 的示例，而<a target="_blank" rel="noopener" href="https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/">这里</a>还有 Scrapy+Splash 指南。</p></blockquote><h4 id="获得相关的博客"><a href="#获得相关的博客" class="headerlink" title="获得相关的博客"></a>获得相关的博客</h4><p>第一步显然是获取数据。我们需要关于编程博客的谷歌搜索结果。你看，如果我们开始仅仅用谷歌自己来搜索，比如说查询 “Python”，除了博客，我们还会得到很多其它的东西。我们需要的是做一些过滤，只留下特定的博客。幸运的是，有一种叫做 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Google_Custom_Search">Google 自定义搜索引擎（CSE）</a>的东西，它能做到这一点。还有一个网站 <a target="_blank" rel="noopener" href="http://www.blogsearchengine.org/">www.blogsearchengine.org</a>，它正好可以满足我们需要，它会将用户请求委托给 CSE，这样我们就可以查看它的查询并重复利用它们。</p><p>所以，我们要做的是到 <a target="_blank" rel="noopener" href="http://www.blogsearchengine.org/">www.blogsearchengine.org</a> 网站，搜索 “python”，并在一侧打开 Chrome 开发者工具中的网络标签页。这截图是我们将要看到的：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201711/09/181024bgawp0jaw3atqkga.png"></p><p>突出显示的是 blogsearchengine 向谷歌委派的一个搜索请求，所以我们将复制它，并在我们的 scraper 中使用。</p><p>这个博客抓取爬行器类会是如下这样的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class BlogsSpider(scrapy.Spider):</span><br><span class="line">    name = &#x27;blogs&#x27;</span><br><span class="line">    allowed_domains = [&#x27;cse.google.com&#x27;]</span><br><span class="line"></span><br><span class="line">    def __init__(self, queries):</span><br><span class="line">        super(BlogsSpider, self).__init__()</span><br><span class="line">        self.queries = queries</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>与典型的 Scrapy 爬虫不同，我们的方法覆盖了 <code>__init__</code> 方法，它接受额外的参数 <code>queries</code>，它指定了我们想要执行的查询列表。</p><p>现在，最重要的部分是构建和执行这个实际的查询。这个过程放在 <code>start_requests</code> 爬虫的方法里面执行，我们愉快地覆盖它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def start_requests(self):</span><br><span class="line">    params_dict = &#123;</span><br><span class="line">        &#x27;cx&#x27;: [&#x27;partner-pub-9634067433254658:5laonibews6&#x27;],</span><br><span class="line">        &#x27;cof&#x27;: [&#x27;FORID:10&#x27;],</span><br><span class="line">        &#x27;ie&#x27;: [&#x27;ISO-8859-1&#x27;],</span><br><span class="line">        &#x27;q&#x27;: [&#x27;query&#x27;],</span><br><span class="line">        &#x27;sa.x&#x27;: [&#x27;0&#x27;],</span><br><span class="line">        &#x27;sa.y&#x27;: [&#x27;0&#x27;],</span><br><span class="line">        &#x27;sa&#x27;: [&#x27;Search&#x27;],</span><br><span class="line">        &#x27;ad&#x27;: [&#x27;n9&#x27;],</span><br><span class="line">        &#x27;num&#x27;: [&#x27;10&#x27;],</span><br><span class="line">        &#x27;rurl&#x27;: [</span><br><span class="line">            &#x27;http://www.blogsearchengine.org/search.html?cx=partner-pub&#x27;</span><br><span class="line">            &#x27;-9634067433254658%3A5laonibews6&amp;cof=FORID%3A10&amp;ie=ISO-8859-1&amp;&#x27;</span><br><span class="line">            &#x27;q=query&amp;sa.x=0&amp;sa.y=0&amp;sa=Search&#x27;</span><br><span class="line">        ],</span><br><span class="line">        &#x27;siteurl&#x27;: [&#x27;http://www.blogsearchengine.org/&#x27;]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    params = urllib.parse.urlencode(params_dict, doseq=True)</span><br><span class="line">    url_template = urllib.parse.urlunparse(</span><br><span class="line">        [&#x27;https&#x27;, self.allowed_domains[0], &#x27;/cse&#x27;,</span><br><span class="line">         &#x27;&#x27;, params, &#x27;gsc.tab=0&amp;gsc.q=query&amp;gsc.page=page_num&#x27;])</span><br><span class="line">    for query in self.queries:</span><br><span class="line">        for page_num in range(1, 11):</span><br><span class="line">            url = url_template.replace(&#x27;query&#x27;, urllib.parse.quote(query))</span><br><span class="line">            url = url.replace(&#x27;page_num&#x27;, str(page_num))</span><br><span class="line">            yield SplashRequest(url, self.parse, endpoint=&#x27;render.html&#x27;,</span><br><span class="line">                                args=&#123;&#x27;wait&#x27;: 0.5&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里你可以看到相当复杂的 <code>params_dict</code> 字典，它控制所有我们之前找到的 Google CSE URL 的参数。然后我们准备好 <code>url_template</code> 里的一切，除了已经填好的查询和页码。我们对每种编程语言请求 10 页，每一页包含 10 个链接，所以是每种语言有 100 个不同的博客用来分析。</p><p>在 <code>42-43</code> 行，我使用一个特殊的类 <code>SplashRequest</code> 来代替 Scrapy 自带的 Request 类。它封装了 Splash 库内部的重定向逻辑，所以我们无需为此担心。十分整洁。</p><p>最后，这是解析程序：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    urls = response.css(&#x27;div.gs-title.gsc-table-cell-thumbnail&#x27;) \</span><br><span class="line">        .xpath(&#x27;./a/@href&#x27;).extract()</span><br><span class="line">    gsc_fragment = urllib.parse.urlparse(response.url).fragment</span><br><span class="line">    fragment_dict = urllib.parse.parse_qs(gsc_fragment)</span><br><span class="line">    page_num = int(fragment_dict[&#x27;gsc.page&#x27;][0])</span><br><span class="line">    query = fragment_dict[&#x27;gsc.q&#x27;][0]</span><br><span class="line">    page_size = len(urls)</span><br><span class="line">    for i, url in enumerate(urls):</span><br><span class="line">        parsed_url = urllib.parse.urlparse(url)</span><br><span class="line">        rank = (page_num - 1) * page_size + i</span><br><span class="line">        yield &#123;</span><br><span class="line">            &#x27;rank&#x27;: rank,</span><br><span class="line">            &#x27;url&#x27;: parsed_url.netloc,</span><br><span class="line">            &#x27;query&#x27;: query</span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>所有 Scraper 的核心和灵魂就是解析器逻辑。可以有多种方法来理解响应页面的结构并构建 XPath 查询字符串。您可以使用 <a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/shell.html">Scrapy shell</a> 尝试并随时调整你的 XPath 查询，而不用运行爬虫。不过我更喜欢可视化的方法。它需要再次用到谷歌 Chrome 开发人员控制台。只需右键单击你想要用在你的爬虫里的元素，然后按下 Inspect。它将打开控制台，并定位到你指定位置的 HTML 源代码。在本例中，我们想要得到实际的搜索结果链接。他们的源代码定位是这样的:</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201711/09/181052l5g5svh3hqosogyy.png"></p><p>在查看这个元素的描述后我们看到所找的 <code>&lt;div&gt;</code> 有一个 <code>.gsc-table-cell-thumbnail</code> CSS 类，它是 <code>.gs-title</code> <code>&lt;div&gt;</code> 的子元素，所以我们把它放到响应对象的 <code>css</code> 方法（<code>46</code> 行）。然后，我们只需要得到博客文章的 URL。它很容易通过<code>&#39;./a/@href&#39;</code> XPath 字符串来获得，它能从我们的 <code>&lt;div&gt;</code> 直接子元素的 <code>href</code> 属性找到。（LCTT 译注：此处图文对不上）</p><h4 id="寻找流量数据"><a href="#寻找流量数据" class="headerlink" title="寻找流量数据"></a>寻找流量数据</h4><p>下一个任务是估测每个博客每天得到的页面浏览量。得到这样的数据有<a target="_blank" rel="noopener" href="https://www.labnol.org/internet/find-website-traffic-hits/8008/">各种方式</a>，有免费的，也有付费的。在快速搜索之后，我决定基于简单且免费的原因使用网站 <a target="_blank" rel="noopener" href="http://www.statshow.com/">www.statshow.com</a> 来做。爬虫将抓取这个网站，我们在前一步获得的博客的 URL 将作为这个网站的输入参数，获得它们的流量信息。爬虫的初始化是这样的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class TrafficSpider(scrapy.Spider):</span><br><span class="line">    name = &#x27;traffic&#x27;</span><br><span class="line">    allowed_domains = [&#x27;www.statshow.com&#x27;]</span><br><span class="line"></span><br><span class="line">    def __init__(self, blogs_data):</span><br><span class="line">        super(TrafficSpider, self).__init__()</span><br><span class="line">        self.blogs_data = blogs_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>blogs_data</code> 应该是以下格式的词典列表：<code>&#123;&quot;rank&quot;: 70, &quot;url&quot;: &quot;www.stat.washington.edu&quot;， &quot;query&quot;: &quot;Python&quot;&#125;</code>。</p><p>请求构建函数如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def start_requests(self):</span><br><span class="line">    url_template = urllib.parse.urlunparse(</span><br><span class="line">        [&#x27;http&#x27;, self.allowed_domains[0], &#x27;/www/&#123;path&#125;&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;&#x27;])</span><br><span class="line">    for blog in self.blogs_data:</span><br><span class="line">        url = url_template.format(path=blog[&#x27;url&#x27;])</span><br><span class="line">        request = SplashRequest(url, endpoint=&#x27;render.html&#x27;,</span><br><span class="line">                                args=&#123;&#x27;wait&#x27;: 0.5&#125;, meta=&#123;&#x27;blog&#x27;: blog&#125;)</span><br><span class="line">        yield request</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>它相当的简单，我们只是把字符串 <code>/www/web-site-url/</code> 添加到 <code>&#39;www.statshow.com&#39;</code> URL 中。</p><p>现在让我们看一下语法解析器是什么样子的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    site_data = response.xpath(&#x27;//div[@id=&quot;box_1&quot;]/span/text()&#x27;).extract()</span><br><span class="line">    views_data = list(filter(lambda r: &#x27;$&#x27; not in r, site_data))</span><br><span class="line">    if views_data:</span><br><span class="line">        blog_data = response.meta.get(&#x27;blog&#x27;)</span><br><span class="line">        traffic_data = &#123;</span><br><span class="line">            &#x27;daily_page_views&#x27;: int(views_data[0].translate(&#123;ord(&#x27;,&#x27;): None&#125;)),</span><br><span class="line">            &#x27;daily_visitors&#x27;: int(views_data[1].translate(&#123;ord(&#x27;,&#x27;): None&#125;))</span><br><span class="line">        &#125;</span><br><span class="line">        blog_data.update(traffic_data)</span><br><span class="line">        yield blog_data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>与博客解析程序类似，我们只是通过 StatShow 示例的返回页面，然后找到包含每日页面浏览量和每日访问者的元素。这两个参数都确定了网站的受欢迎程度，对于我们的分析只需要使用页面浏览量即可 。</p><h3 id="第二部分：分析"><a href="#第二部分：分析" class="headerlink" title="第二部分：分析"></a>第二部分：分析</h3><p>这部分是分析我们搜集到的所有数据。然后，我们用名为 <a target="_blank" rel="noopener" href="https://bokeh.pydata.org/en/latest/">Bokeh</a> 的库来可视化准备好的数据集。我在这里没有给出运行器和可视化的代码，但是它可以在 <a target="_blank" rel="noopener" href="https://github.com/Databrawl/blog_analysis">GitHub repo</a> 中找到，包括你在这篇文章中看到的和其他一切东西。</p><blockquote><p>最初的结果集含有少许偏离过大的数据，（如 google.com、linkedin.com、Oracle.com 等）。它们显然不应该被考虑。即使其中有些有博客，它们也不是针对特定语言的。这就是为什么我们基于这个 <a target="_blank" rel="noopener" href="https://stackoverflow.com/a/16562028/1573766">StackOverflow 回答</a> 中所建议的方法来过滤异常值。</p></blockquote><h4 id="语言流行度比较"><a href="#语言流行度比较" class="headerlink" title="语言流行度比较"></a>语言流行度比较</h4><p>首先，让我们对所有的语言进行直接的比较，看看哪一种语言在前 100 个博客中有最多的浏览量。</p><p>这是能进行这个任务的函数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def get_languages_popularity(data):</span><br><span class="line">    query_sorted_data = sorted(data, key=itemgetter(&#x27;query&#x27;))</span><br><span class="line">    result = &#123;&#x27;languages&#x27;: [], &#x27;views&#x27;: []&#125;</span><br><span class="line">    popularity = []</span><br><span class="line">    for k, group in groupby(query_sorted_data, key=itemgetter(&#x27;query&#x27;)):</span><br><span class="line">        group = list(group)</span><br><span class="line">        daily_page_views = map(lambda r: int(r[&#x27;daily_page_views&#x27;]), group)</span><br><span class="line">        total_page_views = sum(daily_page_views)</span><br><span class="line">        popularity.append((group[0][&#x27;query&#x27;], total_page_views))</span><br><span class="line">    sorted_popularity = sorted(popularity, key=itemgetter(1), reverse=True)</span><br><span class="line">    languages, views = zip(*sorted_popularity)</span><br><span class="line">    result[&#x27;languages&#x27;] = languages</span><br><span class="line">    result[&#x27;views&#x27;] = views</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，我们首先按语言（词典中的关键字“query”）来分组我们的数据，然后使用 python 的 <code>groupby</code> 函数，这是一个从 SQL 中借来的奇妙函数，从我们的数据列表中生成一组条目，每个条目都表示一些编程语言。然后，在第 <code>14</code> 行我们计算每一种语言的总页面浏览量，然后添加 <code>(&#39;Language&#39;, rank)</code> 形式的元组到 <code>popularity</code> 列表中。在循环之后，我们根据总浏览量对流行度数据进行排序，并将这些元组展开到两个单独的列表中，然后在 <code>result</code> 变量中返回它们。</p><blockquote><p>最初的数据集有很大的偏差。我检查了到底发生了什么，并意识到如果我在 <a target="_blank" rel="noopener" href="http://blogsearchengine.org/">blogsearchengine.org</a> 上查询“C”，我就会得到很多无关的链接，其中包含了 “C” 的字母。因此，我必须将 C 排除在分析之外。这种情况几乎不会在 “R” 和其他类似 C 的名称中出现：“C++”、“C”。</p></blockquote><p>因此，如果我们将 C 从考虑中移除并查看其他语言，我们可以看到如下图:</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201711/09/181118vdtga5053aa9z9ta.png"></p><p>评估结论：Java 每天有超过 400 万的浏览量，PHP 和 Go 有超过 200 万，R 和 JavaScript 也突破了百万大关。</p><h4 id="每日网页浏览量与谷歌排名"><a href="#每日网页浏览量与谷歌排名" class="headerlink" title="每日网页浏览量与谷歌排名"></a>每日网页浏览量与谷歌排名</h4><p>现在让我们来看看每日访问量和谷歌的博客排名之间的联系。从逻辑上来说，不那么受欢迎的博客应该排名靠后，但这并没那么简单，因为其他因素也会影响排名，例如，如果在人气较低的博客上的文章更新一些，那么它很可能会首先出现。</p><p>数据准备工作以下列方式进行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def get_languages_popularity(data):</span><br><span class="line">    query_sorted_data = sorted(data, key=itemgetter(&#x27;query&#x27;))</span><br><span class="line">    result = &#123;&#x27;languages&#x27;: [], &#x27;views&#x27;: []&#125;</span><br><span class="line">    popularity = []</span><br><span class="line">    for k, group in groupby(query_sorted_data, key=itemgetter(&#x27;query&#x27;)):</span><br><span class="line">        group = list(group)</span><br><span class="line">        daily_page_views = map(lambda r: int(r[&#x27;daily_page_views&#x27;]), group)</span><br><span class="line">        total_page_views = sum(daily_page_views)</span><br><span class="line">        popularity.append((group[0][&#x27;query&#x27;], total_page_views))</span><br><span class="line">    sorted_popularity = sorted(popularity, key=itemgetter(1), reverse=True)</span><br><span class="line">    languages, views = zip(*sorted_popularity)</span><br><span class="line">    result[&#x27;languages&#x27;] = languages</span><br><span class="line">    result[&#x27;views&#x27;] = views</span><br><span class="line">    return result</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>该函数接受爬取到的数据和需要考虑的语言列表。我们对这些数据以语言的流行程度进行排序。后来，在类似的语言分组循环中，我们构建了 <code>(rank, views_number)</code> 元组（从 1 开始的排名）被转换为 2 个单独的列表。然后将这一对列表写入到生成的字典中。</p><p>前 8 位 GitHub 语言（除了 C）是如下这些：</p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201711/09/181142lofi8ol3awlzugsz.png"></p><p><img src="https://linuxcn.img.undefined.today/data/attachment/album/201711/09/181205envskfnfnxzc3v3t.png"></p><p>评估结论：我们看到，所有图的 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">PCC （皮尔逊相关系数）</a>都远离 1&#x2F;-1，这表示每日浏览量与排名之间缺乏相关性。值得注意的是，在大多数图表（8 个中的 7 个）中，相关性是负的，这意味着排名的降低会导致浏览量的减少。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>因此，根据我们的分析，Java 是目前最流行的编程语言，其次是 PHP、Go、R 和 JavaScript。在日常浏览量和谷歌排名上，排名前 8 的语言都没有很强的相关性，所以即使你刚刚开始写博客，你也可以在搜索结果中获得很高的评价。不过，成为热门博客究竟需要什么，可以留待下次讨论。</p><blockquote><p>这些结果是相当有偏差的，如果没有更多的分析，就不能过分的考虑这些结果。首先，在较长的一段时间内收集更多的流量信息，然后分析每日浏览量和排名的平均值（中值）值是一个好主意。也许我以后还会再回来讨论这个。</p></blockquote><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ol><li>抓取：<ol><li><a target="_blank" rel="noopener" href="https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/">blog.scrapinghub.com: Handling Javascript In Scrapy With Splash</a></li><li><a target="_blank" rel="noopener" href="http://www.blogsearchengine.org/">BlogSearchEngine.org</a></li><li><a target="_blank" rel="noopener" href="https://www.twingly.com/">twingly.com: Twingly Real-Time Blog Search</a></li><li><a target="_blank" rel="noopener" href="http://www.searchblogspot.com/">searchblogspot.com: finding blogs on blogspot platform</a></li></ol></li><li>流量评估：<ol><li><a target="_blank" rel="noopener" href="https://www.labnol.org/internet/find-website-traffic-hits/8008/">labnol.org: Find Out How Much Traffic a Website Gets</a></li><li><a target="_blank" rel="noopener" href="https://www.quora.com/What-are-the-best-free-tools-that-estimate-visitor-traffic-for-a-given-page-on-a-particular-website-that-you-do-not-own-or-operate-3rd-party-sites">quora.com: What are the best free tools that estimate visitor traffic…</a></li><li><a target="_blank" rel="noopener" href="http://www.statshow.com/">StatShow.com: The Stats Maker</a></li></ol></li></ol><hr><p>via: <a target="_blank" rel="noopener" href="https://www.databrawl.com/2017/10/08/blog-analysis/">https://www.databrawl.com/2017/10/08/blog-analysis/</a></p><p>作者：<a target="_blank" rel="noopener" href="https://www.databrawl.com/author/svmosingmail-com/">Serge Mosin</a> 译者：<a target="_blank" rel="noopener" href="https://github.com/Chao-zhi">Chao-zhi</a> 校对：<a target="_blank" rel="noopener" href="https://github.com/wxy">wxy</a></p><p>本文由 <a target="_blank" rel="noopener" href="https://github.com/LCTT/TranslateProject">LCTT</a> 原创编译，<a target="_blank" rel="noopener" href="https://linux.cn/">Linux中国</a> 荣誉推出</p></div></div></div><div class="footer"><p class="footer-copyright"><span>Powered by <a target="_blank" href="https://hexo.io">Hexo</a></span> <span>Theme <a target="_blank" href="https://github.com/tinkink-co/hexo-theme-terminal">Terminal</a></span><script type="text/javascript" src="https://cdn.staticfile.net/jquery/3.4.1/jquery.min.js"></script><script>getCDNinfo=function(){$.ajax({url:"/cdn-cgi/trace",success:function(a,n){let i="Antananarivo, Madagascar - (TNR);Cape Town, South Africa - (CPT);Casablanca, Morocco - (CMN);Dar Es Salaam, Tanzania - (DAR);Djibouti City, Djibouti - (JIB);Durban, South Africa - (DUR);Johannesburg, South Africa - (JNB);Kigali, Rwanda - (KGL);Lagos, Nigeria - (LOS);Luanda, Angola - (LAD);Maputo, MZ - (MPM);Mombasa, Kenya - (MBA);Port Louis, Mauritius - (MRU);Réunion, France - (RUN);Bangalore, India - (BLR);Bangkok, Thailand - (BKK);Bandar Seri Begawan, Brunei - (BWN);Cebu, Philippines - (CEB);Chengdu, China - (CTU);Chennai, India - (MAA);Chittagong, Bangladesh - (CGP);Chongqing, China - (CKG);Colombo, Sri Lanka - (CMB);Dhaka, Bangladesh - (DAC);Dongguan, China - (SZX);Foshan, China - (FUO);Fuzhou, China - (FOC);Guangzhou, China - (CAN);Hangzhou, China - (HGH);Hanoi, Vietnam - (HAN);Hengyang, China - (HNY);Ho Chi Minh City, Vietnam - (SGN);Hong Kong - (HKG);Hyderabad, India - (HYD);Islamabad, Pakistan - (ISB);Jakarta, Indonesia - (CGK);Jinan, China - (TNA);Karachi, Pakistan - (KHI);Kathmandu, Nepal - (KTM);Kolkata, India - (CCU);Kuala Lumpur, Malaysia - (KUL);Lahore, Pakistan - (LHE);Langfang, China - (NAY);Luoyang, China - (LYA);Macau - (MFM);Malé, Maldives - (MLE);Manila, Philippines - (MNL);Mumbai, India - (BOM);Nagpur, India - (NAG);Nanning, China - (NNG);New Delhi, India - (DEL);Osaka, Japan - (KIX);Phnom Penh, Cambodia - (PNH);Qingdao, China - (TAO);Seoul, South Korea - (ICN);Shanghai, China - (SHA);Shenyang, China - (SHE);Shijiazhuang, China - (SJW);Singapore, Singapore - (SIN);Suzhou, China - (SZV);Taipei - (TPE);Thimphu, Bhutan - (PBH);Tianjin, China - (TSN);Tokyo, Japan - (NRT);Ulaanbaatar, Mongolia - (ULN);Vientiane, Laos - (VTE);Wuhan, China - (WUH);Wuxi, China - (WUX);Xi'an, China - (XIY);Yerevan, Armenia - (EVN);Zhengzhou, China - (CGO);Zuzhou, China - (CSX);Amsterdam, Netherlands - (AMS);Athens, Greece - (ATH);Barcelona, Spain - (BCN);Belgrade, Serbia - (BEG);Berlin, Germany - (TXL);Brussels, Belgium - (BRU);Bucharest, Romania - (OTP);Budapest, Hungary - (BUD);Chișinău, Moldova - (KIV);Copenhagen, Denmark - (CPH);Cork, Ireland -  (ORK);Dublin, Ireland - (DUB);Düsseldorf, Germany - (DUS);Edinburgh, United Kingdom - (EDI);Frankfurt, Germany - (FRA);Geneva, Switzerland - (GVA);Gothenburg, Sweden - (GOT);Hamburg, Germany - (HAM);Helsinki, Finland - (HEL);Istanbul, Turkey - (IST);Kyiv, Ukraine - (KBP);Lisbon, Portugal - (LIS);London, United Kingdom - (LHR);Luxembourg City, Luxembourg - (LUX);Madrid, Spain - (MAD);Manchester, United Kingdom - (MAN);Marseille, France - (MRS);Milan, Italy - (MXP);Moscow, Russia - (DME);Munich, Germany - (MUC);Nicosia, Cyprus - (LCA);Oslo, Norway - (OSL);Paris, France - (CDG);Prague, Czech Republic - (PRG);Reykjavík, Iceland - (KEF);Riga, Latvia - (RIX);Rome, Italy - (FCO);Saint Petersburg, Russia - (LED);Sofia, Bulgaria - (SOF);Stockholm, Sweden - (ARN);Tallinn, Estonia - (TLL);Thessaloniki, Greece - (SKG);Vienna, Austria - (VIE);Vilnius, Lithuania - (VNO);Warsaw, Poland - (WAW);Zagreb, Croatia - (ZAG);Zürich, Switzerland - (ZRH);Arica, Chile - (ARI);Asunción, Paraguay - (ASU);Bogotá, Colombia - (BOG);Buenos Aires, Argentina - (EZE);Curitiba, Brazil - (CWB);Fortaleza, Brazil - (FOR);Guatemala City, Guatemala - (GUA);Lima, Peru - (LIM);Medellín, Colombia - (MDE);Panama City, Panama - (PTY);Porto Alegre, Brazil - (POA);Quito, Ecuador - (UIO);Rio de Janeiro, Brazil - (GIG);São Paulo, Brazil - (GRU);Santiago, Chile - (SCL);Willemstad, Curaçao - (CUR);St. George's, Grenada - (GND);Amman, Jordan - (AMM);Baghdad, Iraq - (BGW);Baku, Azerbaijan - (GYD);Beirut, Lebanon - (BEY);Doha, Qatar - (DOH);Dubai, United Arab Emirates - (DXB);Kuwait City, Kuwait - (KWI);Manama, Bahrain - (BAH);Muscat, Oman - (MCT);Ramallah - (ZDM);Riyadh, Saudi Arabia - (RUH);Tel Aviv, Israel - (TLV);Ashburn, VA, United States - (IAD);Atlanta, GA, United States - (ATL);Boston, MA, United States - (BOS);Buffalo, NY, United States - (BUF);Calgary, AB, Canada - (YYC);Charlotte, NC, United States - (CLT);Chicago, IL, United States - (ORD);Columbus, OH, United States - (CMH);Dallas, TX, United States - (DFW);Denver, CO, United States - (DEN);Detroit, MI, United States - (DTW);Honolulu, HI, United States - (HNL);Houston, TX, United States - (IAH);Indianapolis, IN, United States - (IND);Jacksonville, FL, United States - (JAX);Kansas City, MO, United States - (MCI);Las Vegas, NV, United States - (LAS);Los Angeles, CA, United States - (LAX);McAllen, TX, United States - (MFE);Memphis, TN, United States - (MEM);Mexico City, Mexico - (MEX);Miami, FL, United States - (MIA);Minneapolis, MN, United States - (MSP);Montgomery, AL, United States - (MGM);Montréal, QC, Canada - (YUL);Nashville, TN, United States - (BNA);Newark, NJ, United States - (EWR);Norfolk, VA, United States - (ORF);Omaha, NE, United States - (OMA);Philadelphia, United States - (PHL);Phoenix, AZ, United States - (PHX);Pittsburgh, PA, United States - (PIT);Port-Au-Prince, Haiti - (PAP);Portland, OR, United States - (PDX);Queretaro, MX, Mexico - (QRO);Richmond, Virginia - (RIC);Sacramento, CA, United States - (SMF);Salt Lake City, UT, United States - (SLC);San Diego, CA, United States - (SAN);San Jose, CA, United States - (SJC);Saskatoon, SK, Canada - (YXE);Seattle, WA, United States - (SEA);St. Louis, MO, United States - (STL);Tampa, FL, United States - (TPA);Toronto, ON, Canada - (YYZ);Vancouver, BC, Canada - (YVR);Tallahassee, FL, United States - (TLH);Winnipeg, MB, Canada - (YWG);Adelaide, SA, Australia - (ADL);Auckland, New Zealand - (AKL);Brisbane, QLD, Australia - (BNE);Melbourne, VIC, Australia - (MEL);Noumea, New caledonia - (NOU);Perth, WA, Australia - (PER);Sydney, NSW, Australia - (SYD)".split(";"),e=a.split("colo=")[1].split("\n")[0],t=(a.split("colo=")[1].split("\n")[0],a.split("tls=")[1].split("\n")[0]),o=a.split("http=")[1].split("\n")[0],s=a.split("sni=")[1].split("\n")[0],r=a.split("ip=")[1].split("\n")[0],l=a.split("uag=")[1].split("\n")[0];for(var d=0;d<i.length;d++)if(-1!=i[d].indexOf(e)){document.getElementById("cdn").innerHTML=i[d];break}document.getElementById("tls").innerHTML=t,document.getElementById("http").innerHTML=o,document.getElementById("sni").innerHTML=s,document.getElementById("ip").innerHTML=r,document.getElementById("useragent").innerHTML=l}})},$(document).ready((function(){getCDNinfo()}))</script></p><p style="text-align:center">感谢陪伴与布道，开源之火不灭。​</p><p style="text-align:center"><script>document.write("本次加载耗时: "+(performance.getEntriesByType("navigation").reduce((e,r)=>e+r.responseEnd-r.startTime,0)+performance.getEntriesByType("resource").reduce((e,r)=>e+r.responseEnd-r.startTime,0)).toFixed(0)+"ms")</script></p><p style="text-align:center">当前 SNI 状态： <span id="sni">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 TLS 版本： <span id="tls">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前 HTTP 版本： <span id="http">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前您的客户端 IP 是： <span id="ip">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">当前分配的 CDN 节点是: <span id="cdn">正在统计！或可能被浏览器防追踪拦截！</span></p><p style="text-align:center">您的 UserAgent 信息是: <span id="useragent">正在统计！或可能被浏览器防追踪拦截！</span></p><p></p></div></div></body></html>